{"posts":[{"title":"코드로 인프라 관리하기","text":"이번년도에도 한빛 미디어의 나는 리뷰어다에 선정되어 매달 책 한 권씩을 읽을 수 있게 됐다. 9월에는 Infrastructure as Code 책을 받아서 보게 되었다. 이 글은 해당 책에 대한 간단한 리뷰이다. 이 책은 IaC의 아주 폭넓은 관점으로의 학습을 도와주는 책이다. 특정 프레임워크를 가르쳐 주는 느낌은 아니다. 마치 데이터베이스 인터널이라는 책처럼 특정 데이터베이스에 대해 한정된 얘기가 아니라 IaC 라는 것 자체에 대해 가르쳐준다. 장점은 개념 적립과 코어한 컨셉에 대해 학습하기 좋다는 건데, 단점은 이것만으로 IaC를 경험하기는 어려울 것 같다. 이전에 Terraform 이라든지 CloudFormation 등을 사용한 경험이 있는 사람들이 읽어보면 기존 지식과 맞물려 시너지가 있을 것 같다. 책 내용 중에 제일 좋았던 건 인프라 스택을 설계하는 방법에 대해서도 학습할 수 있었다는 점이다. 보통 IaC 관련된 책을 봤을 때는 어떻게 기술을 써야 하는지를 학습하는데 이 책에서는 어떻게 구조를 나눠야 할지 어떤 패턴으로 작성할 수 있는지를 설명해준다. IaC에 대해 어느 정도 지식이 있고 IaC를 더 나은 방식으로 사용하길 원하는 개발자들에게 추천해주고 싶은 책이다.","link":"/posts/books/infrastructure-as-code/"},{"title":"Learning Go 간단 리뷰","text":"이번년도에도 한빛 미디어의 나는 리뷰어다에 선정되어 매달 책 한 권씩을 읽을 수 있게 됐다. 3월에는 Learning Go 책을 받아서 보게 되었다. 이 글은 해당 책에 대한 간단한 리뷰이다. 일단 Go를 자주 사용하는 개발자로서 Go 책이 자주 보이고 있다는 점에서 굉장히 기분이 좋다. 학습을 해왔던 여러 리소스들과 비교하면서 책을 읽어봤다. 일단 책의 난도는 Go를 접한 적 없는 개발자를 위한 책이었다. 물론 개발자가 아니더고 새롭게 Go를 접하는 사람들 역시 이 책으로학습해도 좋을 것 같은 생각이 들었다. 원래 언어 학습을 위한 책들은 이 정도의 난도로 만들어지는 것 같은데 유독 Go 책은 기존에 개발을 하던 사람들 만을 대상으로 쓴 책 처럼 느껴지는 경우가 많았다. 예를 들어서 The Go Programming Language와 같은 책이 그랬다. 이 책은 내부 동작에 대한 구체적인 설명까지 하지는 않았다. 다만 아주 기초적인 CS 지식의 경우 추가 설명처럼 박스를 만들어서 해당 CS 배경 지식을 설명해준다. 예를 들어서 Map 자료구조를 이해하기 위한 Hash에 대한 기본적인 설명 등… 가장 장점으로 느껴졌던 부분은 꽤 실용적인 내용도 담고, 최신 버전의 정보들이 잘 반영되어 있다는 점이다. 초보자가 언어 학습을 위한 교재로 선택하면 좋을 것 같다고 생각했다.","link":"/posts/books/learning-go/"},{"title":"SQL 쿡북 간단 리뷰","text":"올해도 한빛 미디어에서 나는 리뷰어다 활동을 지속하게 되었다. 이번 달 책은 SQL 쿡북을 받았고, 이 글은 이 책에 대한 간단한 리뷰이다. “쿡북” 이라는 이름의 책은 다들 레시피처럼 “어떤 상황에서 어떻게 할 수 있습니다.” 같은 내용을 소개해주는 구조를 띄고 있는 것 같다. 이번에 받은 책 역시 그런 형식이었다. SQL에서 특정 동작을 어떻게 할 수 있는지를 알려주는 레시피 책이었다. 이런 책은 사실 사전 지식이 있다면 모든 내용을 다 읽을 필요가 없다. 약간 사전처럼 필요한 경우 찾아보는 형식이어도 될 것 같다. 쿡북답게, 간결하게 필요한 정보를 전달해주고, 예시 테이블에서 수행했을 때의 결과까지 같이 보여주고 있어서 보기가 편했던 것 같다. 간결하다고는 했지만 쉬운 책은 아닌 것 같다. 예상 독자 역시 SQL을 이미 알고 있는 사람들을 대상으로 한다. 꽤나 복잡하고 디테일한 상황들까지 다루고 있어서 좋은 인사이트를 얻을 수 있었다.","link":"/posts/books/sql-cookbook/"},{"title":"클라우드 네이티브 애플리케이션 디자인 패턴","text":"이번년도에도 한빛 미디어의 나는 리뷰어다에 선정되어 매달 책 한 권씩을 읽을 수 있게 됐다. 7월 미션으로 나온 책 중에 하나인 클라우드 네이티브 애플리케이션 디자인 패턴을 받게 됐고, 이번 달에 읽어보게 됐다. 이 글은 이 책에 대한 간단한 리뷰이다. 내가 개발을 시작할 시점부터 이미 클라우드가 활발히 도입되어 있었고, 실제로 내가 개발을 하는 공간 역시 모두 AWS 위에서만 개발했지만, 무엇이 클라우드 네이티브적 특성을 갖는지 알 수 없었다. 이 책을 읽으면서도 뭔가 이게 클라우드와 연관된 것인지 그냥 MSA를 구성하기 위한 여러 방법들을 나열한 것인가 싶었다. 간간히 클라우드 컴포넌트에 대한 명시적인 언급이 있긴 했지만, 보다 어울리는 제목은 MSA 디자인 패턴이 아니었을까 싶다. 그런 관점에서 이 책은 지식을 넓히기에 적합했다. 굉장히 얇은 느낌으로 이해하고 있던 부분들이 이번 기회에 아주 명확한 표현으로 이해할 수 있었던 것 같다. 현재 실제 업무로 사용하고 있는 사이드카 패턴, BFF, Event Driven 등 개념을 적립하게 도움이 많이 된다. 이 책의 유일한 단점은 책 이름이 책의 일부분에 치중한 느낌이라는 점이고 (이것은 내가 잘 모르기 때문에 이렇게 느끼는 것일 수도 있다고 생각한다.) 특정 규모 이상의 서비스를 만드는 팀에 합류했는데 그곳에서 사용되는 인프라 아키텍처에 대해 생소한 감이 드는 개발자들에게 추천해주고 싶은 책이다.","link":"/posts/books/design-pattern-for-cloud-native-application-review/"},{"title":"유난한 도전과 훌륭한 조직","text":"그리스에서 “유난한 도전“이라는 책을 읽었다. 유난한 도전은 토스팀이 달려왔던 길을 얘기하는 책이다. 토스팀은 외부인으로 봤을 때 정말 멋지다고 생각하던 팀 중 하나이다. 이번 책이 나왔을 때, 내부인이 전달하는 현란한 물장구 이야기를 기대하며 책을 구매했다. 책은 생각보다 만족스러웠으며, 이 글은 그 책을 읽고 난 뒤 좋은 조직에 대한 생각을 정리하기 위한 글이다. 본인은 토스 내부와 완전히 단절된 사람으로, 완전히 외부인으로서 책을 통해 느낀 점을 말한다. 토스에 있는 친구들과 토스 내부에 대해 얘기해 본 적도 없으며 내가 느낀점이 실체와 다를 수도 있고 브랜딩이 된 외면을 보는 것일 수도 있다. 그런 건 상관 없고 내가 생각하는 이데아에 가까운 토스팀이 다시 한 번 재생산 되기 위해 무엇이 핵심일까를 고민하면서 읽었다. 토스의 문화내가 외부인으로써 공개된 자료를 통해 느낀 토스의 문화는 정말 “가슴뛰게 벅찬” 같은 느낌이다. 문제를 정의하고 해결하는 과정에 걸림돌 없이 목표를 달성하고 싶어하는 마음을 가진 동료들이 뭉쳐서 엄청난 속도로 밀고 나간다. 또한 절대적인 상급자가 존재하지 않고 자신이 책임지는 프로젝트를 끝까지 밀어붙일 수 있는 자율성을 보장한다. 마지막으로 이러한 과정에 발생하는 실패를 관용적으로 받아들이며 이전 실패와 상관 없이 다음 타석에 풀 스윙을 할 수 있는 타선으로 인재를 배치한다. 이 세 가지 정도가 외부인이 느낀 토스팀의 성공 조건이다. 많은 스타트업이 자신들의 회사가 그렇다고 말 하지만 실감은 안나는 것 같다. 누구나 이 방법이 스타트업의 정수라고 생각하지만 그런 능력이나 그런 가치관을 가진 사람들로 구성된 조직을 구성하거나 유지하는 것이 쉽지 않기에 적절한 규모를 가진 사례를 찾기 어렵다. 정말 극초기 스타트업은 모든 일이 이렇게 돌아가지만, 규모가 조금만 커져도 이런 모습이 발견되기란 쉽지 않은 것 같다. 속도를 유지하는 것책 기준 2019년 코로나 지원금 신청 및 조회 서비스를 만들 때 토스팀스러운 속도가 나왔다고 말한다. 정부 지원금을 신청하는 프로세스를 제공해야 한다는 목표를 팀원들끼리 만들고 이 목표 달성이 줄 수 있는 임팩트를 상상하면서 많은 사람들이 이 프로젝트에 자발적으로 참여했다. 토스는 2019년도에도 규모있는 스타트업이었지만 메인 페이지에 걸릴 서비스를 개발하는 TF 조직을 자유롭게 구성하고 엄청난 속도로 완성했으며 심지어 대표조차 퇴근할 무렵에나 이 스레드를 확인하며 “무슨 일이 벌어지고 있느냐”고 물었다고 한다. 이런 과정으로 서비스 개발이 가능했던 이유는 다음 세 가지 정도인 것 같다. 상급자와 무관한 의사 결정 빠른 공유와 정보 확산 빠르게 테스트 할 수 있는 인프라 상급자와 무관한 의사 결정을 하는 것은 작든 크든 지켜지기 꽤 어려운 것 같다. 작으면 작기 때문에 크면 크기 때문에 상급자의 개입이 들어간다. “우리는 상급자가 없는데요?” 라고 말하는 팀이 있을 수도 있다. 하지만 웬 뜬금없이 한 개발자가 “이거 만들어야겠는데요” 라고 말하면서 스레드를 열고, 여기에 사람이 모여서 프로젝트가 완성되는 과정은 정말 드물게 발생한다. 물론 토스 입장에서도 오랜 시간 동안 회자될 토스 다운 업무 진행이었다고 말할 정도로 자주 있는 일은 아니었겠지만 개인적으로 “그 당시 토스 규모 정도에서도 이런 동작이 가능하구나”라고 생각했다. 빠르게 정보를 공유하는 것은 이상적이지만 본래 목적을 잃고 노이즈가 되어버리는 경우도 자주 있다. 구성원들이 소통하는 채널이 노이즈로 인지되지 않도록 유지해야 한다. 중요한 정보가 확실히 전달되어야 하는데, 이때 “중요한 정보”라는 기준이 너무 모호하다. 모두가 중요한 정보라고 판단하는 기준을 비슷하게 가지고 있어야 하는데 이는 전사적인 비전, 미션이라고 불리는 것들이 유의미하게 구성원들과 합의되어야 할 것 같다. 바라보는 방향이 같으면 어떤 정보를 접했을 때 이 정보가 우리 팀에게 어떤 의미이고, 무엇이 필요한지 말하지 않아도 모두 알 수 있다. 물론 세부적이고 현실적인 알람 기준, 얼마나 구체적인 정보인지 등을 파악하고 유의미한 범위에게 우선 전달 되는 것도 중요하지만, 어떤 정보를 보고 각 조직이 어떤 도움을 제공할 수 있는지 자발적으로 참여하며 소통하는 조직이 되는 것이 중요하다. 빠르게 시도한다는 것은 조금 관용적으로 표현해서 빠르게 실패한다와 같다. 그러나 실패만으로 끝나면 안되고, 회수되어 팀에게 겸험치로서 흡수되어야 한다. 말은 쉽지만 회수되는 과정도 일이다. 만들었던 코드를 수정하고 데이터나 자료들을 정리하는 과정이 필요하다. 반복되는 시도와 실패를 회수하는 과정을 선형적인 속도로 유지하기 위해서는 인프라에 대한 투자가 필요하다. 토스팀은 실패를 기본으로 깔고 가던 시기가 있는데, “어짜피 실패할 건데 무슨 리팩토링이야” 같은 느낌으로 개발을 시작하는 것이다. 전재가 그렇다면 논리적으로 미래의 선형적인 개발 속도는 그렇게 중요한 의사 결정 요인이 아니다. 하지만 이는 언젠가 팀의 발목을 잡는다. 많은 스타트업들처럼 토스팀 역시 미래의 선형적인 속도 유지, 안정적인 개발은 당장 중요한 게 아니였으므로 이 작업은 계속 미뤄졌다. 많은 극복 스토리가 그렇듯 토스팀도 대장애 시대를 맞이하고 나서야 속도만큼이나 중요한 포인트가 있음을 인정하고 인프라에 힘을 쏟고 디자인 시스템을 개발했다. 인재 밀도토스의 문화로 잘 알려진 것들 중 여러 스타트업이 차용해 사용하고 있는 문화들이 있다. 상하 관계가 없는 조직, 정보의 투명성, 상향식이 아닌 자유로운 업무 프로세스 같은 것들인데, 토스가 시작했다고는 볼 수 없고 실리콘벨리의 문화가 적용된 것으로 알고 있다. 현재는 대부분의 IT 스타트업에서 이를 당연시하고 있고 이와 반대되는 조직은 스타트업이라 할 수 없다는 수준의 인식이 있다. “이런 게 스타트업 아니겠어요?”라는 맥락과 대중들이 가지고 있는 이미지가 있는데 이는 지금까지는 그렇게 부정적으로 사용되고 있지는 않다. 자유로움, 성공에 대한 욕망 등… 창의적이고 부지런한 이미지로 많이 사용되고 있다. 하지만 만약 내가 스타트업이라는 주제로 시트콤을 만들면 이 주제로 전체의 1/3 분량을 만들 수 있을 것 같다. 어찌됐든 토스팀은 이런 전형적인 스타트업의 문화를 가지고 성공했다. 토스팀을 보면서 이런 문화가 올바르게 동작하기 위해서는 먼저 인재 밀도가 일정 수준 이상 유지되어야 한다고 생각했다. 그래서 이 모든 걸 아우르는 주제는 인재 밀도이다. 스타트업에게 “인재”는 상황마다 다르다. 현재 어떤 스테이지에 있는가, 구성원과 팀원이 몇 명인가, 서비스는 어디에서 하고 있는가 등 상황에 따라 회사가 필요한 인재는 많이 바뀌는 것 같다. 하지만 개인적으로 아주 초기부터 유니콘에 도달한 이후에도 공통적으로 요구되는 인재상이 있는 것 같다. 크게 주도력, 실행력, 논리력 정도로 설명할 수 있을 것 같다. 책에서는 토스에서 자랑하는 프로덕트 오너(PO)가 정확히 이러한 능력들을 요구하는 것 같다. 논리적으로 설득적인 가설과 주장을 주도적이고 강하게 몰아붙여 검증하는 사람들이 필요하다. 이후 나오는 “인재”는 이러한 능력이 남들보다 월등한 사람들이라고 가정하고 작성했다. “블리츠 스케일링”이라는 책에서는 회사의 규모에 따라 단계를 나눠 블리츠 스케일링 전략을 설명하는 부분이 있다. 블리츠 스케일링에 대한 내용을 언급하려는 건 아니고, 약 마을(100명 초과 ~ 999명 이하) 단계까지는 적용될 수 있는 인재의 공통 분모가 아닐까 싶다. 그 이상의 기업에는 있어본 적이 없고 프로세스를 경험해보지 않아서 모르겠다. 스타트업에서 상하 관계라는 것은 적대시해야 할 것으로 치부되고 있다. 이런 경향은 오래 지속되었고 이제 어떤 스타트업의 소개 페이지에 “수평적인 조직문화”를 써놓는 게 오히려 더 이상하게 느껴진다. 수평적이라는 뜻은 이상적인 의미로 누군가의 의견에 실질적인 무게가 더 실리는 것이 없다는 뜻이다. 하지만 이게 현실적으로 효과적일까? 만약 구성원 중에 비즈니스를 잘 모르는 사람과 잘 아는 사람 사이의 의견이 동일한 경중으로 다뤄지고 절충안을 선택했다면 최선의 선택이었음을 확신할 수 있을까? 또는 설득을 통해 서로의 의견을 좁히는 과정이 더 합리적인 결론에 다다를 수 있다고 쳤을 때 더 경험 많은 사람의 결정을 보고 배우는 것보다 빠르고 효과적인가? 그니까 만약 수평적인 조직 문화를 만들려 “저 동료가 나보다 더 나은 결정을 할 수 있다.” 라는 가정이 필요하다. 가능성을 언급하는 전재이므로 항상 참이지만, 동료로부터 더 나은 결정이 자주 나와야 이 가정을 유지함으로써 얻는 효과가 크다. 따라서 수평적인 조직 문화를 만들고 싶으면 인재 밀도가 높아야 한다. 수평적이라는 것이 회사 의사결정에 자신의 의견이 반영됨으로써 회사에 기여하고 있음을 직접적으로 느끼는 데 도움이 되므로, 구성원들의 만족을 높여준다는 얘기는 부차적인 얘기다. 우리가 수평적일 수 있기 때문에 구성원들은 만족을 느끼는 방향이어야만 한다. 회사 정보의 투명성도 최근 많은 회사에서 가져다 쓰는 문구이다. 회사가 구성원들에게 최소한의 정보(연봉이나 구성원의 프라이버시, 보안 관련 정보 등)를 제외하고 모두 열람이 가능하게 하는 것을 말한다. 투명하게 정보를 공개해서 얻을 수 있는 장점은 무엇일까? 모든 구성원들이 회사와 관련된 많은 정보를 얻음으로써 각자의 문제에 가장 적합한 방법으로 문제를 해결하는 데 도움을 준다. 하지만 상하 관계와 비슷한 맥락으로 인재 밀도가 높아야 정보 공개도 유의미하다. 하달 방식이 아니라 자유롭게 만들어지는 업무 프로세스는 어떨까? 회사가 유기체처럼 성장한다는 느낌을 받을 수 있을 것 같다. 물론 엄청 뜬금 없는 걸 개발해서 넣겠다고 하는 것은 리더격인 사람이 보기에 당황스러울 수는 있는데 만약 이 사람이 진행하고자 하는 일이 회사에 좋은 결과를 가져올 수 있을 것이라고 믿을만 하다면 이 프로세스는 굉장히 효율적이고 빠르게 서비스를 성장시킬 수 있다. 인재는 왜 모이는가책을 보면 토스에는 적기에 최적의 인재가 필요한 역할을 맡아줬다. “마침 ~ 관련된 경험이 있는 OOO가 합류해 이 역할을 맡았다.”는 뉘앙스가 자주 나온다. 물론 현재 문제를 가장 잘 해결할 수 있어 보이는 사람들을 찾았겠지만 이 사람들이 돈을 많이 받았기 때문에 와서 이 역할을 수행하지는 않았을 것 같다. 물론 토스는 돈을 많이 주는 조직으로 유명하다. 이런 분들이 모이는 이유가 연봉만 있는 것이 아니라고 가정하고, 책을 읽으면서 그 외 몇 가지가 있을 수 있겠다고 생각한 포인트가 있다. 일단 인재가 가서 해결해야 하는 문제의 규모와 영향력이 얼마나 될지를 볼 것 같다. 책에서 초반 토스를 견인하던 제네럴리스트들이 더 이상 자신들의 역량을 발휘하지 못할 것 같다며 다음 스탭을 찾아 떠나는 내용이 있다. 본인이 해결하고 영향을 주는 범위가 큰 시점에 합류해 충분히 자신의 영향력을 행사한 후, 그 시점이 지나가자 퇴사를 하는 모습으로 보였다. 인재 모으는 일반화된 방법이 있을 것이라 생각하지는 않는다. 인복은 그야말로 복이 아닐까. 정리토스의 이야기를 다큐멘터리를 보듯 재밌게 읽었다. 마침 뛰어난 조직에 대해 고민하던 시기와도 겹쳐서 이런 저런 생각을 하면서 토스팀이 발전해온 길을 뜯어본 것 같다. 또 이러한 조직이 나타나길 바라고 거기에 내가 포함되어 있으면 좋겠다.","link":"/posts/essay/outstanding-team/"},{"title":"어떤 글을 써야 잘 썼다고 소문이 날까","text":"최근에 열심히 공부한 내용을 바탕으로 기존 글과 다르게, 많은 노력을 들이며 ‘좋은 글 쓰기가 뭘까?’ 생각하면서 나름 열심히 글을 작성한 적이 있는데, 쓰던 버릇도 있고… 시간 투자에 대한 부담도 있어서 다시 원래 내가 보기 위한 글을 썼던 기존과 비슷한 느낌이 나는 글을 썼다. 글을 쓴 다음 블로그 애널리틱스를 확인하게 되었는데, 과거와 다르게 많은 글들이 올라가 있어서 그런지, 생각보다는 많은 사람들이 블로그를 구경하고 가신다는 걸 알게 되었다. 본인도 리서치 하면서, 저질의 글을 보면 썩 좋지 않기 때문에, 조금씩 글의 질에 신경을 많이 써야 할 것 같다고 생각했다. 필자 역시 아티클을 자주 검색해 보는 편인데, 그 중에서 확실히 선호하는 느낌의 글을 자주 보던 블로그들을 확인해보고 어떤 점이 좋은 글 느낌을 내는지 고민해봤다. 과거에 자기 만족을 위해 쓰던 글블로그를 시작한 이유 중 가장 큰 이유는, 내가 공부한 내용을 어떤 공간에 정리를 하고 싶다는 것이었다. 깃헙 TIL 레포지토리, Notion 처럼 개인적인 공간에 작성한 적도 있지만, 작성하다 보니 이왕이면 누군가에게 피드백 받을 수 있는 공간이면 좋지 않을까 싶은 생각에 블로그를 만들게 되었다. 여러 블로그 서비스를 사용하지 않은 이유는 기록 자체를 마크다운 형태로 가지고 있고 싶었기 때문이었다. 그런 요구사항때문에 자연스럽게 깃헙 블로그를 사용하게 되었다. 근본적으로 본인의 만족과 효용을 타겟으로 글을 쓰다 보니 가장 큰 목표는 공부했던 내용을 천천히 정리하는 것과, 나중에 봤을 때 ‘아 이게 이랬지’ 하는 효과를 느끼도록 글을 쓰는 것이었다. 그 결과로 지금 다시 글을 보면서 느껴지는 점은, 많은 블로그 글들이 나에게만 읽기 쉬운 경우가 많았다. 타인이 보기엔 지나치게 듬성듬성하지 않았을까 싶다. 블로그 방문자의 점진적 증가블로그 글을 쓴지는 대충 일년 정도가 되었는데, 그래도 나름 짜잘한 내용의 포스트를 여러 개 올려서 그런지, 점진적으로 블로그 방문자들이 증가하고 있다. 신기함을 느끼는 중 부끄럽지만 현재 블로그에는 부실한데 부실하지 않은 척 하는 글들이 많다. 본인이 보기에는 부실하지 않겠지만, 처음 맥락을 접하는 사람들에게는 갸우뚱 할 수 있는 전개를 가진 글들이 많이 있다고 생각하는데, 보통 이런 글을 보게 되면 온전한 정보를 획득하지 못하는 경우가 많이 발생하게 된다. 글을 쓰면 사람들에게 유기적으로 전파될 정도의 퀄리티를 가진 글을 쓰고 싶다는 생각이 들어서, 평소에 어떤 아티클을 보면서 필자가 ‘좋은 글이다’ 라고 느꼈는지 고민해봤다. 좋은 블로그 특좋은 블로그는 사실 아주 많다. 그 중에서도 아주 개인적인 기준으로 만족감이 높았던 블로그 특징이 뭐가 있을까 고민해 보면서 (개인적이지만, 커뮤니티에서 공유되는 횟수를 봤을 때 객관적이라고 생각된다.) 느꼈던 것들을 정리했다. evan-moon 블로그Evan Moon이라는 이름으로 블로그를 운영하시는 개발자 개인 블로그인데, 이곳에 올라오는 글을 빠지지 않고 보는 편이다. 이 블로그 글을 읽으면 읽기 편하고 부담스럽지 않다고 생각 된다. 왜 그런 생각을 하게 되었을까? evan-moon.github.io 블로그의 최근 글 이 블로그와 포스트를 보면 항상 생각 나는 점은 깔끔한 블로그 모습과 UI와 중간 중간 끼워져 있는 위와같은 여담 또는 적절한 이미지 배치이다. 실제로 굳이 필요한 이미지가 아니더라도 글 흐름상 읽는 텀을 만들어주고 다음 문장을 읽으로 가기 편하게 만들어준다. 또 이 글을 쓰면서 느꼈는데, 폭보다 좁은 이미지의 가운데 정렬이라든지, 이미지 설명을 가운데 정렬 한다든지, 이미지에 border-radius가 적용되어있는 것 등이 깔끔하고 읽으면서나 글을 작성 하면서 불편함을 느끼지 않게 해줄 수 있을 것 같다고 생각이 들었다. 마지막으로 기술적인 내용에 대해서 전개가 아주 적절하다고 느꼈다. 예를 들어서 3만명이 읽었다고 하는 CORS는 왜 이렇게 우리를 힘들게 하는걸까? 글은 글쓴이가 겪은 스토리를 앞서 간단하게 보여면서 시작하고, CORS가 무엇인지 등 정말 기본적인 내용, 그리고 문제 해결을 위한 보다 근본적인 동작 방식, 마지막으로 본격 문제 해결하는 방법. 이런 식으로 구성되어있다. 떠올리기 힘들다거나 시도하지 못할 구성이라는 건 아니지만, 저런 구성 역시 앞서 고민을 한 후에 적용할 수 있는 방법일 것 같다. 본인은 이런 구성에 대한 부분을 크게 신경 쓰면서 글을 쓰지 못 하고 있었기 때문에… 어느 정도 반성이 되는 성찰이었다. jbee 블로그유우명한 블로그 중에 하나인 jbee님 개인 블로그 글은 아주 적당한 마크다운 문법이 자주 사용되는 걸 자주 봤다. 지금 글처럼 소주제에 맞게 ### 하는 거 외에도 여러 방법으로 문단을 나누고 내용을 보여준다. 읽기에 불편함이 없을 정도로 문단을 구성하고 새로운 형태로 글이 보여지게 만들면, 글을 읽는 입장에서 조금 더 쉽게 읽혀지는 것 같다. 이와 유사한 현상은 Evan moon님 블로그에서도 느껴졌었다. 이렇게 귀여운 썸네일은 직접 만드시는 걸까 위는 정말 정말 마음에 드는 썸네일인데 내용과 상관은 없지만, 비모를 사용한 게 제대로 취저인 썸네일이었다. 아, 비모 얘기도 사실이지만, 썸네일에 들이는 공도 고민해볼만 했다. 사실 본인은 썸네일에 큰 노력을 들이는 편이 아니다. 그냥 있던 이미지이거나(현재 글) 흔히 구글에서 사용되던 이미지를 찾아서 사용하는 편이다. 가끔 정말 아키텍쳐 모습이라든지, 그려서 만든 적은 있는데… 본인 만족을 위해 써오던 글에서 썸네일에 공을 들이는 경우는 거의 없었던 것 같다. 두 개인 블로그 모두 공교롭게도 Gatsby를 사용하고 있다. 현재 블로그에서 큰 불편함을 느끼거나, 옮겨타고 싶은 욕구가 솟지는 않지만, 왜 그러셨는지 조사해볼만 할 것 같다. 현재 사용 중인 테마도 Evan moon님이 사용하시던 테마였는데, 너무 깔끔해보여서 따라 선택하게 되었다. 이 테마에서 Gatsby로 옮기시게 된 까닭을 작성하셨던 적이 있었던 것 같으니 참고하고… 조금 더 깔끔한 블로그 모습을 만들 수 있을 것 같다고 생각되면 옮기는 것도 고민해봐야겠다. Toast 기술 블로그이 기술블로그는 NHN Toast 팀이 운영하는 기술블로그이다. 정말 기술적으로 좋은 글이다 라고 느꼈던 많은 글들이 이 블로그에 있었다. 레디스 관련 글이 퍼뜩 떠오르는데, 이 글 말고도 정말 퀄리티 높은 많은 글이 있다. 이 시리즈… 정말 못 버려… 아무래도 개인 공부한 내용이 블로그 글의 주 타겟이 되다 보니, 특정 기술의 공식 문서를 빠르게 보고 배우는 시리즈를 작성한 경험이 많은데, 사실 최근엔 이런 식으로 정리 할 필요가 있을까 싶은 생각도 든다. 특정 기술에 한정된 경우 버전업이 되면서 자연스럽게 레거시 포스팅이 되어버리고 필자 조차도 다시 사용해야 할 경우에 이 글을 찾지 않고 새로운 내용을 확인하게 된다. 이번에 새롭게 Toast 팀이 쓰는 글을 살펴보면서, 보다 전문성이 느껴지도록, 휙휙 바뀌는 내용이 아니라 기술들이 가지고 있는 패러다임이나 컨셉을 가지고 깊은 글을 쓰면 좋겠다고 생각하게 되었다. 이 내용은 사실 여전히 고민 중이다. 필자가 쓸 수 있는 글의 범위가 많이 줄지 않을까 하는 염려도 섞여있다. 그래서 앞으로는여러 블로그를 다니며 성찰한 결과, 앞으로는 대충 아래와 같은 점을 유의해봐야겠다고 결심했다. 글의 주제를 범용적이면서 기술이나 공학의 핵심에 가까운 내용으로 고민해보자 글의 구성을 먼저 고민하고, 입시하면서 썼던 일반적인 논술 쓰기처럼 뼈대와 구조에 대한 부분도 많은 고민을 해야겠다. 글 자체가 여유로운 느낌, 잘 읽히는 느낌을 주기 위해 적절한 마크다운 문법과 이미지, 문단 나누기, 여담 등을 끼워 넣어 보자 블로그가 조금 더 깔끔한 글쓰기 플랫폼이 될 수 있도록 자체적인 커스텀을 해볼까 싶다. 조금 더 깔끔하고 쓰기 편하게! 마지막을 제외하고는 당장 실행해볼 법 하지만, 1년간 블로그를 대하는 태도가 하루 아침에 싹 바뀌겠는가. 그리고 너무 부담을 느끼고 글 쓰는 빈도가 확 줄어들까봐 걱정스럽기도 하다. 그래도 위 정도는 할만하지 하면서 작성한 거니까… 트라이 해보자. 이런 기술 외적인 내용을 다루는 글도 가끔 써야겠다. 카테고리는 essay","link":"/posts/essay/what-post-is-good-post/"},{"title":"DynamoDB Internals (1) - Dynamo","text":"아마존은 지구 규모 스케일 서비스를 운영하면서 자신들의 요구와 가장 잘 들어맞는 범용적인 분산 스토리지 시스템을 만들어냈는데 이 시스템이 바로 Dynamo이다. 시스템을 만들고 운영한 경험을 논문으로 발표했고, 이 논문은 분산 스토리지 시스템 생태계에 큰 영향을 주었다. 이 논문에 영향을 받아 오픈소스에서는 Cassandra가 개발되었고 AWS 서비스의 SimpleDB, DynamoDB를 만드는 기초가 되었다. DynamoDB의 구조가 완전히 Dynamo와 같지는 않지만, 뿌리가 되는 Dynamo 시스템에 대해 먼저 알아보자. RDB가 적절하지 않았던 이유 Dynamo는 아주 가용성 높은 키 값 스토어이다. 애초에 이러한 시스템을 구성하게 된 이유가 뭘까? 아마존은 Oracle이 제공하는 엔터프라이즈 데이터베이스 스케일을 넘어선 지구 단위의 글로벌 서비스로 성장했다. 이런 스케일을 감당하기 위해 아마존은 직접 DB를 설계하고 관리하기로 결정했다. 이를 위해 아마존은 그동안의 데이터베이스 사용 패턴에 대해 조사했고 다음과 같은 특징들을 확인할 수 있었다. 스케일아웃 처리를 위한 JOIN 사용 제거 (JOIN을 사용하지 않음) 인덱스를 통한 단일한 데이터 검색이 대부분 복수의 데이터를 가져오는 패턴도 있었지만, 이 경우 보통 하나의 테이블에서만 데이터를 가져옴 이런 특징들은 Key-Value 형태로 매칭되는 쿼리 조건으로 충분히 해결할 수 있고, RDB의 아주 강력한 쿼리들을 사용할 필요가 없었다. 즉, 규모있는 데이터 처리를 위해 RDB가 요구하는 컴퓨팅 리소스를 준비할 필요가 없다. 따라서 아마존에서는 RDB 사용이 데이터베이스 접근 패턴에 적합하지 않다고 판단했다. 아마존에서 필요한 데이터베이스는 일반적인 RDB와 다르게 다음과 같은 특징을 가져야 한다 간단한 쿼리 모델: 프라이머리 키를 통해 데이터를 정의하고 읽어올 수 있는 간단한 쿼리 모델이 필요하고, 관계형 스키마라든지, 복잡한 테이블 연결은 불필요하다. 느슨한 ACID: 트랜잭션의 ACID, 특히 일관성을 보장하는 것은 데이터 가용성 측면에서 좋지 않다. 약한 일관성을 가지고 동작하는 애플리케이션에 적합한 데이터베이스여야 한다. 효율성: 아마존 플랫폼의 엄격한 지연율 제한을 여러 서비스 도메인에서 충족할 수 있어야 한다. 아마존 플랫폼은 일반적으로 500TPS 기준으로 99.9% 백분위가 300ms 안에 처리되어야 한다는 지연율 기준이 있다. 서비스마다 읽기와 쓰기 비율이 다르기 때문에 데이터베이스의 설정을 통해 어떻게 분산 환경의 읽기 쓰기를 수행할지 결정할 수 있어야 한다. 효율성은 조금 모호한 단어같이 보일 수 있는데, 설정값을 통해 범용적으로 인프라에 도입할 수 있다는 측면에서의 효율성을 뜻하는 것 같다. 시스템 디자인 고민위와 같은 목표를 달성하기 위해 필요한 몇 가지 고민이 있었다. 이 고민을 해결한다면 시스템 디자인의 목표를 달성할 수 있다. 전통적인 RDB의 데이터 복제 알고리즘은 강력한 일관성을 위해 동기적으로 데이터를 복제한다. 이 수준의 일관성을 얻기 위해서는 특정 시나리오에서 데이터 가용성을 포기해야 한다. 즉, 높은 수준의 일관성은 정확한 답을 공유하지 못하는 불확실한 상황일 때 차라리 데이터를 사용 불가능하게 만들어버린다. 분산 시스템에서 아주 유명한 이론인 CAP 이론에서 말하듯 일관성, 가용성, 그리고 네트워크 파티션 내구성 세 가지를 모두 충족시킬 수 없다. 네트워크 파티션 상황은 반드시 발생하게 되어있고, 데이터 일관성을 유지하기 위해서는 가용성을 포기해야 한다. 강력한 일관성을 유지하는 데이터 복제는 “전통적”이라고 표현했지만, 이 논문이 쓰이기 전의 상황인듯 싶다. 정확한 히스토리는 잘 모르지만, 최근 RDB에서 레플리카를 운영하는 방법이 꼭 완전한 일관성을 요구하지는 않았던 것 같다. 핵심 고민Dynamo는 네트워크 장애가 무조건 발생한다는 가정 아래 가용성을 최대로 높이도록 디자인되었다. 이를 위해 데이터가 레플리카에 동기적으로 전파되도록 처리하지 않고, 비동기적으로 전파되도록 했다. 구체적으로 어떻게 전파되고, 어떤 상황에서 성공으로 판단하는지 등은 이후 후술한다. 아무튼 이러한 비동기 전파 상황에서는 데이터의 충돌 문제가 발생할 수 있다. 하나의 데이터를 수정한 값이 모종의 이유로 두 가지 이상의 버전으로 분기되는 것을 데이터 충돌 상황이라고 표현한다. A 노드에 a → a'가 되도록 수정했는데 비동기 전파로 인해 다른 노드에 전파가 되기 전에 성공 응답을 보내고 A 노드에 네트워크 파티션이 발생했다고 가정해보자. 그다음 같은 데이터 a를 갖고 있던 B 노드에 a → a''로 수정을 가했다면 시스템은 두 가지 버전의 a 데이터를 갖게 되는 것이다. 이 상황을 해결하는 방법은 다음 두 가지 방향의 문제를 해결하는 것이다. 언제 해결할 것이냐? (쓰기 시점 or 읽기 시점) 누가 해결할 것이냐? (클라이언트 or 데이터베이스) 전통적으로는 쓰기 시점에 이 문제를 해결하며 읽기 작업의 복잡도를 단순하게 유지한다. 이런 시스템에서는 주어진 타임아웃 시간 내에 모든(혹은 특정 정족수) 데이터 저장소에 닿지 못하면 쓰기가 실패할 수 있다. Dynamo는 “항상 쓰기 가능한“ 데이터 스토어를 목표로 한다. 아마존의 많은 서비스에서 고객의 업데이트 작업을 거절하는 경우 고객 경험을 해치는 결과를 가져온다. 예를 들어서 쇼핑 카트는 반드시 소비자가 담거나 지운 아이템을 네트워크 파티션이 발생하더라도 반영할 수 있어야 한다. 따라서 쓰기 작업의 가용성을 위해 Dynamo는 읽기 작업에 이 충돌 해결 역할을 맡겼다. 쓰기 작업에서 이 문제를 해결한다면 버저닝이 발생하지 않도록 회피하는 형태로 문제를 해결한다고 볼 수 있고, 읽기 시점에서 해결한다면 문제 발견 후 복구하는 과정이 있다고 볼 수 있다. 그다음 “누가 해결할 것인지” 선택해야 한다. 데이터베이스에서 일관적으로 처리하도록 하는 방법은 굉장히 제한적이다. 예를 들어서 마지막 업데이트가 발생한 시점을 기준으로 데이터를 덮어씌우는 방법처럼 아주 간단하고 단순한 방법으로만 문제를 해결할 수 있다. 반대로 애플리케이션에서 이를 해결하도록 두면, 데이터가 어떻게 비즈니스 로직과 연결되는지 이해하고 있기 때문에 더 적절한 방법을 선택할 수 있다. 어떻게 버저닝 문제를 해결하는지 구체적인 내용은 후술한다. 기타 고려사항위 가장 큰 두 가지 설계 고민 외에 다음과 같은 고민을 하며 시스템을 설계했다. 증분 확장성 (Incremental Scalability): 데이터 스토리지 노드를 추가 또는 삭제할 때 데이터베이스 운영 및 시스템에 최소한의 영향만 주면서 이를 수행해야 한다. 대칭성: 모든 스토리지 노드가 동일한 역할 수행해야 한다. 이유는 특수한 역할을 하는 노드를 지정하기 시작하면 프로비저닝 시스템의 복잡도가 증가하기 때문이다. 탈중앙성: 중앙에서 컨트롤하는 시스템보다 P2P를 통한 분산된 컨트롤이 더 선호되는 방향으로 시스템을 설계한다. 이에 대해서는 구체적인 이유보다는 과거에 중앙 시스템의 문제로 인한 운영 중단 사태가 발생한 적이 있기 때문에 이를 피하기 위해서 이러한 고민을 했다고 한다. 이질성 (Heterogeneity): 시스템에서 동작하는 노드들이 불균일하다는 특성을 이용할 수 있는 시스템이어야 한다. 예를 들어 작업 분배가 각 노드의 Capacity에 비례해 분배될 수 있는 시스템이어야 한다. 이를 통해 보다 효율적으로 작업 분배가 이뤄질 수 있다. 시스템 목표와 그 목표를 위한 고민이 잘 매칭되는지 확인해보자. 간단한 쿼리 모델은 Key-Value 스토리지라는 점, 느슨한 ACID는 비동기적 데이터 전파를 통해 가용성을 높인다는 점(그리고 데이터 충돌을 해결하기 위한 핵심 고민 두 가지), 효율성의 경우 여러 설정값으로 위 고민을 해결하도록 했다. 이 여러 설정값에 대해서는 나중에 더 자세히 다룬다. 시스템 아키텍처위 고민을 어떻게 해결했는지 구체적으로 살펴보자. Dynamo는 독자적인 기술의 탄생이 아니고 그 전부터 논의되어온 분산 시스템을 구성하기 위한 기술들의 집합이다. 요약하자면 다음 방법들을 사용하고 있다. 문제 해결책 해결 목표 파티셔닝 (노드 추가 or 제거) Consistency Hashing 증분 확장성을 보장할 수 있음 쓰기 HA 구성 vector clock 개념과 클라이언트에서 데이터 충돌 해결 쓰기 가용성이 증가 일시적 실패 쿼럼 (Quorum)과 Hinted Handoff 범용적인 시스템을 위한 일시적인 실패와 복구 처리 방법 영구적 실패 Merkle trees 데이터 동기화를 위한 데이터 전송량을 최소화 멤버십, 실패 탐지 가십 기반 멤버십 프로토콜 노드 기능의 동일성을 유지하면서 탈중앙화 시스템을 유지할 수 있는 시스템 글에서는 고민을 해결하기 위해 핵심적이라고 생각되는 파티셔닝문제, 쓰기 HA, 일시적 실패에 대해 다룰 예정이다. 파티셔닝데이터 또는 트랜잭션이 증가함에 따라 스토리지 노드가 감당해야 하는 트랜잭션이 점점 늘어나면 이를 처리하기 위한 스토리지 노드가 추가로 붙어야 한다. 즉, 동적인 파티셔닝을 위한 방법이 필요하다. Dynamo에서는 이를 위해 Consistent Hashing 방법을 사용하고 있다. Consistent Hashing은 해시 함수의 결과 범위가 고정된 원형 공간(Ring) 안에서 다뤄진다. 링 형태라는 것은 가장 큰 해시값의 마지막이 가장 작은 해시값으로 연결된다는 것을 의미한다. 해당 시스템 안에서 각 스토리지 노드는 랜덤 값이 할당되고, 이 랜덤 값의 해시값을 가지고 링 위에 위치 시키는 방법이다. 스토리지 노드 안에 들어갈 데이터 역시 키를 해싱한 값을 기준으로 링 위에 배치된다. 이때 데이터의 위치에서 시계 방향으로 돌았을 때 처음 만나게 되는 스토리지 노드에 실제 데이터가 저장되게 된다. 일반적인 방법으로 데이터의 키를 해싱한 값으로 샤딩을 진행한 경우, 노드가 추가되거나 삭제될 때마다 데이터를 다시 해싱해야 하는 큰 비용이 있다. Consistent Hashing은 데이터를 균일하게 분산하면서도 노드가 추가되어도 이런 재해싱 과정 필요 없이 이웃한 노드에만 영향을 주는 방식으로 스토리지 노드를 추가한다. 동적인 스케일링을 위해 아주 적합한 방법이지만, 스토리지 노드의 이질성을 고려하지 않는다. 위에서 언급했던 “이질성“을 고려한 시스템을 위해 Dynamo는 하나의 물리적인 노드가 여러 개의 가상 노드(Virtual Node)와 매칭되며 가상 노드들이 링 위에 올라가도록 설계되었다. 각 가상 노드는 실제 물리 노드와 연결되고 가상 노드의 개수는 물리적 노드의 실제 성능에 맞게 조절된다. 즉, 성능이 좋은 스토리지 노드는 많은 가상 노드와 연결되어 링 위의 비교적 많은 영역을 담당한다. 반대의 경우 적은 수의 가상 노드와 연결되게 된다. 랜덤하게 배치된다는 점 역시 분산이 고르지 못하다는 단점이 있다. 운영하면서 몇 가지 버전을 거친 이후 링을 고르게 나눠 노드를 배치하는 방법으로 이 문제를 해결할 수 있었다고 한다. 이 글에서는 해당 내용에 대해 설명하고 있지 않다. 궁금하다면 이 링크에서 설명을 더 읽어보자. HA(High Availability)HA를 위해서는 복제 시스템이 필요하다. 즉, 데이터를 본래 저장해야 하는 스토리지 노드 외에 다른 스토리지 노드에도 저장할 수 있어야 한다. Dynamo는 몇 개의 호스트에 데이터를 복제할 것인지 설정 파라미터로 결정할 수 있도록 만들었다. 몇 개의 호스트에 복제하는지 결정하는 파라미터는 이 글에서 N으로 표기한다. 위 이미지에서 키 K를 가진 데이터는 (A, B] 사이에 위치하게 되고, B 노드에 기본적으로 저장된다. 그다음 데이터가 복제되어야 하는 노드는 N - 1 만큼 시계 방향으로 돌며 만나는 노드이다. N = 3 이라면, B 외 C, D도 이 데이터를 저장하는 대상이 된다. 이렇게 특정 데이터에 대해 저장할 책임이 있는 노드 리스트를 preference list라고 부른다. preference list 안에 가상 노드로 인해 물리 노드가 중복되어 들어가지 않도록 같은 물리 노드를 가리키는 가상 노드를 스킵하면서 리스트를 채운다. 데이터가 여러 노드에 전파되는 것은 비동기적으로 (아직 어떻게 비동기적으로 동작하는지 설명하지 않았다. 후술할 예정) 발생한다. 이러한 이유로 Dynamo는 결과적 일관성(Eventual Consistency) 특성을 갖게 된다. 결과적 일관성은 일시적으로 데이터가 일관적이지 않을 수 있지만 결국 같은 데이터를 보장한다는 뜻이다. 예를 들어 Put을 호출해 데이터를 쓰는 작업을 할 때, 필요한 모든 노드에 데이터가 복제되는 것을 기다렸다가 응답을 보내주지 않는다. 아직 몇 개의 노드는 데이터를 쓰기 전이지만 Put에 대한 성공 응답을 보낸다. 만약 이런 상황에서 연속적으로 Get 요청을 보내면 어떤 노드의 데이터를 읽느냐에 따라 최신 버전의 데이터를 가져오지 못할 수도 있다는 것을 뜻한다. 위에서 살짝 예시를 들었는데, 쓰기 상황에서 이렇게 최신 데이터가 아닌 오브젝트를 기반으로 업데이트를 진행하게 되면 데이터 버저닝(분기, 데이터 충돌)이 발생한다. 앞서 언급한 것처럼 분기된 데이터를 통합하기 위해서 두 가지 결정이 필요하다. 언제 통합할 것인지? 누가 통합할 것인지? 그리고 Dynamo에서는 읽기 시점에 클라이언트에서 해결한다고 설명한다. 데이터 버저닝과 이를 해결하는 방법을 설명하기 전에 먼저 간단하게 Dynamo의 시스템 인터페이스를 확인해보자. 데이터에 접근하기 위한 Get과 업데이트를 위한 Put이 있다. 각각은 다음과 같이 호출된다. get(key): 스토리지 시스템에서 키와 연결된 오브젝트 복제본을 찾아 컨텍스트가 담긴 단일 오브젝트 또는 컨텍스트가 담긴 오브젝트 리스트를 반환한다. put(key, context, object): 오브젝트 복제본이 연관된 키에 의존해서 어디에 위치해야 하는지 결정하고, object를 디스크에 쓴다. get의 결과를 “컨텍스트가 담긴 오브젝트“라고 표현하고 있다. 컨텍스트는 데이터의 버전 정보를 포함한 메타데이터를 의미한다. 복수가 될 수 있다는 것은 하나의 데이터에 대해 여러 갈래로 나눠진 데이터 버전이 있는 경우 해당 버전들을 모두 가져오기 때문이다. put의 인자에서 context를 통해 수정 대상인 오브젝트 컨텍스트를 전달하고 기존에 나뉘어있던 버전들을 마지막 put을 기준으로 통합하게 한다. Dynamo의 쓰기 과정은 읽기 이후 쓰기를 수행하는 식으로 클라이언트에서 사용해야 하는 구조인 것으로 보인다. Dynamo의 오브젝트 버전 관리는 vector clock을 사용한다. 이는 어떤 노드에 저장되어 있는지, 몇 번째 데이터 수정인지를 담고 있는지, 이렇게 두 가지를 갖는 튜플 리스트이다. 1vector clock = [(Node, Counter)...] 컨텍스트 안의 vector clock을 비교해서 인과적인 순서(앞선 버전인지, 동시에 나눠진 버전인지)를 밝힐 수 있다. 많은 경우 새 버전이 과거 버전을 포함하고 있기 때문에 시스템 안에서 정규 버전(나눠진 두 버전을 조정한 새로운 버전)을 결정할 수 있다. 이렇게 시스템에서 버전을 조정하는 과정을 “syntactic reconciliation“이라고 한다. 그러나 동시 업데이트에 의한 분기는 클라이언트에 의해 조정이 된다. 위에서 언급했던 것처럼 get(key)를 통해 복수의 오브젝트 버전을 받으면 클라이언트에서 적절한 로직을 통해 이를 합치고 업데이트해 줘야 한다. 이렇게 클라이언트에 의해서 조정하는 것은 “semantic reconciliation“이라고 한다. 따라서 정확하게는 Dynamo는 데이터베이스에 의한 조정과 클라이언트에 의한 조정, 두 가지 방법 모두 사용하고 있다고 볼 수 있다. 일시적 실패쓰기 동작 중 특정 노드의 장애로 인해 일시적인 실패가 발생할 수 있다. 이에 대한 처리를 Hinted Handoff라고 불리는 방식으로 해결하고 있다. 일단 이 설명을 하기 전에 먼저 “쓰기 실패 상황“ 또는 “읽기 실패 상황“을 정의해야 한다. 구체적으로 어떻게 비동기 복제를 하면서 응답을 전달해주는지 확인해보자. Sloppy Quorum쿼럼(Quorum)은 “정족수”라는 뜻이다. 조금 단순하게 설명하자면 읽기의 최소 성공 수를 R, 쓰기의 최소 성공 수를 W라는 변수를 사용해 시스템에서 읽기와 쓰기의 실패 여부를 확인하는 방법이다. Dynamo 시스템에서 Get과 Put 요청은 어떤 스토리지 노드든 받을 수 있다. Read 또는 Write 요청을 처음 받은 노드를 “coordinator“라고 부른다. 일반적으로 coordinator는 preference list를 만들 때 첫 번째 노드이다. 요청은 HTTP를 통해 전달되는데, 로드 밸런싱을 통해 앞단에서 요청을 관리한다면 coordinator는 맨 첫 번째 노드가 아닐 수도 있다. 로드 밸런서를 사용하지 않는 경우 Partition-Aware 클라이언트 라이브러리를 사용해 어떤 노드에 요청을 보내야 하는지 클라이언트에 의해 결정하도록 한다. 위 이미지처럼 복제해야 하는 노드 수만큼 preference list가 지정되고 coordinator는 이 리스트 안의 나머지 노드에 데이터를 전파한다. 그리고 정족수만큼의 정상 응답을 받으면 클라이언트에게 성공 응답을 보내준다. 예시에서는 W = 2로 설정되었기 때문에 위 이미지의 녹색 박스 노드에서 정상적으로 값을 저장했다면 이 요청은 성공한 요청이 된다. 이 방법은 “Strict Quorum” 시스템으로, 만약 복제되어야 하는 노드들 중에 최소 정족수만큼의 성공을 만들지 못하면 실패 처리 된다. 만약 preference list 안에 있는 N개의 노드 중 N - W - 1개만큼의 노드가 장애 상황이라면 쓰기 실패 상황이 발생한다. 그러나 아마존은 실패 처리를 극단으로 최소화하고 싶어 했다. 그래서 도입한 시스템이 “Sloppy Quorum”이다. 이름에서도 알 수 있듯 조금 느슨하게 쿼럼을 만족시키는 방식이다. 본래 preference list는 키를 해시한 다음 링 위에서 시계 방향으로 봤을 때 첫 번째로 만나는 노드를 포함해 상위 N개의 노드가 속하게 된다. 그런데 이 노드를 단순히 상위 N개가 아니라 “건강한 노드 상위 N개”로 만드는 것이다. 위 예시에서 설정 파라미터값이 N = 3, W = 3이고 만약 노드 D에 장애가 발생했다고 가정해보자. 그렇다면 preference list는 실제로 B, C, E 노드를 담고 있게 된다. 이런 방식에서 장점은 가용성을 높일 수 있다는 장점이 있지만, 문제는 약속과 다른 노드가 데이터를 저장할 수 있다. 레플리카의 범위는 노드 D까지인데, E에 데이터가 저장된 상황이다. Hinted Handoff이 문제를 해결하기 위해 Hinted Handoff라는 전략을 사용한다. E로 전달된 복제본 데이터는 메타 데이터 안에 원래 저장될 타겟 노드 정보를 포함하고 있다. 이 정보를 “힌트”라고 표현한 건데, 힌트가 박힌 복제본을 받으면 노드는 원래 저장소와 분리된 다른 로컬 임시 저장소에 해당 데이터를 저장한다. 본래 노드가 다시 복구되면 임시 저장소에 담긴 데이터를 보내주고 임시 저장소에서는 삭제한다. 이러한 Sloppy Quorum과 Hinted Handoff를 가지고 가용성을 크게 높일 수 있게 된다. 만약 W = 1이라면, 모든 노드가 장애 상태여야 쓰기가 실패한다. 하지만 이 방법은 일시적인 장애 상황에서 처리를 위한 방법이고, 영구적인 실패 이후 데이터 동기화 과정을 위해서는 다른 방법이 필요하다. 간단히 설명하자면, 가지고 있는 데이터를 머클 트리로 구성해 변경이 발생한 지점을 특정하고 변경된 부분만 데이터를 전송할 수 있게 해준다. 머클 트리로 두 노드의 데이터가 같은지 확인하는 케이스는 분산 시스템에서 자주 등장한다. 가장 유명해진 예시로는 블록체인이 있다. 이 링크에서 머클 트리에 대한 설명과 머클트리를 블록체인과 Dynamo에 적용한 유즈 케이스 설명을 확인할 수 있다. 실제 운영Dynamo는 몇 가지 서비스에서 다양한 설정값으로 사용되고 있다. 각 서비스는 데이터 버전을 합의하는 로직을 독립적으로 구성하고, 서비스의 특성에 따라서 쿼럼 파라미터인 R, W값을 설정한다. 일관성이 중요하다면 W값을 높게 설정하고, 읽기(쓰기) 성능 자체가 더 중요하다면 R(W)값을 낮춘다. Dynamo의 일반적인 (N, R, W)는 (3, 2, 2)로 구성되어있다. Reference https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf https://en.wikipedia.org/wiki/CAP_theorem https://ko.wikipedia.org/wiki/%EC%9D%BC%EA%B4%80%EB%90%9C_%ED%95%B4%EC%8B%B1 https://www.waitingforcode.com/general-big-data/dynamo-paper-consistent-hashing/read https://www.allthingsdistributed.com/2017/10/a-decade-of-dynamo.html https://jimdowney.net/2012/03/05/be-careful-with-sloppy-quorums/ https://medium.com/coinmonks/merkle-trees-concepts-and-use-cases-5da873702318","link":"/posts/database/dynamodb-internals-1/"},{"title":"DynamoDB Internals (2) - DynamoDB","text":"지난 글에서 DynamoDB를 지탱하는 큰 축인 Dynamo 시스템에 대해서 알아봤다. Dynamo 시스템은 DynamoDB가 등장하기 한참 전에 설계되었지만 이름에서 쉽게 알 수 있듯 굉장히 깊은 부분을 공유하고 있다. 그러나 DynamoDB는 관리형 인프라 서비스로 제공되는 만큼 사람들이 더 쉽고 범용적으로 사용할 수 있게 설계되었다. 구체적으로 어떻게 어떤 점이 다른지 알아보자. 특정 영상에서 “Dynamo와 다르게 ~ “라고 하면서 특정 컴포넌트가 어떻게 다른지 설명하는 부분은 있지만, 공식적인 텍스트로 차이점에 대해 명시한 문서는 찾지 못했다. 다만 공부하면서 ‘이 부분은 이런 방법으로 변경했구나’ 같은 느낌을 많이 받을 수 있었는데, 이런 포인트에 집중해서 글을 쓰려 노력했다. 본인의 의견에 해당하는 경우 의견임을 명시했고 그 외는 Reference의 도큐먼트 또는 AWS re:Invent 영상을 참조했다. 글에서 DynamoDB와 Dynamo를 명확히 구분한다. Dynamo는 DynamoDB의 기반이 되는 분산 스토리지 시스템을 의미한다. 이전 글을 참고하자. 기본적인 이야기이 글을 읽고 있다면 DynamoDB의 기본적인 이야기에 대해서는 이미 알고 있을 확률이 높으므로, 다음 섹션부터 읽어도 좋다. Item, Attribute, Primary KeyDynamoDB는 Key Value Storage NoSQL이다. Redis처럼 키값에 따라 데이터가 매칭이 되고, 해당 데이터의 스키마가 정해지지 않고 자유롭다. 하나의 데이터, 즉, RDB에서 하나의 로우라고 부를 수 있는 것을 DynamoDB에서는 아이템(Item)이라고 부른다. 이 아이템은 속성(Attribute)값으로 이루어져 있다. Attribute가 가질 수 있는 값은 아래와 같다. Number (N) String (S) Boolean (BOOL) Binary (B) List (L) Map (M) String Set (SS) Number Set (NS) Binary Set (BS) Number, String, Boolean, Binary 같은 단순한 타입도 있지만, List, Set, Map과 같은 복합 타입도 존재한다. 이 값들을 읽고 쓰는 동작들은 모두 Atomic 하게 전달된 순서대로 동작한다. 위 리스트의 괄호로 쳐진 값은 Attribute를 지정할 때 사용하는 코드이다. 직접 사용하다 보면 이해하기 쉽다. 아이템의 스키마가 정해지지 않았다는 것은 아이템마다 자유롭게 속성값을 바꿀 수 있다는 것을 의미하는데, 예외가 존재한다. DynamoDB에서 테이블을 구성할 때 설정하는 Primary Key가 바로 그 예외이다. 이 값은 아이템을 식별하는 키 역할을 한다. 따라서 모든 아이템에서 Non-Nullable한 값이다. 이 키를 구성하는 방법은 기본적으로 2가지가 있다. 하나는 간단하게 Partition Key(파티션 키, PK, Hash Key)만 사용하는 것과 다른 하나는 PK와 함께 Sort Key(소트 키, SK, Range Key)를 함께 사용하는 것이다. 이 글에서 PK는 모두 Partition Key를 의미하고, Primary Key는 줄여 쓰지 않았다. 위 두 키를 제외하고는 쿼리를 할 수 있는 방법이 기본적으로는 없다. 만약 일반 Attribute를 가지고 필터링하려면 Scan을 사용해야 한다. 따라서 굉장히 디테일하게 애플리케이션에서 사용하는 쿼리 패턴에 대해 이해하고 키값을 설계해야 한다. DynamoDB를 설계하는 방법으로 가장 유명한 방법은 Single Table Design이 있다. 이는 과거에 한 번 소개한 바 있다. RDB에서도 키를 사용하지 않으면 Scan을 하는 것은 마찬가지이다. 하지만 RDB는 필요에 따라 인덱스를 쉽게 추가할 수 있지만 DynamoDB의 경우엔 제약 조건도 있고, 고민해야 할 포인트가 몇 가지 있다. Secondary Index기본 키만으로 필요한 쿼리를 모두 할 수 없는 경우가 많기 때문에, 추가로 두 종류의 보조 인덱스를 지원한다. Local Secondary Index (LSI) Global Secondary Index (GSI) LSI의 경우 테이블을 생성할 때 만드는 기본 인덱스를 가진 테이블(이하 베이스)과 동일한 PK를 사용해야 한다는 제약 조건이 있다. SK만을 설정할 수 있으며, 베이스가 만들어질 때만 설정할 수 있다. 테이블이 이미 운영 중인 경우엔 LSI를 추가할 수 없다. 장점으로는 GSI와 다르게 강력한 읽기 일관성을 제공한다는 점이다. 읽기 일관성에 대해 자세히 모른다면, 이하 글 내용에서 대략 알 수 있다. LSI를 굳이 사용해야 하는 케이스는 별로 없다. 강력한 읽기 일관성이 제공되고, GSI보다 비교적 싸게 운영할 수 있다는 장점이 있지만, 파티셔닝에 제약 사항(PK값이 같은 아이템들의 크기의 합이 파티션의 최대 사이즈보다 커질 수 없다는 점)이 생기기도 하고, 생성 시점이 정해져 있다는 점 역시 큰 장애물이다. 공식 문서에서도 GSI 유즈 케이스가 더 많다고 설명하고 있다. 강력한 읽기 일관성이 필요한 경우엔 DynamoDB 자체가 그다지 좋은 선택지가 아닐 수 있다. 일관성이 중요한 애플리케이션은 RDB를 사용하는 것이 일반적으로 더 좋은 선택일 것 같다. GSI의 경우 LSI와 다르게 파티셔닝의 제약도 붙지 않고, PK를 아예 별도로 설정할 수 있다. 또한 테이블이 생성된 이후에도 자유롭게 GSI를 설정할 수 있다. 다만 강력한 읽기 일관성을 제공하지 않고, 쓰기 읽기 용량을 새로 생성해야 하므로 추가적인 비용이 든다. LSI는 공짜라는 건 아니다. 베이스의 읽기 쓰기 용량을 공유하게 된다. Key &amp; Partition파티션 키는 테이블 안에서 아이템이 어떤 파티션에 속하는지를 결정하는 키다. 해시 함수를 가지고 키를 해시한 다음 해시값을 이용해 각 파티션으로 분할한다. SK는 파티션 안에서 아이템을 정렬하는 용도로 사용된다. 각 파티션의 최대 사이즈는 10GB로, 만약 파티션 키에 속하는 아이템이 10GB가 넘는다면 SK를 기준으로 파티션을 분할한다. 아이템은 400KB가 최대 사이즈이기 때문에 각 파티션마다 최소 25,000개 정도의 아이템을 담을 수 있다. 이미지 출처 SK가 없는 경우 PK가 같은 아이템이 테이블마다 하나씩 있을 수 있고, 아이템은 400KB 사이즈 제한이 있기 때문에 파티션 안에서 아이템 컬렉션(PK가 같은 아이템의 모음)이 10GB가 넘을 수 없다. LSI가 설정된 테이블은 위에서 말한 것처럼 10GB 이상의 아이템 컬렉션을 유지할 수 없다. 이 값은 꽤 큰 값이고 PK를 애플리케이션에서 잘 나누면 해결할 수 있는 문제이지만, 아무튼 이런 찜찜한 제약이 생긴다. 각 파티션은 읽기 용량과 쓰기 용량을 설정할 수 있다. On Demand 방식으로 설정한다면 들어오는 처리량을 적절히 받아주겠지만 가격이 비싸진다. 예측 불가능할 정도로 성장하는 서비스가 아니라면 보통 예측치를 기반으로 Read Capacity Unit(RCU), Write Capacity Unit(WCU)를 설정한다. 보내는 요청이 어떤 것인지(일관적인 읽기, 트랜잭션 쓰기 등)에 따라 다르지만, RCU 한 개는 기본 읽기의 경우 4KB 아이템 사이즈 기준 초당 2개의 아이템을 읽을 수 있고 WCU 한 개는 1KB 아이템을 초당 한 개 쓸 수 있다. On Demand 방식과 Auto Scaling에 대해서는 이 글에서 다루고 있지 않다. 간단히 설명하자면 Auto Scaling은 배달앱처럼 “점심시간에 높은 트래픽을 찍고 평시에는 낮다” 처럼 범위에 대한 이해가 있는 애플리케이션에 대해 해당 범위에 맞춰서 처리 용량을 늘이고 줄이는 설정이다. On Demand는 서비스가 예측 불가능한 속도로 성장하는데, 이를 문제 없이 받아주기 위해 넣는 설정이다. 둘의 차이가 명확히 있다. 테이블마다 최대 RCU, WCU가 정해져 있어서 프로비저닝 당시 설정한 값에 따라 초기 파티셔닝이 결정된다. 읽기 용량은 파티션마다 최대 3,000 그리고 쓰기 용량은 파티션마다 1,000이 최대이다. 따라서 다음 식으로 초기 파티셔닝이 결정된다. 1initial partition = (RCU / 3000) + (WCU / 1000) 합친 값을 정수로 올림 한 값을 기본 파티션 수로 설정하고, 2개 이상의 파티션이 생성되면 Capacity Unit은 공평하게 분배된다. 예를 들어 RCU 3000, WCU 1000으로 설정하면 초기 파티션은 2개로 나눠지고, 각각 RCU 1500, WCU 500을 나눠 갖게 된다. Request RouterDynamo 논문에서는 클라이언트에서 직접 파티션을 선택해 요청을 보낼 수 있는 방법과 그 앞단에 로드밸런서를 통해 파티션을 찾아가는 방법에 관해 얘기한다. 특별히 어떻게 사용 중이라는 말은 없었지만, Client 쪽에서 어떤 스토리지 노드로 요청을 보내야 하는지 알고 있는 “Partition-Aware”를 사용하는 것으로 보였다. 하지만 DynamoDB에서는 모든 요청을 Request Router라고 불리는 중간자에 보낸다. Request Router는 두 가지 컴포넌트와 상호작용 후 실제 데이터가 있는 스토리지 노드에 접근하게 된다. Authentication System: AWS 플랫폼에서 공통으로 사용되는 권한 확인 컴포넌트 Partition Metadata System: 파티션의 리더 스토리지 노드를 관리해, Request Router가 요청을 보낼 노드를 선정 Partition Metadata System 내부적으로 Auto Admin이라는 시스템이 동작하고 있다. 이는 관리형 서비스를 만들어주는 핵심적인 컴포넌트로, AWS에서는 이를 DynamoDB의 DBA라고 부른다고 한다. 구체적인 관리에 대해서는 조금 있다가 후술한다. Partition Replication해시 함수에 의해 정의되는 파티션은 각각 레플리카 셋(Replica Set)을 갖고 있다. 리더 노드와 두 개의 추가적인 복제 노드를 갖게 되는데, 이는 AWS의 가용 영역에 골고루 나눠서 운용된다. Dynamo 시스템에서는 Sloppy Quorum을 사용하고 어떤 스토리지 노드든 요청을 처음 받게 되는 coordinator로서 동작할 수 있다. DynamoDB에서는 이와 다르게 리더 노드를 선출한다. 이 과정은 Paxos 알고리즘으로 구성되어있다고 하는데, Paxos를 이번 글에서 다루고 있지는 않다. Paxos는 분산 시스템에서 특정 값이 합의되도록 하는 합의 알고리즘이다. DynamoDB에서는 파티션을 구성하는 세 개의 스토리지 노드들 사이에 “리더”가 어떤 노드인지를 합의하는 과정에 Paxos를 사용한다. Paxos는 스탠포드 대학생을 가르쳐도 이해하기 위해 1년이 걸렸다는 “이해 불가능한” 알고리즘이라는 슬픈 이야기가 있다. 이에 따라 “이해 가능한 합의 알고리즘”이라는 느낌으로 Raft가 나왔다고 한다. 대략 설명하자면, 집단의 리더는 현재 정상 상태임을 다른 스토리지 노드에 하트 비트를 통해 알린다. 영상에서는 1.5초? 2초쯤 한 번씩 하트 비트를 보내고 있다고 하는데, 만약 다른 스토리지 노드가 이를 받지 못하면 새로운 리더를 선출하기 위해 다른 노드에 본인을 리더로 주장하는 정보를 보낸다. 이 요청을 다른 스토리지 노드가 동의하면 새로운 리더가 선출된다. 어찌 됐든 리더가 있다는 사실이 중요한데, Dynamo와 다르게 쓰기 요청이 리더에게만 보내지기 때문이다. 리더는 항상 최신 데이터를 갖게 된다. 쓰기 요청을 받은 리더는 로컬 데이터를 수정하고 다른 두 레플리카에게 이 요청을 전파한다. 그리고 둘 중 하나의 성공 응답을 받으면 이 쓰기 요청이 성공했다고 응답한다. 반면 읽기의 경우 세 개의 레플리카 중 하나에게 요청을 보낸다. 따라서 연속된 요청을 통해 값을 읽는다면 1/3 확률로 최신 데이터가 아닐 수도 있다. Dynamo와 마찬가지로 Eventual Consistency가 발생한다. Dynamo의 Quorum이 아예 적용 안 된 건 아니라는 것을 알 수 있다. 쓰기 쿼럼이 3개 중 2개 성공으로 설정된 것이나 다름없다. 위 설명은 모두 기본 읽기와 쓰기에 대한 설명이고, 트랜잭션 또는 강력한 읽기 일관성에 대한 옵션은 다르게 동작한다. Storage Node위에서 언급했지만, 스토리지 노드는 파티션을 구성하는 레플리카 셋이다. 내부적으로는 크게 두 가지 컴포넌트로 구성되어있다. B Tree: 쿼리와 뮤테이션이 발생할 때 사용되는 자료구조 Replication Log: 파티션에서 발생하는 모든 Mutation Log를 기록하는 시스템 B 트리의 경우 우리가 흔히 아는 그 자료구조가 맞다. RDB에서 인덱스로 사용되는 트리 자료구조가 똑같이 사용된다. Replication Log도 다른 DB에서 레플리카 셋을 유지할 때 복구를 위해 사용되는 컴포넌트와 똑같다. 위에서 잠깐 말했던 Auto Admin이 이 복구 과정에 개입한다. Auto Admin은 레플리카 셋의 리더와 그 위치를 관리하고 스토리지 노드를 모니터링하는데 스토리지 노드에 장애가 발생해서 다운되면 새로운 스토리지 노드를 생성하고 다른 스토리지 노드의 Replication Log를 가지고 자료구조를 복사해간다. 새로운 스토리지 노드의 Replication Log가 성공적으로 B 트리에 적용되면 파티션에 합류할 자격이 생긴 것으로 보고 레플리카 셋에 합류시킨다. Secondary Index위 기본 내용에 보조 인덱스에 대한 이야기를 짧게 했는데 이 구조가 어떻게 되어있는지 확인해보자. 프로세스는 일반적인 테이블과 비슷하다. PK를 해시 해서 각 파티션으로 나눠 보낸다. 다른 점은 보조 인덱스는 베이스 테이블과 독립적으로 파티션을 구성한다는 점이다. 그리고 테이블 내에서 보조 인덱스에 해당하는 Attribute이 수정되면 이 작업은 Log Propagator라고 하는 컴포넌트에 의해 보조 인덱스 파티션의 리더 스토리지 노드에 전파된다. Log Propagator는 스토리지 노드의 Replication Log를 바라보고 있다가 보조 인덱스 수정이 발생하면 Request Router가 베이스 테이블에 요청하듯 보조 인덱스 파티션에게 변경을 요청하게 된다. 이렇게 비동기적으로 전파되는 구조이므로 보조 인덱스의 Eventual Consistency는 필수적이다. 해시 기반으로 샤딩 된 데이터를 수정할 때 원래 위치한 스토리지 노드에서 데이터를 삭제하고 해시 된 위치에 맞는 스토리지 노드에 새로 쓰는 작업을 해야 하기 때문에 쓰기 작업이 생각보다는 무겁다. 파티션에 뮤테이션을 만드는 작업은 레플리카 셋 3개에 동일한 작업을 하는 것과 같기 때문에 베이스 파티션 레플리카 셋, 보조 인덱스에서 삭제될 파티션 레플리카 셋, 보조 인덱스에 추가될 파티션 레플리카 셋에 뮤테이션이 발생한다. 즉 하나의 보조 인덱스를 수정하는 작업은 9개의 스토리지 노드의 수정을 가져온다. 따라서 Secondary Index의 수정은 자주 발생하지 않는 것이 좋다. 이는 샤딩을 할 때 기준이 되는 필드가 자주 수정이 발생하면 안 된다는 얘기와 같다. 영상에서 별다른 설명은 없었지만, 위 설명은 GSI에 해당하는 설명일 것이라 생각한다. LSI는 테이블이 생성될 때 같이 생성되어야 한다는 점, PK는 베이스 테이블과 같아야 한다는 점, 강력한 읽기 일관성이 제공된다는 점, 베이스 테이블의 RCU와 WCU를 공유한다는 점, 그리고 이름에서 알 수 있듯, 베이스 테이블과 같은 파티션을 공유하는 것 같다. Provisioning &amp; Adaptive Capacity처리 용량 프로비저닝에 대해서는 위에 파티셔닝에 대해 설명하면서 함께 얘기했다. 이 섹션에서는 조금 구체적인 동작 방식에 대해서 짧게 설명한다. RCU, WCU는 쿼터를 정할 때 흔히 사용되는 Token Bucket 알고리즘으로 구성되어있다. 매초 RCU 수만큼 토큰이 버킷에 쌓이는데 버킷의 총용량은 설정한 RCU의 300배 정도이다. 따라서 아무것도 안 하면 5분 정도는 토큰이 쌓인다. 다만 버킷의 용량을 초과하면 토큰은 버려진다. 이런 구조로 트래픽이 치솟는 상황에서도 일시적으로나마 스로틀링이 생기는 것을 막아준다. 위에서 잠깐 언급한 적 있는데 RCU, WCU는 파티션에 골고루 분산된다. 만약 파티션이 3개이고 RCU가 300이라면 각 파티션은 100씩 RCU를 나눠 갖는다. 이렇게 나눠진 파티션에서 실제 운영할 때 아래 이미지처럼 25 RCU, 150 RCU, 50 RCU를 사용한다고 가정해보자. 위와 같은 상황에서 두 번째 파티션의 RCU가 50만큼 부족하고 나머지는 남는 상황이다. 총합은 75 RCU 만큼 남기 때문에 위 요청량이 잘 처리되어야 하지만 데이터가 고르게 분산되지 않아 Hot Partition이 생길 수 있다. AWS에서는 이 문제를 해결하고자 Adaptive Capacity를 도입했다. Adaptive Capacity는 Adaptive Multiplier라는 실숫값을 RCU, WCU에 곱해 일시적으로 처리 용량을 수정해주는 방식이다. Adaptive Multiplier는 피드백 루프를 돌며 지속해서 조절되는데, 이런 조절 루프를 돌리는 컴포넌트를 PID Controller라고 한다. DynamoDB에서 PID Controller는 인풋으로 소비된 용량, 프로비전된 용량, 스로틀링 속도, 현재 Multiplier 값을 받아서 결과로 새로운 Multiplier 값을 넘겨준다. 위 예시에서는 1.5 정도 값이 Multiplier로 설정되면 스로틀링이 해결된다. 하지만 처리 용량은 테이블에 적용되는 개념이기 때문에 파티션마다 부족한 값에 대해 Multiplier를 곱하는 구조가 아니고, 테이블의 전체 처리 용량에 곱해진다. 따라서 위 예시에서는 총 150만큼 처리 용량 초과가 발생한다. 그러나 이 상태는 잠깐 지속되고 PID Controller에 의해 정상화된다. 일시적으로 스로틀링을 해결해주는 장치라고 볼 수 있다. 즉, Adaptive Capacity는 Hot Partition을 완전히 해결해주는 장치는 아니다. 애초에 Hot Partition이 나오지 않도록 데이터 분산을 잘 만들어야 하고 특별히 많은 요청을 처리해야 하는 파티션이 있다면 아예 별도로 테이블을 관리하는 것도 좋은 방법이다. 이번 글은 DynamoDB를 가장 기본적으로 사용하는 상황에서 거치게 되는 컴포넌트 구성에 대해 작성했다. 특히 파티셔닝 얘기와 프로비저닝 얘기는 우리가 어떻게 키를 설계해야 할지 대략적인 감을 잡을 수 있게 해준다. 이 외에도 설명하지 않은 Auto Scaling, Stream, 강력한 읽기 일관성, 트랜잭션 등 여러 기능이 DynamoDB에 있다. 특수한 유즈 케이스가 있다면 따로 AWS 문서를 확인해보자. Reference https://en.wikipedia.org/wiki/Amazon_DynamoDB https://www.alexdebrie.com/posts/dynamodb-partitions/ https://dzone.com/articles/partitioning-behavior-of-dynamodb https://www.allthingsdistributed.com/2012/01/amazon-dynamodb.html https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/ServiceQuotas.html#default-limits-throughput-capacity-modes https://www.dynamodbguide.com AWS re:Invent 2018: Amazon DynamoDB Under the Hood: How We Built a Hyper-Scale Database (DAT321) AWS re:Invent 2018: Amazon DynamoDB Deep Dive: Advanced Design Patterns for DynamoDB (DAT401)","link":"/posts/database/dynamodb-internals-2/"},{"title":"DynamoDB 설계 방법: Single Table Design","text":"NoSQL 종류 중 하나인 DyanamoDB는 일반적인 SQL 테이블과 다르게, query를 할 때 조건을 설정할 수 있는 대상이 Partition Key (이하 PK)와 Sort Key (이하 SK) 그리고 추가적으로는 Global Secondary Index (이하 GSI)와 Local Secondary Index(이하 LSI)로 구분되는 Secondary Index로 한정된다. 다른 속성 필드에 대해서는 쿼리 조건을 설정할 수 없다. 만약 다른 속성에 대해 결과를 보려면 scan을 사용해야 한다. scan은 테이블의 모든 데이터를 조회하기 때문에 성능면에서 좋지 않은 모습을 보여준다. 이러한 특성이 있어서, DynamoDB 테이블은 일반적으로 SQL 테이블을 만들듯 만들면 안 된다. 이 글은 AWS에서 공식적으로 추천하고 있는 Single Table 구조로 설계하는 방법에 대해서 다루고 있다. DynamoDB와 RDBS 설계의 차이점전통적으로 SQL 데이터베이스에서 테이블을 설계할 때, 스키마를 디자인하고, 데이터를 정규화 한 다음 사용하면서 필요한 쿼리를 작성하게 된다. AWS에서는 이 순서를 뒤집어 생각해야 한다고 설명한다. 다시 말해서 사용하게 될 쿼리들에 대해서 테이블을 만들기 전에 알고 있어야 한다는 뜻이다. 그래야 위에서 언급한 scan과 filter를 사용하는 것을 최소화 할 수 있게 된다. 또한, GSI는 테이블이 만들어진 이후에도 추가 또는 삭제가 가능하지만, LSI는 테이블이 만들어질 때 설정 해줘야 한다는 이유도 있다. 또한 AWS는 Single Table형태로 DynamoDB 테이블을 설계할 것을 추천한다. SQL 데이터베이스를 설계할 때는 보통 여러 테이블에 데이터를 나눠 담고, 관계에 따라 Relation을 설정한다. 데이터를 쿼리 할 때는 적절하게 Join하는 방식으로 데이터를 가져오게 된다. 이 방식은 개발자 입장에서 개발의 편리함을 주지만, 성능면에서 비용이 있는 방식이다. 하지만 DynamoDB는 SQL처럼 Join을 기본적으로 지원해주지 않는다. 즉, 일반적으로 SQL 테이블처럼 테이블을 나누기 시작하면, 복잡한 쿼리를 사용해야 할 때 여러 번의 쿼리를 사용해야 한다는 뜻이다. 이는 RDS보다 더 큰 비용을 감수해야 하는 샘이다. 배경 지식간단하게 Primary Key와 Secondary Index에 대해 확인해보자. 글 전체에서 Primary Key는 줄이지 않고 사용했다. Partition Key는 PK로 줄여 사용했다. 마찬가지로 Secondary Index는 줄이지 않고 사용하고, Sort Key는 SK로 줄여 표현했다. Primary KeyPrimary Key는 항목을 나타내는 고유 식별자가 되어야 하며, 두 아이템이 동일한 키를 가질 수는 없다. DynamoDB에서 Primary Key는 단일한 PK, 또는 PK와 SK로 구성될 수 있다. Primary Key라는 조건때문에 단일 PK로 이루어진 경우는 아이템마다 다른 PK를 보장해야 하지만, SK와 함께 Primary Key를 구성한다면, 중복된 PK가 존재해도 된다. PK와 SK가 함께 사용된 Primary Key를 Composite primary key라고 하고, 이 경우에는 PK와 SK에 의해 항목이 구분될 수 있어야 한다. DynamoDB는 PK의 해시 값을 계산해, 항목을 저장할 파티션을 결정한다. 동일한 PK를 가질 수 있고, 같은 값에 대해서는 같은 파티션에 SK의 오름차순으로 저장하게 된다. Secondary Index보조 인덱스라고 문서에 해석되어있지만, 글에서는 Secondary Index로 사용했다. 기본적으로 쿼리를 위해 PK와 SK를 사용하게 되지만, Secondary Index를 추가해 사용할 수 있게 된다. DynamoDB는 다음 두 가지 형태의 Secondary Index를 지원하고 있다. Global Secondary Index (GSI) Local Secondary Index (LSI) PK와 SK로 구성되고, 기존 테이블과 달라도 된다. 테이블과 같은 PK를 사용하지만, 다른 SK를 사용하게 할 수 있다. 할당량 30개 할당량 5개 테이블 사용 중에 추가나 삭제 가능 테이블 만들 때만 생성이 가능하고 중간 삭제도 할 수 없음 Single Table Design설계 방식Single Table은 관계가 있는 테이블을 하나로 관리하는 것을 의미한다. 이 방법은 관계가 있는 테이블을 pre-join 하는 방법으로써 SQL 테이블 설계 할 때처럼 DynamoDB를 설계하면 나타날 수 있는 문제를 해결한 것으로 볼 수 있다. Single Table을 설계하는 과정은 다음과 같다. SQL 설계 하듯, 일반적인 ERD를 설계한다. 데이터 접근 패턴을 정의한다. Primary Key와 Secondary Indexes을 디자인 한다. 위 순서에서, 3 번의 경우, PK와 SK를 합쳐서 Primary Key를 구성하게 되는데, 이때 여러 테이블이 pre-join 되어있다고 봐야 하기 때문에, PK와 SK를 디테일한 이름을 가진 필드로 두지 말고 아주 일반적인 이름을 사용할 것을 권장한다. 그렇게 설계한 뒤 PK와 SK를 통해 어떤 Entity에 접근 하고 있는 건지 구분 할 수 있게 해야 한다. 자세한 예시를 확인해보자 예시2019 AWS re:invent에서 들어준 예시를 확인해보자. E-commerce 서비스를 위해 User와 Order테이블을 설계하는 예시이다. 실제로는 아래 예시에서 조금 더 나아가 Filtering Pattern에 대해서도 다룬다. 정말 정리 해두고 싶은 내용이지만, 글 분량이 너무 길어지고 적당하게 잘 정리할 수 있을지 모르겠어서 생략했다. 먼저 ERD 정의를 해줘야 한다. ERD를 정의는 다음과 같이 했다. 다음은 데이터 접근을 어떻게 할지 미리 정의해야 한다. 예시에서 애플리케이션에서 다음과 같은 접근 패턴을 갖고 있다고 가정해보자. User Profile 가져오기 User에 대한 Order 리스트 가져오기 단일한 Order와 그에 대한 Order Items를 가져오기 Primary Key와 Secondary Indexes를 설계할 차례이다. 위에서 말한 것 처럼 일반적인 이름으로 PK와 SK를 만들고, 두 값으로 어떤 Entity에 접근하는지 구분할 수 있게 설계한다. 데이터 예시 위 데이터는 users, user_address, orders 관계를 일부 반영한 모습이다. ERD와 설계한 접근 패턴을 반영했는지 확인해보자. Primary Key는 PK와 SK로 구성되고, PK는 USER#username, SK는 #PROFILE#username 또는 ORDER#orderId 형태로 구성되고 있다. 이렇게 패턴을 정의해서 어떤 Entity에 접근하는 건지 구분 할 수 있게 만들면 된다. user_address는 1:N 관계지만, user_address 자체를 PK또는 SK로 쿼리 해야하는 경우는 없다. (예를 들어서, 주소를 기반으로 유저를 검색하거나, 주소에 따라 유저들을 가져오는 쿼리는 필요가 없다.) 따라서 비정규화 + Document 타입의 Address라는 이름으로 필드에 추가된 모습이다. 다만 Address는 Orders의 필드로서의 역할도 한다. 두 경우는 다른 모습을 가질 수 있다. orders와 users도 마찬가지로 1:N 관계이다. 다만 위와 다르게 Order에 따른 쿼리가 필요하다. 따라서, PK와 SK로 User에 따라 Order 리스트를 쿼리할 수 있어야 한다. 예를 들면 아래와 같이 쿼리할 수 있게 된다. 1PK = USER#alexdebrie AND BEGIN_WITH(SK, 'ORDER#') 위 쿼리로 유저에 대한 Order 리스트를 가져올 수 있게 된다. 지금까지 설계된 테이블은 아래 모습을 갖췄다. 하지만 아직 orders와 order_items 관계는 설계되기 전 모습이다. Order에 따라서 Item을 가져오지 못하므로 남은 부분들을 고려해 테이블을 설계해보자. orders와 order_items의 관계를 살펴보면, 마찬가지로 1:N 관계이다. 그리고 단일한 Item 조회를 해야 한다는 점을 반영해 아래와 같이 PK 패턴을 정의해 넣을 수 있다. 결과적으로 Entity를 아래 이미지 처럼 설계하게 되었는데, 이때 관계에 따라 동일한 패턴을 PK또는 SK로 쓰게 하는 것이 중요한 핵심이고, 이 방식으로 설계 해야 Join 하는 것과 같은 효과를 가질 수 있다. 지금까지 진행된 설계의 문제는 Order를 기준으로 데이터를 가져오는게 불가능하다는 점이다. PK에서 Order를 쿼리할 수 없기 때문인데, 이를 해결하기 위해서 Secondary Index를 설정해보자. 이번 예시에서는 Inverted Index라고 불리는 전략을 소개한다. 이 전략은 말 그대로 PK와 SK를 뒤집은 Primary Key를 GSI로 만드는 것이다. 위와 같은 GSI를 만들면, Order를 쿼리 했을 때, 두 가지 타입의 Entity에 바로 접근이 가능해지는데, Order에 따른 Item 리스트와, Order의 User를 가져오게 된다. 이렇게 데이터를 쿼리하면 orders에 users와 order_items를 조인한 것과 같은 결과를 갖게 된다. 위 예시를 정리해 보자면 ERD에서 1:N 관계로 설계 했을 때, DynamoDB의 테이블을 설계하는 패턴들에 대해서 다루고 있다. Denormalizing + Attributes화 하기 (user_address와 users) Primary Key로 구성해서 쿼리 (users와 orders) Secondary Index를 구성해서 쿼리 (orders와 order_items) 미리 두 번째 단계에서 설계했던 접근 패턴에 대해서 이제 다음과 같은 쿼리 형태로 접근할 수 있게 됐다. (나타낸 코드는 실제 코드가 아닌 의사코드이다.) 123456- User Profile 가져오기 PK = USER#alexdebrie AND SK = #PROFILE#alexdebrie- User에 대한 Order 리스트 가져오기 PK = USER#alexdebrie AND BEGINS_WITH(SK, 'ORDER#')- 단일한 Order와 그에 대한 Order Items를 가져오기 SK = ORDER#orderId AND BEGIN_WITH(PK, 'ITEM#') Single Table Design의 한계Single Table Design은 DynamoDB에 아주 적합한 설계 방식이지만, 몇 가지 한계점이 있다. 우선 가장 큰 문제는 필요한 쿼리를 미리 알고 있어야 한다는 점이고, 새로운 접근 방식을 정의해야 하는 경우에 어려움이 따를 수 있다는 점이다. 그리고 설계에 있어서 유연성이 떨어지는 점도 있다. 특히 LSI는 한 번 만들어지고 나면 수정이 되지 않기 때문에 여러 테이블을 모아놓은 하나의 테이블의 설계가 적합하지 않는 경우에 수정이 어렵다. 또한 이 방법 자체에 대한 이해와 적용이 까다롭다는 점도 한계라고 볼 수 있을 것 같다. Reference The What, Why, and When of Single-Table Design with DynamoDB Advanced Design Patterns for Amazon DynamoDB Single Table - Week 4 | Coursera AWS re:Invent 2019: Data modeling with Amazon DynamoDB (CMY304) - YouTube AWS DynamoDB Docs: Core Components of Amazon DynamoDB AWS DynamoDB Docs: Partitions and Data Distribution AWS DynamoDB Docs: Improving Data Access with Secondary Indexes","link":"/posts/database/dynamodb-single-table-design/"},{"title":"etcd deep dive - Client Model","text":"etcd를 사용하는 개발자들은 etcd가 공식적으로 제공해주는 클라이언트를 사용해 서버에 접근하는 것이 일반적이다. Go로 만들어진 클라이언트를 잘 관리해주고 있어서 보통은 이 클라이언트를 쓰는 것 같다. 지금 진행하는 프로젝트 역시 Go 클라이언트를 통해 etcd에 접근한다. 이번 글은 클라이언트가 어떻게 발전해 왔는지를 알려주는 Learning의 글을 번역하고 공부했던 내용을 간단히 정리했다. Requirements클라이언트의 구현체는 다음과 같은 요구사항을 가지고 있다 Correctness: 서버 실패와 상관없이 일관성 보장을 훼손하면 안 된다. 예를 들어 글로벌 순서, 손상된 데이터를 쓰지 않는 것, 최대 한 번(At Most Once) 동작하는 Mutable Operation, 부분적인 데이터를 Watch 하지 않는 등의 동작을 의미한다. Liveness: 서버는 간단히 실패하거나 연결이 끊어질 수 있는데, 클라이언트는 특별한 설정으로 통제하고 있는 게 없다면 문제 상황에서 서버가 다시 회복될 때를 기다리며 DeadLock이 발생하지 않도록 해야 한다. Effectiveness: 최소의 리소스를 사용해야 한다. 예를 들어 TCP 컨넥션은 엔드포인트가 교체되면 안전하게 정리되어야 한다. Portability: 공식적인 클라이언트의 형태가 문서화 되어있고, 여러 다른 언어에서 이를 구현할 수 있어야 한다. 또한 다른 언어들 사이의 에러 핸들링 방식이 일관되어야 한다. 요구사항의 많은 부분이 gRPC 클라이언트의 동작으로 인해 해결될 수도 있겠다고 생각했다. Client Overview클라이언트는 다음 세 가지 컴포넌트로 구성되어 있다. Balancer: etcd 클러스터와 gRPC 컨넥션을 만드는 컴포넌트 API Client: RPC를 etcd 서버로 보내는 컴포넌트 Error Handler: gRPC 에러를 처리하며 엔드포인트를 변경할 것인지, 재시도할 것인지를 결정하는 컴포넌트 API Client와 Error Handler의 경우는 v3로 오면서 큰 변화가 없었는지, 이하 내용에서는 밸런서의 변화에 관해서만 이야기한다. 밸런서의 변화는 클러스터 구성과 연결을 어떻게 할 것 인지를 결정할 때 도움을 줄 것 같다. clientv3-grpc1.0Overview여러 엔드포인트 컨넥션을 유지하지만 하나의 엔드 포인트와 Pinned Connection을 가정하고 통신한다. 즉, 모든 클라이언트의 요청을 하나의 클라이언트에 먼저 보내는 구조다. 이 상황에서 장애가 발생하면 밸런서는 다른 엔드포인트를 선택해서 연결하거나 재시도하게 된다. Limitation이렇게 구성하게 되면 여러 엔드포인트에 대해 연결을 유지하므로 더 빠른 FailOver(페일오버)가 가능하지만, 더 많은 리소스를 사용하고 있게 된다. 또한 밸런서가 노드의 상태나 클러스터 멤버십 상태에 대해 이해할 수 없기 때문에 Network Partition이 발생한 노드에 갇힐 수 있다. clientv3-grpc1.7Overview여러 엔드포인트 중 하나의 엔드포인트에만 TCP 컨넥션을 구성한다. 처음 연결할 때 클라이언트는 주어진 모든 엔드포인트에 컨넥션을 시도하는데, 가장 빠르게 연결된 컨넥션이 나오면 해당 주소를 Pinned Connection으로써 사용하고, 나머지 연결을 종료한다. Pinned 주소는 연결이 에러에 의해 닫힐 때까지 유지된다. 에러가 발생한다면 클라이언트의 에러 핸들러가 이를 받아 재시도할 수 있는 경우인지 아닌지 판단해서 다음 동작을 적절히 수행한다. Stream RPC인 Watch, KeepAlive는 타임아웃 설정 없이 보내지는 경우가 많은데, 클라이언트는 HTTP/2 PING을 통해 서버의 상태를 체크한다. 서버의 응답이 없으면 새로운 서버에 연결한다. 장애로 인해 Pinned Connection 상태에서 내려갔든, 모종의 이유로 새로운 연결을 시도하든 건강하지 않다고 판단된 엔드포인트는 Unhealthy List에 등록된다. 만약 해당 리스트에 올라가게 되면 기본값으로 5초 동안은 해당 엔드포인트를 사용하지 않게 된다. 위 동작은 grpc-1.0 버전 당시에도 마찬가지로 있었다고 한다. Limitationgrpc-1.0 버전처럼 여전히 멤버십 정보는 알지 못하므로 동일하게 네트워크 파티션을 판단 못 하는 문제가 발생할 수 있다. 그리고 밸런서가 잘못된 Unhealthy List를 관리하는 문제가 있을 수 있다. 예를 들어 Unhealthy 마킹을 하고 나서 바로 서버가 정상화되었다면 5초 동안 건강한 서버를 사용하지 못하는 문제가 생긴다. 또한 Unhealthy로 관리되는 Recovery 과정이 gRPC의 Dial을 사용하고 하드 코딩된 부분이 있어 굉장히 복잡한 구현체였다고 한다. 위 문제는 사실 이번 버전의 문제는 아니고 이전 버전과 동일한 구현으로 인해 생기는 문제이다. 이번 버전은 컨넥션을 위한 리소스 소비를 줄였지만, 페일오버의 속도를 느리게 만든다는 단점이 생긴다. 하위 버전과 비교했을 때 페일오버 문제는 트레이드 오프라고 생각된다. clientv3-grpc1.23Overviewgrpc1.7 버전은 gRPC 인터페이스와 강하게 결합하여 gRPC 버전을 올릴 때마다 클라이언트의 동작이 망가지기 일쑤였고, 개발 과정의 많은 부분이 이를 호환되게 하는 수정이었다고 한다. 결과적으로 구현체는 복잡해지는 문제가 지속되었다. 개발 과정에서도 gRPC 메인테이너가 과거 버전의 인터페이스를 유지하지 말 것을 권장했다고 한다. grpc1.23으로 오면서 가장 주안점으로 둔 점은 밸런서의 페일오버 로직을 단순화하는 것이었다. Unhealthy List를 관리하지 않고 현재 사용 중인 엔드포인트에 문제가 생기면 다른 엔드포인트로 라운드로빈 하도록 바꿨다. 이에 따라 복잡한 상태 체크가 필요 없어졌다. 다른 포인트는 gRPC의 인터페이스와 강결합하지 않도록 바꾸는 것이었다. 관련된 내용은 자세히 다루지 않는데, 내부적인 변화가 많이 있었다고 한다. 이로써 Backward Compatibility(하위 호환성)를 유지하면서 gRPC 업그레이드에 쉽게 깨지지 않도록 구성했다. 컨넥션 방법에도 변경이 생겼는데, 여러 엔드포인트가 주어졌을 때 클라이언트는 다시 여러 sub-connection을 만드는 방법으로 바뀌었다. 한 서브 컨넥션은 각 엔드포인트를 의미하는 gRPC의 인터페이스(gRPC SubConn)이다. 5개의 노드가 있다면 5개의 TCP 컨넥션 풀을 만든다. 초기 버전에서 설명한 것처럼 TCP 컨넥션은 자원을 더 소모하지만, 더 유연한 페일오버가 가능해진다. 대신 grpc1.0과는 다르게 Pinned Connection을 사용하지 않고 모든 연결 포인트에 요청을 분산했다. 이로써 더 공평하게 부하를 분산할 수 있게 되었다. 기본적으로는 라운드 로빈을 사용하고 있는데 gRPC처럼 갈아 끼울 수 있다. grpc1.7과 High Level은 유사하지만, 내부적으로 gRPC의 기능을 많이 활용하고 있다. 밸런서는 gRPC의 resolver group을 사용하고 Balancer Picker Policy를 구현해서 복잡한 동작을 gRPC가 수행하도록 위임했다. Retry의 경우도 gRPC의 인터셉터를 통해 처리함으로써 체인 안에서 자동으로, 그리고 보다 정교한 방법으로 처리되도록 했다. 과거 버전에서는 Retry 로직이 직접 구현되어 복잡한 부분이 있었다고 한다. Limitation이 버전은 현재 구현 상태이기 때문에, 앞으로 어떻게 발전시킬 것인지를 중점적으로 설명했다. 우선 현재 각 엔드포인트 상태를 캐싱함으로써 성능적 향상이 가능하다고 한다. 예를 들어 TCP 연결을 유지하기 때문에 PING을 보내고 Health가 보장되는 앤드포인트를 우선하도록 만들 수 있다. 하지만 Unhealthy List를 관리하는 것처럼 복잡도 증가 우려가 있기 때문에 논의가 더 필요한 주제라고 한다. 그리고 클라이언트 사이드의 KeepAlive PING을 여전히 사용 중인데, 네트워크 파티션 등 클러스터 멤버십을 고려한 Health Check가 필요하다. (관련 논의) 마지막으로 Retry를 인터셉터에 의해 처리되도록 하고 있는데, 이후 공식적인 gRPC 스펙으로서 처리할 수도 있다. 아직 관련된 gRPC의 Retry는 Proposal 상태인 것 같다. Reference https://etcd.io/docs/v3.5/learning/design-client/","link":"/posts/database/etcd-client-model/"},{"title":"MongoDB 모델링","text":"최근에 시험도 준비하고, 일도 바빠서 쉽게 글을 남기기 힘들었는데, 쓸 내용들은 차곡차곡 쌓아두긴 했었다. 우선 정리와 복습도 할 겸, 최근 서비스에서 사용하고 있는 MongoDB 모델링 하는 걸 공부한 내용을 정리했다. 이 내용은 공식 문서를 보고 번역하고 재배열한 내용이다. Flexible SchemaMongoDB는 어떤 데이터를 집어 넣을지 미리 결정되어 있는 “테이블”과 다르게, 스키마에 대해 자유로운 document(다큐멘트)가 모이는, collection(콜렉션)으로 구성된다. 하나의 콜랙션에 있는 다큐멘트들은 같은 필드나, 같은 데이터 타입을 가질 필요가 없다. 필드를 더하거나, 필드의 데이터 타입을 바꾸는 등, 다큐멘트의 구조를 바꾸려면, 그 특정 다큐멘트의 구조를 바꿔주면 된다. 실제로는 document rules를 강제해 하나의 콜렉션이 유사한 구조를 공유하도록 강제한다. Document Structure“MongoDB란 무엇일까?”에 대한 내용을 다루는 글이 아니므로, 스키마의 특징, 데이터 타입 등은 건너 뛰고, 다큐멘트 구조에 대해서 한 번 살펴보자. MongoDB 앱의 데이터 모델을 설계할 때 중요한 사항은 다큐멘트의 구조와 애플리케이션이 데이터 관계를 어떻게 나타내는지이다. MongoDB는 관련 데이터를 하나의 다큐멘트 안에 포함하도록 한다. 두 가지 방법으로 관계에 대한 표현을 할 수 있는데, 하나의 다큐멘트 안에 포함시키는 Embedded 방식, 참조하도록 하는 Reference 방식이다. Embedded DataEmbedded Data 모델에서는 유관한 데이터를 하나의 다큐멘트에 담는다. 이러한 스키마는 일반적으로 “비정규화” 모델로 알려져있다. 임베디드된 데이터 모델들은 애플리케이션이 유관한 정보 조각들을 하나의 데이터베이스 레코드에 저장할 수 있게 한다. 결과적으로 애플리케이션은 더 적은 쿼리와 업데이트를 가지고 필요한 작업을 수행할 수 있게 된다. 엔티티 사이에 포함 관계가 있는 경우 (1:1 관계) Embedded Document Pattern“고객”과 주소 관계를 이어주는 예시를 생각해보자. address는 patron에게 종속되는 관계이다. 정규화 되어있다면, 아래와 같은 모습을 보인다. 1234567891011121314// patron document{ _id: &quot;joe&quot;, name: &quot;Joe Bookreader&quot; } // address document { patron_id: &quot;joe&quot;, street: &quot;123 Fake Street&quot;, city: &quot;Faketon&quot;, state: &quot;MA&quot;, zip: &quot;12345&quot; } 이 경우, 만약 address의 데이터가 자주 이름 정보와 함께 검색되는 편이라면, 레퍼런스를 참조하기 위해 여러 쿼리를 실행하게 된다. 해결 방안은 address 데이터를 patron 데이터에 임베드 하는 것이다. 임베디드 데이터 모델은 애플리케이션에서 완전한 patron 정보를 가져오기 위해 한 번의 쿼리만 사용할 수 있다. 12345678910{ _id: &quot;joe&quot;, name: &quot;Joe Bookreader&quot;, address: { street: &quot;123 Fake Street&quot;, city: &quot;Faketon&quot;, state: &quot;MA&quot;, zip: &quot;12345&quot; }} Subset Pattern Embedded Document Pattern은 애플리케이션이 사용할 때 별로 필요 없는 정보를 포함한 큰 다큐멘트를 만들 수 있다는 문제가 있다. 이런 불필요한 데이터는 추가적인 로드를 야기하고, 당연히 읽기 퍼포먼스를 떨어뜨릴 수 있다. 대신, 자주 접근하는 정보를 모아 Subset 패턴틀 사용할 수 있다. 영화 정보를 보여주는 애플리케이션을 생각해보자. 데이터베이스는 아래와 같은 movie 스키마를 가지고 있다. 123456789101112131415161718192021222324252627{ _id: 1, title: &quot;The Arrival of a Train&quot;, year: 1896, runtime: 1, released: ISODate(&quot;01-25-1896&quot;), poster: &quot;http://ia.media-imdb.com/images/M/MV5BMjEyNDk5MDYzOV5BMl5BanBnXkFtZTgwNjIxMTEwMzE@._V1_SX300.jpg&quot;, plot: &quot;A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, ...&quot;, fullplot: &quot;A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, the line dissolves. The doors of the railway-cars open, and people on the platform help passengers to get off.&quot;, lastupdated: ISODate(&quot;2015-08-15T10:06:53&quot;), type: &quot;movie&quot;, directors: [ &quot;Auguste Lumière&quot;, &quot;Louis Lumière&quot; ], imdb: { rating: 7.3, votes: 5043, id: 12 }, countries: [ &quot;France&quot; ], genres: [ &quot;Documentary&quot;, &quot;Short&quot; ], tomatoes: { viewer: { rating: 3.7, numReviews: 59 }, lastUpdated: ISODate(&quot;2020-01-09T00:02:53&quot;) }} movie 콜렉션은 애플리케이션이 간단한 영화의 Overview를 보여줄 때 사용하지 않는 정보들을 가지고 있다. 예를 들어서, fullPlot, rating 관련된 정보들이 그렇다고 가정하자. 이 경우, 모든 영화 데이터를 하나의 콜렉션에 저장하기 보다는 두 콜렉션으로 나눌 수 있다. 123456789101112// 영화 기본 정보를 담고 있는 movie 콜렉션. 애플리케이션이 기본적으로 불러오게 되는 데이터{ _id: 1, title: &quot;The Arrival of a Train&quot;, year: 1896, runtime: 1, released: ISODate(&quot;1896-01-25&quot;), type: &quot;movie&quot;, directors: [ &quot;Auguste Lumière&quot;, &quot;Louis Lumière&quot; ], countries: [ &quot;France&quot; ], genres: [ &quot;Documentary&quot;, &quot;Short&quot; ],} 123456789101112131415161718192021// movie_detail 콜렉션은 부가적이고, 덜 접근이 되는 정보를 담는다.{ _id: 156, movie_id: 1, // reference to the movie collection poster: &quot;http://ia.media-imdb.com/images/M/MV5BMjEyNDk5MDYzOV5BMl5BanBnXkFtZTgwNjIxMTEwMzE@._V1_SX300.jpg&quot;, plot: &quot;A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, ...&quot;, fullplot: &quot;A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, the line dissolves. The doors of the railway-cars open, and people on the platform help passengers to get off.&quot;, lastupdated: ISODate(&quot;2015-08-15T10:06:53&quot;), imdb: { rating: 7.3, votes: 5043, id: 12 }, tomatoes: { viewer: { rating: 3.7, numReviews: 59 }, lastUpdated: ISODate(&quot;2020-01-29T00:02:53&quot;) }} 이 방법은 더 적은 데이터를 일반적인 요청 상황에서 불러오기 때문에, 읽기 퍼포먼스를 향상시킨다. 애플리케이션에서 필요하다면, movie collection의 데이터를 불러올 수 있게 구성한다. 이 방식에서는 Trade-off가 존재한다. 자주 접근되는 데이터를 담은 더 작은 다큐멘트들은 Working set의 전체적인 크기를 줄여주고, 읽기 퍼포먼스 향상, 그리고 애플리케이션 메모리를 절약해준다. 하지만, 애플리케이션이 데이터를 읽어오는 방식에 대한 이해도가 필요하다. 만약 데이터를 여러 콜렉션으로 부적절하게 나눈다면, 애플리케이션은 필요한 데이터를 가져오기 위해 JOIN을 수행하게 될 수 있다. 게다가, 데이터를 많은 조각으로 나누면, 어떤 데이터가 어떤 콜렉션에 저장되어 있는지 추적하기 어려워지기 때문에 필요한 데이터베이스 유지보수가 증가할 수 있다. 엔티티 사이에 1:N 관계가 있고, N이 1 부분에 항상 보여야 할 때이 경우에도 위와 같이, Embedded Document Pattern과, Subset Pattern이 있다. 구현 방식 역시 유사하다. Embedded Document Pattern아래 예시에서는 고객과 여러 주소 관계를 표현하고 있다. 많은 데이터 엔티티들을 다른 하나의 컨텍스트 안에서 봐야 할 때, 임베딩 하는 것이 레퍼런스 하는 경우보다 나은 경우를 설명한다 (안 그런 경우도 있다). patron과 address 사이의 1:N 관계에서, patron은 복수의 address를 가지고 있다. 정규화 되어있는 경우, address 다큐멘트는 patron 다큐멘트를 참조하고 있다. 12345678910111213141516171819202122// 정규화 된 데이터{ _id: &quot;joe&quot;, name: &quot;Joe Bookreader&quot;}// address documents{ patron_id: &quot;joe&quot;, // reference to patron document street: &quot;123 Fake Street&quot;, city: &quot;Faketon&quot;, state: &quot;MA&quot;, zip: &quot;12345&quot; } { patron_id: &quot;joe&quot;, street: &quot;1 Some Other Street&quot;, city: &quot;Boston&quot;, state: &quot;MA&quot;, zip: &quot;12345&quot; } 만약 애플리케이션이 address 데이터를 이름 정보와 함께 조회하는 경우가 많다면, 참조된 데이터를 가져오기 위해 여러번의 쿼리를 수행해야 한다. 더 최적화된 스키마는 address 데이터를 patron에 임베드 시키는 것이다. 123456789101112131415161718{ _id: &quot;joe&quot;, name: &quot;Joe Bookreader&quot;, addresses: [ { street: &quot;123 Fake Street&quot;, city: &quot;Faketon&quot;, state: &quot;MA&quot;, zip: &quot;12345&quot; }, { street: &quot;1 Some Other Street&quot;, city: &quot;Boston&quot;, state: &quot;MA&quot;, zip: &quot;12345&quot; } ] } Subset PatternEmbbed Document Pattern의 문제는 거대한 다큐멘트를 만들 수 있다는 점이다. 이런 경우, Subset Pattern을 사용해서 오직 애플리케이션이 요구하는 데이터에만 접근하도록 수 있다. 프로덕트에 리뷰를 달 수 있는 커머스 사이트를 예로 들어보자. 123456789101112131415161718192021222324252627{ _id: 1, name: &quot;Super Widget&quot;, description: &quot;This is the most useful item in your toolbox.&quot;, price: { value: NumberDecimal(&quot;119.99&quot;), currency: &quot;USD&quot; }, reviews: [ { review_id: 786, review_author: &quot;Kristina&quot;, review_text: &quot;This is indeed an amazing widget.&quot;, published_date: ISODate(&quot;2019-02-18&quot;) }, { review_id: 785, review_author: &quot;Trina&quot;, review_text: &quot;Nice product. Slow shipping.&quot;, published_date: ISODate(&quot;2019-02-17&quot;) }, ... { review_id: 1, review_author: &quot;Hans&quot;, review_text: &quot;Meh, it's okay.&quot;, published_date: ISODate(&quot;2017-12-06&quot;) } ]} 리뷰는 최신순으로 정렬되어있다. 프로덕트 페이지에 들어가면, 애플리케이션은 10개의 가장 최근 리뷰를 보여준다. 이 경우 모든 리뷰를 하나의 콜렉션에 담을 필요는 없다. 그 대신, 콜렉션을 두 개로 나눌 수 있다. 12345678910111213141516171819202122// 10개의 최신 리뷰와 함게, 각 프로덕트 정보를 저장하느 product 콜렉션{ _id: 1, name: &quot;Super Widget&quot;, description: &quot;This is the most useful item in your toolbox.&quot;, price: { value: NumberDecimal(&quot;119.99&quot;), currency: &quot;USD&quot; }, reviews: [ { review_id: 786, review_author: &quot;Kristina&quot;, review_text: &quot;This is indeed an amazing widget.&quot;, published_date: ISODate(&quot;2019-02-18&quot;) } ... { review_id: 776, review_author: &quot;Pablo&quot;, review_text: &quot;Amazing!&quot;, published_date: ISODate(&quot;2019-02-16&quot;) } ] } 1234567891011121314151617181920212223// 모든 리뷰들을 저장하는 review 콜렉션, 각 리뷰들은 product를 참조한다.{ review_id: 786, product_id: 1, review_author: &quot;Kristina&quot;, review_text: &quot;This is indeed an amazing widget.&quot;, published_date: ISODate(&quot;2019-02-18&quot;)}{ review_id: 785, product_id: 1, review_author: &quot;Trina&quot;, review_text: &quot;Nice product. Slow shipping.&quot;, published_date: ISODate(&quot;2019-02-17&quot;)}...{ review_id: 1, product_id: 1, review_author: &quot;Hans&quot;, review_text: &quot;Meh, it's okay.&quot;, published_date: ISODate(&quot;2017-12-06&quot;)} 최근 10개의 리뷰만 product 콜랙션에 담아두는 것을 통해서, 필요한 부분만 production 콜랙션에 요청을 했을 때 받을 수 있다. 만약 유저가 추가적인 리뷰를 보고 싶어 하면, 애플리케이션은 review 콜랙션에서 이를 가져오면 된다. 마찬가지로, 이 경우의 Subset Pattern에서도 Trade off가 존재한다. 더 자주 접근 되는 데이터만 담고 있는 작은 다큐멘트를 쓰는 것은 working set의 전체 사이즈를 줄여준다. 이러한 더 작은 다큐멘트들은 결과적으로 읽기 퍼포먼스를 향상시킨다. 하지만, Subset 패턴은 데이터 중복으로 이어진다. 예를 들어서, review는 product와 review 콜랙션 모두에게 있다. 추가적인 단계들이 각 콜랙션 사이에 동일성을 유지하기 위해 필요하다. 또한 애플리케이션에서 제한된 Subset 수를 유지하기 위한 추가적인 로직이 필요하다. (최근 리뷰가 들어오면, 마지막 리뷰를 빼주는 등) 일반적으로 임베딩 하는 것이 읽기 작업에서 더 나은 퍼포먼스를 보여준다. 임베디드 데이터 모델은 연관된 데이터를 업데이트 하는 것을 하나의 쓰기 작업으로 가능하게 만든다. Normalized Data Model (Reference)정규화된 데이터 모델은 관계를 reference를 통해 표현한다. 일반적으로, 정규화된 데이터 모델은 다음과 같은 상황에서 사용한다. 1:N 관계에서 임베딩 하는 것이 데이터 중복을 일으키고, 충분한 읽기 퍼포먼스를 제공하지 못하는 경우 더 복잡한 N:M 관계를 표현해야 할 때 큰 계층적 데이터 셋을 표현해야 할 때 문서에서는 N:M에 대한 부분을 직접 언급하는 페이지는 없었고, 1:N 관계와, 트리구조라는 형태로 분류되어있다. 1:N 관계publisher와 book의 관계를 연결하는 예시를 살펴보자. 아래 예시의 경우는 publisher 정보의 반복을 피하기 위해서는 임베딩 하는 것보다 참조를 사용하는 것이 더 좋다는 내용이다. 임베딩 된 경우를 확인해보면 아래와 같다. publisher에 대한 정보가 book 다큐멘트 안에서 중복되고 있다는 것을 알 수 있다. 12345678910111213141516171819202122232425{ title: &quot;MongoDB: The Definitive Guide&quot;, author: [ &quot;Kristina Chodorow&quot;, &quot;Mike Dirolf&quot; ], published_date: ISODate(&quot;2010-09-24&quot;), pages: 216, language: &quot;English&quot;, publisher: { name: &quot;O'Reilly Media&quot;, founded: 1980, location: &quot;CA&quot; } }{ title: &quot;50 Tips and Tricks for MongoDB Developer&quot;, author: &quot;Kristina Chodorow&quot;, published_date: ISODate(&quot;2011-05-06&quot;), pages: 68, language: &quot;English&quot;, publisher: { name: &quot;O'Reilly Media&quot;, founded: 1980, location: &quot;CA&quot; }} 이런 중복을 피하기 위해서 reference를 사용한다. 참조를 사용할 때 관계들의 증가에 따라 참조를 저장할 위치를 결정한다. 위 예시 상황을 말 해보자면, publisher 당 book의 수가 조금만 증가 하는 경우, book 참조를 publisher 다큐멘트 안에 넣는것이 좋다. 그렇지 않다면, 즉, publisher 당 book의 수가 제한 없이 커진다면, 이 데이터 모델은 아래 예시처럼 변경 가능하고 증가 하는 배열 형태를 가져야 한다. 123456789101112131415161718192021222324{ name: &quot;O'Reilly Media&quot;, founded: 1980, location: &quot;CA&quot;, books: [123456789, 234567890, ...] }{ _id: 123456789, title: &quot;MongoDB: The Definitive Guide&quot;, author: [ &quot;Kristina Chodorow&quot;, &quot;Mike Dirolf&quot; ], published_date: ISODate(&quot;2010-09-24&quot;), pages: 216, language: &quot;English&quot;}{ _id: 234567890, title: &quot;50 Tips and Tricks for MongoDB Developer&quot;, author: &quot;Kristina Chodorow&quot;, published_date: ISODate(&quot;2011-05-06&quot;), pages: 68, language: &quot;English&quot;} 트리 구조 Parent ReferencesParent References 패턴은 각 트리 노드를 다큐멘트에 저장한다. 트리 노드 외에도 문서는 노드의 부모 ID를 저장한다. 위 이미지 형태의 구조를 아래와 같이 저장한다고 볼 수 있다. 12345678db.categories.insertMany( [ { _id: &quot;MongoDB&quot;, parent: &quot;Databases&quot; }, { _id: &quot;dbm&quot;, parent: &quot;Databases&quot; }, { _id: &quot;Databases&quot;, parent: &quot;Programming&quot; }, { _id: &quot;Languages&quot;, parent: &quot;Programming&quot; }, { _id: &quot;Programming&quot;, parent: &quot;Books&quot; }, { _id: &quot;Books&quot;, parent: null }] ) 노드의 부모를 검색할 때는 다음과 같이 검색할 수 있다. 1db.categories.findOne( { _id: &quot;MongoDB&quot; } ).parent 부모 필드를 통해 children을 검색할 수 있다. 1db.categories.find( { parent: &quot;Databases&quot; } ) Child ReferencesChild References 패턴은 각 트리 노드를 하나의 다큐멘트에 저장한다. 트리 노드 데이터에 자식 노드들의 id 배열을 저장한다. 즉 아래와 같이 위 이미지 형태의 트리 구조를 저장한다는 뜻이다. 12345678db.categories.insertMany( [ { _id: &quot;MongoDB&quot;, children: [] }, { _id: &quot;dbm&quot;, children: [] }, { _id: &quot;Databases&quot;, children: [ &quot;MongoDB&quot;, &quot;dbm&quot; ] }, { _id: &quot;Languages&quot;, children: [] }, { _id: &quot;Programming&quot;, children: [ &quot;Databases&quot;, &quot;Languages&quot; ] }, { _id: &quot;Books&quot;, children: [ &quot;Programming&quot; ] }] ) 자식 노드들을 찾을 때는 아래와 같이 쿼리를 하게 된다. 1db.categories.findOne( { _id: &quot;Databases&quot; } ).children 특정 자식 노드 값을 가지고 있는 부모 값을 찾기 위해서는 다음과 같은 쿼리를 사용한다. 1db.categories.find( { children: &quot;MongoDB&quot; } ) Children References 패턴은 하위 트리에 대한 작업이 필요하지 않다면 적합한 방법이다. 이 패턴은 노드가 여러 개의 부모를 가질 수 잇는 경우에도 적합한 방법이 될 수 있다. Array of AncestorsArray of Ancestors 패턴은 각 트리 노드를 하나의 다큐멘트에 담는 방법이다. 트리 노드 데이터에, 노드의 조상들 또는 경로의 id 배열을 저장하는 방법이다. 위 이미지 형태를 아래와 같이 저장하는 방법이다. 12345678db.categories.insertMany( [ { _id: &quot;MongoDB&quot;, ancestors: [ &quot;Books&quot;, &quot;Programming&quot;, &quot;Databases&quot; ], parent: &quot;Databases&quot; }, { _id: &quot;dbm&quot;, ancestors: [ &quot;Books&quot;, &quot;Programming&quot;, &quot;Databases&quot; ], parent: &quot;Databases&quot; }, { _id: &quot;Databases&quot;, ancestors: [ &quot;Books&quot;, &quot;Programming&quot; ], parent: &quot;Programming&quot; }, { _id: &quot;Languages&quot;, ancestors: [ &quot;Books&quot;, &quot;Programming&quot; ], parent: &quot;Programming&quot; }, { _id: &quot;Programming&quot;, ancestors: [ &quot;Books&quot; ], parent: &quot;Books&quot; }, { _id: &quot;Books&quot;, ancestors: [ ], parent: null }] ) 조상들 또는 경로를 찾기 위해 다음과 같이 쿼리를 사용할 수 있다. 1db.categories.findOne( { _id: &quot;MongoDB&quot; } ).ancestors 자손들을 모두 찾기 위해서 다음과 같이 쿼리를 사용할 수 있다. 1db.categories.find( { ancestors: &quot;Programming&quot; } ) Array of Ancestors 패턴은 빠르고 효율적으로 자손과 조상들을 찾을 수 있게 해준다. Array of Ancestors 패턴은 Materialized Paths 패턴보다는 약간 느리지만, 좀 더 직관적으로 사용할 수 있다. Reference https://docs.mongodb.com/manual/core/data-modeling-introduction/","link":"/posts/database/mongodb-modeling/"},{"title":"etcd deep dive - Data Model","text":"etcd 공식 페이지에 가보면 “A distributed, reliable key-value store for the most critical data of a distributed system”라고 설명하고 있다. ZooKeeper와 유사하지만 gRPC를 베이스로 하는 현대적인 코디네이터 역할을 한다. 메타 데이터를 담기 위한 Key-Value 저장소로 사용이 되는 편이고 가장 유명한 활용처는 쿠버네티스가 아닐까 싶다. 최근 사용할 일이 생기고 있어서 깊게 공부해보려고 하나씩 파헤치고 있다. 첫 글은 etcd의 데이터 모델이다. etcd는 고맙게도 공식 페이지나 CNCF 발표 영상 등에서 구체적인 디자인들에 대해 여러 방법으로 알려주고 있다. 이번 글은 etcd의 공식 문서의 Learning 섹션에서 제공해주는 etcd data model을 정리한 글이다. Overviewetcd는 멀티 버전 Key-Value 스토리지이다. 즉, 이전 버전의 키값 쌍을 새 값으로 대체하기 전까지 보존한다. etcd는 여러 버전의 데이터를 효율적이고 불변 데이터 형태로 관리한다. 불변 데이터라고 하면 In-Place 형태로 디스크의 데이터를 업데이트하지 않고 아니라 항상 새로운 버전을 만드는 것을 말한다. In-Place 방식보다 공간 측면에서는 덜 효율적일 것 같긴 한데 “효율적”이라는 키워드를 사용한 이유는 이후 후술한다. 아무튼 이렇게 과거 버전도 모두 가지고 있게 되므로 특정 키에 대한 과거 버전 데이터들은 나중에도 접근이 가능하고 Watchable 하다. 한편으로는 과거 버전이 무한히 늘어나는 것을 막기 위해 Compaction을 진행하기도 한다. Logical Viewetcd를 논리적인 측면에서 봤을 때 간단히 말하자면 바이너리 키 공간이다. 키는 복수의 Revision(리비전)을 가지고 있을 수 있다. 리비전은 스토리지의 트랜잭션 번호라고 생각해도 된다. 정수 형태이며 스토어가 세팅되면 초기 리비전 값은 1부터 시작하게 된다. Atomic 한 요청 단위(트랜잭션)이 수행될 때마다 새로운 리비전으로 값을 쓰게 된다. 즉, 트랜잭션마다 리비전이 단조 증가하게 된다. 과거의 리비전 역시 스토리지 내에서 일정 기간 보유하고 있어서 실제 쿼리를 통해 접근이 가능하며, 리비전 정보 역시 인덱싱되어 있어서 특정 리비전을 기준으로 쿼리하거나 Watch 하는 것도 빠르게 처리할 수 있다. 만약 저장소가 압축을 수행하면 압축 이전의 리비전들은 모두 삭제되며 쿼리할 수 없게 된다. 123456$ etcdctl compact 5compacted revision 5# 수동으로 리비전을 압축하면 그 이후 리비전을 기준으로 쿼리할 때 찾을 수 없다는 rpc 에러를 만난다.$ etcdctl get --rev=4 fooError: rpc error: code = 11 desc = etcdserver: mvcc: required revision has been compacted 그러면 리비전 Overflow가 생길 수 있는 건가? 라고 생각했는데, 역시나 이런 생각을 한 사람이 있었고 이슈에서 찾아볼 수 있었다. 그러나 그러한 걱정은 하덜 말라는 답변. 초당 2만 번씩 53억 년 동안 Mutation이 발생해야 오버플로우가 발생한다고 한다. etcd는 단순히 put과 delete로 스토리지 안의 키를 변경한다. 키에 대한 값은 리비전에 대한 정보뿐 아니라 키의 변경 버전을 기록한다. Protocol Buffer 메시지 타입은 다음과 같이 생겼다. 12345678message KeyValue { bytes key = 1; int64 create_revision = 2; int64 mod_revision = 3; int64 version = 4; bytes value = 5; int64 lease = 6;} 키를 생성하면 키의 버전이 증가하는 것과 동일하다. 만약 기존에 해당 키가 존재하지 않았다면 1부터 시작하게 하는 것이고 기존에 해당 키로 이미 값이 존재한다면 그 키에 대해 버전을 올리는 것과 같다. 키를 삭제하는 것은 해당 키에 대한 Tombstone을 만들고 해당 키의 버전을 0으로 만드는 것과 같다. 압축이 발생하면 압축 이전의 모든 세대가 제거되고 가장 최근 버전만 남게 된다. 글에서는 생명 주기로 표현하는데, 굳이 생명 주기라고 할만한 건 없는 것 같다. 그냥 키가 리비전을 통해 여러 버전을 가지고 있을 수 있고 해당 변경 사항들이 불변 데이터처럼 쌓인다는 게 본질적인 내용이다. Tombstone은 Append Only Log 같은 데이터에서 삭제 표기를 하기 위해 사용하는 데이터이다. Physical View실제 구현 레벨에서 etcd는 크게 두 가지 레이어를 통해 스토리지를 구성한다. Persistent B+tree In-memory B Tree Persistent B+Tree영구 저장소로 B+Tree를 사용하고 있다. B+Tree에 키를 저장할 때 다음과 같이 3개의 튜플로 키를 만들어준다. Major: 해당 키를 들고 있는 Revision을 의미한다. Sub: 같은 리비전 내의 다른 키들과 구분을 위해 사용된다. (아마 개발자가 설정하는 값이 아닐까) Type: 특수한 값을 위한 Optional 한 값이다. (ex. tombstone) 키에 대한 값은 효율성을 위해 이전 리비전과의 차이(delta)만 담고 있다. B+Tree는 키를 단순 Byte Order 로 정렬한다. 따라서 특정 리비전으로부터 다른 리비전의 수정 사항을 빠르게 찾을 수 있다. 컴팩션이 발생하면 Outdated Key-Value 쌍은 삭제되는 구조이다. Compaction은 위에서처럼 수동으로 수행할 수도 있지만 특정 시점마다 컴팩션을 수행하도록 할 수도 있다. Compaction이 키가 사용하는 스토리지 공간을 줄여주는 것은 맞지만, 데이터를 쓰고 지우면서 생기는 단편화 문제를 해결해주지는 않는다. 단편화를 해결하려면 defragmentation(defrag 명령어)을 해줘야 한다. 이 글에서 해당 내용을 깊게 다루지는 않으므로 문서를 참고하면 좋을 것 같다. In-memory B Treeetcd는 일반적인 데이터베이스들처럼 캐시 레이어를 가지고 있는데, 이 부분이 In-memory B Tree이다. B Tree 인덱스의 키들이 실제 유저들(개발자)에게 노출되는 키이다. 해당 B Tree의 값으로는 키에 대한 Persistent B+Tree의 수정 내역을 가리키는 포인터를 들고 있다. Compaction이 발생하면 삭제되었던 키들(dead pointers)을 삭제하게 된다. Reference https://etcd.io/docs/v3.5/learning/data_model/ https://etcd.io/docs/v3.5/op-guide/maintenance/","link":"/posts/database/etcd-data-model/"},{"title":"RDB 스케일링","text":"RDB는 흔히 말하길 스케일링 (스케일 아웃) 하기 까다로운 데이터베이스라고들 한다. NoSQL이 등장하며 내세웠던 차별점 역시 이러한 부분(확장성)이 포함되어있다. 하지만 RDB가 스케일 아웃이 불가능하다는 건 절대 아니다. 많은 거대한 서비스들이 RDB를 사용하고 있고, 이 서비스들은 많은 방법으로 스케일 아웃을 구현하고 있다. 이 방법에 대해서 정리한 글이다. 우선 글 내용에서 구체적인 부분들은 InnoDB 스토리지 엔진의 케이스를 다루고 있다. 핵심적인 원리에 대해서는 사실 모든 RDB에서 같다고 생각된다. 거대한 테이블의 문제점스케일 아웃이 필요한 근본적인 이유는 무엇일지 생각해보자. In-memory 데이터베이스가 아니라면 일반적으로 RDB는 정보의 영구적인 저장을 위해 디스크에 파일을 작성하게 된다. 서비스가 성장하면서 메모리 사이즈보다 데이터 용량이 커지면 OS 레벨에서 캐시해주는 범위를 초과하면서 Disk I/O가 급등하게 된다. 인덱스도 마찬가지이다. 인덱스 역시 파일로 관리되게 되는데, 이 인덱스 파일의 사이즈가 커지면 같은 이유로 Disk I/O가 많아지고 속도는 Memory 접근에 비해 백만 배까지 느려진다. CRUD를 할 때 직, 간접적으로 인덱스 파일을 사용하게 되는데, 모든 동작이 이렇게 느려진다. 해결할 수 있는 원리근본적으로 해결하기 위해서는 테이블의 사이즈를 줄여줘야 한다. 이 방법으로는 두 가지를 여기서 언급하는데, 첫 번째는 일반적으로 RDB에서 제공하는 파티셔닝과 엔지니어가 직접 테이블을 분리하는 샤딩에 대해 다룬다. 간단히 말해서 파티셔닝은 하나의 RDB 안에서 테이블 하나를 내부적으로 여러 테이블로 나눠주는 것이고, 샤딩은 여러 RDB 서버를 사용해 데이터를 분할하는 방식이다. 파티셔닝 (Partitioning)파티셔닝은 논리적으로는 하나의 테이블인데, 내부에서는 물리적으로 여러 테이블로 나눠 관리하는 방법이다. RDB마다 다를 수 있지만 일반적으로는 PARTITION 키워드를 통해 테이블을 분할할 수 있다. 사용하는 데이터들의 인덱스를 여러 개로 분할해서 사용할 수 있게 된다. 따라서 이전에 발생한 문제를 해결할 수도 있고, 데이터를 목정성에 맞게 나눠 관리하다가 요구에 따라 간단하게 삭제할 수도 있다. 흔히 이 파티셔닝을 “스케일 아웃”이라고 표현하지 않는다. 일종의 기술로 대량의 데이터를 특정 기준별로 데이터베이스에 부하가 적게 생기면서 삭제할 수 있도록 하는 목적이 더 크다. 그렇지만 근본적으로 큰 테이블을 여러 테이블로 나눠주는 과정이 포함되어있어서 큰 테이블에서 발생할 수 있는 문제를 해결해줄 수 있다. 파티션 키파티션을 만들 때, 특정 데이터가 어디에 위치하게 될지를 결정하는 키를 파티션 키라고 한다. CRUD를 할 때, 이 키를 활용해(활용할 수 있는 상황이라면) 파티션을 선택한다. 그다음 명령 동작을 수행하는 구조이다. 한 단계를 거치지만 거대한 테이블을 모두 찾아보지 않아도 된다. 이렇게 불필요한 다른 서브 테이블을 배제하는 동작을 프루닝이라고 한다. 조금 구체적인 얘기인데, 파티션 키를 선택할 때 제한사항이 존재한다. 유니크 키는 논리적인 테이블 안에서 유일해야 하는 값이기 때문에 파티션 키를 통해 해당 유니크 키가 어디에 있는지 결정할 수 있어야 한다. 따라서 파티션 키는 유니크 인덱스의 일부 또는 전체를 사용해 표현해야 한다. 예를 들어서 유니크 키가 다음과 같이 설정되어있다고 생각해보자. 1PRIMARY (fd1, fd2) -- PRIMARY도 유니크 키 파티션 키는 fd1을 사용하거나, fd2를 쓰거나, 둘 다 사용해야 한다. 그래야만 파티션 키가 유니크 값들이 무조건 같은 테이블에 있음을 확인해줄 수 있다. 파티션에서 쿼리가 발생하는 과정 우선 먼저 파티션을 구분할 수 있는 조건절이 사용되었는지 확인하고 파티션 프루닝을 시도한다. 위에서 살짝 언급했지만, 파티션 프루닝은 찾을 필요가 없는 파티션을 걸러 내는 과정이다. 그다음 일반적인 테이블을 스캔하는 과정이 발생한다. 일반적인 테이블을 스캔하는 과정은 조건절에 인덱스가 포함된 경우 인덱스를 통해 쿼리를 하고, 그렇지 않으면 테이블 풀 스캔을 하는 과정을 말한다. 따라서 쿼리를 하는 방법에서도 어떤 키를 기준으로 파티셔닝을 해야 할지 신중하게 결정해야 한다. 쿼리 패턴에 맞게 파티셔닝을 해야 파티셔닝을 한 효과를 최대화할 수 있다. 파티션에서 업데이트가 발생하는 과정업데이트라고 썼지만 실제로 파티셔닝이 된 데이터베이스에서는 읽기, 삭제, 삽입이 포함될 수 있는 과정이다. 업데이트 동작을 수행하기 위해서는 먼저 테이블에서 해당 데이터를 찾아야 한다. 이 과정에서 위에서 말한 쿼리 과정이 수행된다. 그다음 데이터를 수정하게 되는데, 만약 업데이트한 필드가 파티션 키와 상관없는 필드인 경우엔 값만 수정하고 끝난다. 그런데 만약 파티션 키를 수정하게 되면 해당 데이터를 재배치하는 과정이 필요하다. 즉, 데이터를 삭제 후 알맞은 파티션에 삽입하는 과정이 발생한다. 이런 동작을 하므로 파티션 키는 쉽게 변하지 않는 값으로 설정하는 것이 퍼포먼스 측면에서 좋다. 파티션 프루닝지금까지 이 글을 따라오다 보면 파티션 프루닝을 몇 차례 만날 수 있다. 파티션 프루닝은 파티셔닝의 핵심이다. 이 작업은 EXPLAIN 명령으로 확인할 수 있다. 해시 파티셔닝을 한 다음 쿼리를 EXPLAIN으로 확인한 모습. p0 파티션만 사용되고 나머지는 사용되지 않음 테이블을 분리해서 인덱스의 크기를 줄이는 것이 파티셔닝의 외적으로 드러나는 장점이지만, 사실 프루닝을 잘 할 수 있도록 쿼리를 하지 않으면 오히려 안 좋은 퍼포먼스를 발생시킨다. 따라서 무턱대고 파티션을 많이 만들어서 인덱스 사이즈를 줄이기보단 파티션 프루닝이 최적으로 발생하도록 만들고, 인덱스 서치를 한 번만 발생하도록 하는 것이 더 중요한 파티셔닝 전략이다. 방법이 글에서는 파티셔닝 방법 4가지를 설명한다. 구체적인 내용에 대해서는 MySQL, 특히 InnoDB 스토리지 엔진을 기준으로 설명하고 있다. Range 범위를 기반으로 데이터를 나누기 쉬운 경우 사용할 수 있는 방법이다. 로그 데이터를 예로 들어볼 수 있다. 데이터가 시간에 따라 쌓이기 때문에 필요에 따라 월 단위나 연 단위로 테이블을 나눌 수 있다. 1234567891011CREATE TABLE example_logs ( ... reg_date DATETIME NOT NULL, PRIMARY KEY (id, reg_date)) PARTITION BY RANGE (YEAR(reg_date)) ( PARTITION p2017 VALUES LESS THAN (2018), PARTITION p2018 VALUES LESS THAN (2019), PARTITION p2019 VALUES LESS THAN (2020), PARTITION p2020 VALUES LESS THAN (2021), PARTITION p9999 VALUES LESS THAN MAXVALUE); 위 SQL을 보면 파티션 키로 내부 함수가 사용된 것을 볼 수 있다. 모든 내장 함수가 가능한 것은 아니고, InnoDB인 경우에는 이 링크에 있는 내장함수들이 가능하다. 그리고 범위 마지막 부분은 MAXVALUE 키워드가 사용된 것을 확인할 수 있다. 위 SQL대로면 p9999 파티션에 2021년도 이후 로그가 쌓이고 있다고 보면 된다. 이 때 2021년도 이후 파티션을 구성하려고 하면 단순히 ADD PARTITION 키워드로는 동작하지 않는다. 123ALTER TABLE example_logs ADD PARTITION ( PARTITION p2021 VALUES LESS THAN (2022)); -- ERROR 맨 처음 CREATE TABLE을 한 SQL에서 알 수 있듯, 파티션으로 나눠질 때 위에서부터 차례대로 파티션 위치를 판단해 나누는 것을 알 수 있는데, ADD PARTITION을 하게 되면 이미 만들어진 파티션들 뒤에 파티션을 추가하기 때문에 마지막에 MAXVALUE를 사용하지 않는 상황이 된다. 따라서 파티션에 범위를 추가하기 위해서는 REORGANIZE를 사용해야 한다. 1234ALTER TABLE example_logs REORGANIZE PARTITION p9999 INTO ( PARTITION p2021 VALUES LESS THAN (2022), PARTITION p9999 VALUES LESS THAN MAXVALUE); 다만 REORGANIZE 작업은 기본적으로 그 전 파티션을 복사하는 작업이다. 따라서 데이터가 많은 경우 오래 걸릴 수도 있다. 이런 문제를 해결하기 위한 일반적인 패턴 중 하나로 MAXVALUE 키워드를 쓰지 않고, 미래에 사용될 범위의 파티션을 미리 만들어두는 방법이 있다. 이렇게 하면 ADD PARTITION을 통해 간단하게 범위를 늘릴 수 있다. 당연히 문제가 발생할 여지가 있다. 이 작업이 모종의 이유로 생략되거나 문제가 생겨 생성되지 못한 상태로 해당 테이블을 사용하게 되면 파티션에 들어가야 할 데이터의 INSERT 작업이 동작하지 않는다. 이렇게 만들어진 파티션은 간단하게 드랍할 수 있다. 1ALTER TABLE example_logs DROP PARTITION p2017; 위 코드로 2017년 로그를 삭제할 수 있다. 조건절을 통해 삭제하는 것보다 데이터베이스에 생기는 부하도 적고 빠르게 데이터를 삭제할 수 있다. List리스트 방식은 파티션 키가 어떤 케이스에 속하는지 직접 지정해주는 방법이다. IN (...) 안에 파티션으로 선택되는 리스트를 만들어주어야 한다. 12345678910CREATE TABLE posts ( id INT NOT NULL, title VARCHAR(50), ... category_id INT NOT NULL) PARTITION BY LIST(category_id) ( PARTITION fleamarket VALUES IN (1), PARTITION town VALUES IN (2), PARTITION etc VALUES IN (3, NULL)); 당연히 리스트 안의 값은 겹치면 안 되고, 만약 겹치게 되면 에러를 발생시킨다. 파티션 키의 값이 지정된 코드나 값일 때 사용할 수 있다. 위 예시에서는 포스트의 카테고리에 따라 테이블을 나눠 구성한 모습이다. 또한 키 값이 오름차순이나 내림차순의 의미가 없는 경우라면 Range를 사용할 수 없으므로 List 방법이 적합한지 생각해볼 수 있다. 위에서 파티션을 추가하는 방법처럼 ADD 키워드를 통해 파티션을 추가할 수 있고, DROP PARTITION을 통해 파티션을 지울 수 있다. 또 하나의 파티션을 분리 및 병합할 때는 REORGANIZE PARTITION을 사용할 수 있다. List, Range 파티션의 경우 Subpartition (Composite Partition)을 구성할 수 있다. Hash해시 함수에 의해 파티션을 결정할 수 있다. 123456CREATE TABLE accounts ( ...) PARTITION BY HASH(id) PARTITIONS 4 ( PARTITION p0, ...); PARTITIONS 4는 4개의 파티션에 의해 분할되는 것을 의미한다. 해시 작업이라고 하는 것은 쉽게 말해서 모듈러 연산하는 작업이다. 따라서 파티션 키로 사용되는 값은 정수값을 반환해줘야 한다. 파티션 이름을 지정하려면 위에서처럼 직접 이름을 정해줄 수도 있는데, 만약 정의하지 않으면 p0, p1, … 이런 식으로 지정된다. 데이터를 균일하게 파티션에 분배되어야 잘 파티셔닝 된 것이라 볼 수 있는데, 해시의 경우 아주 균일하게 파티션을 분배한 것이라 볼 수 있다. 자원을 효율적으로 사용할 수 있지만, 데이터 목적이나 유형을 고려해서 파티션을 나눈 것은 아니다. 따라서 모든 데이터에 대해 용도가 비슷하고 사용 빈도도 비슷한 큰 데이터를 파티셔닝 해야할 때 사용하기 좋다. 예를 들어서 계정 정보는 오래 전에 가입했든 최근에 가입했든 지속해서 사용하는 사람들의 정보가 계속해서 사용된다. 하지만 이 방법으로 파티션을 분할하면 파티션을 구성을 변경하는 과정의 비용이 많이 들거나 불가능하다. 예를 들어서 하나의 파티션을 더 넣는다는 것은 해시 함수를 바꿔주는 것과 같은 의미이다. 따라서 바뀐 해시 함수로 기존 데이터를 모두 재배열 해줘야 한다. 또한 해시로 나눠진 파티션을 삭제할 일도 사실상 없다. 해시 파티션을 했을 때, 각 파티션에 어떤 데이터가 있는지에 대한 의미가 없기 떄문이다. 따라서 특정 파티션을 삭제할 이유가 없고, 실제로 삭제한려고 한다 하더라도 DROP PARTITION은 에러를 발생시킨다. 그리고 병합하거나 분할하는 작업도 불가능하다. 이 과정은 그냥 파티션을 늘려주거나 줄여주는 작업을 해야 한다. 예상할 수 있겠지만 파티션을 늘이고 줄이는 작업은 아주 비싼 작업이다. COALESCE PARTITION 1과 같은 방법으로 파티션을 줄여줄 수 있다. 1ALTER TABLE example COALESCE PARTITIONS 1; 파티셔닝 이후의 유연성이 부족한 방법이기 때문에, 설계할 때 몇 개의 파티션이 적합할지 생각해보는 것이 중요하다. 하지만 해시로 사용하는 컬럼값이 조건절에 사용되면 아주 효율적으로 파티션 프루닝이 가능하다. 유연성이 부족한 해시 파티션 문제를 해결하기 위해서 Linear Hash를 사용할 수 있다. 그러나 사용되는 특정한 알고리즘으로 인해 데이터의 분배가 덜 균등해질 수 있다. Key키 파티션 방법은 해시와 거의 비슷한데 해시 함수의 모듈러를 위해 정수형 타입을 사용해야 했던 해시와는 다르게, 대부분의 타입을 파티션 키로 사용할 수 있다. 파티션 키를 MD5를 통해 해시값을 계산하고 그 값을 모듈러 연산해서 파티셔닝을 해주는 구조이다. 12345678CREATE TABLE k1 ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20))PARTITION BY KEY() -- 괄호가 비어있으면, 프라이머리 키 모든 칼럼을 사용함PARTITIONS 2;-- 프라이머리 키가 따로 없으면 유니크 키를 사용 해시 파티션에 비해서 더 균등하게 나눠질 수 있다. 따라서 보다 효율적이고, 파티션 키로 사용되는 필드가 정수형이 아니어도 되기 때문에 Hash 방법이 사용될 수 없는 상황에서 고려해볼 수 있다. 파티셔닝 정리파티션 키와 파티셔닝 방법을 선택하는 기준은 데이터 접근 패턴, 어떤 유형의 데이터인지, 등 고려해볼 만한 상황이 많이 있다. 그리고 단순히 인덱스를 작게 만드는 것보다 효율적인 DML을 쓰기 위해 어떻게 설계하는 것이 좋을지도 고민해봐야 한다. 샤딩 (Sharding)단일 서버에서 효율적으로 테이블을 나눈다고 하더라도 물리적인 한계는 반드시 존재한다. 예를 들어서 데이터베이스의 디스크 크기를 늘리기 어렵다든지, 늘릴 수 있더라도 근본적인 문제 해결 방법이 아니라든지(Network 부하, 꾸준하고 급격하게 증가하는 데이터들, 서버 자체의 부하), 이러한 이유로 결국 서버를 물리적으로 여러 대를 사용해서 해결해야 한다. 이렇게 수평적인 방식으로 파티셔닝을 하는 것을 샤딩이라고 한다. 위에서 언급한 것처럼 파티셔닝은 스케일 아웃이라고 보기 어렵다. 스케일 아웃은 결국 “수평적 확장”을 의미한다. 수평적 파티셔닝 방법이 일반적인 데이터베이스의 스케일링 방법이다. 그런데 “수평적 방식의 파티셔닝”이라는 말처럼 방법 측면에서 파티셔닝의 방법과 유사하다. 샤딩이라고 하는 것은 RDB의 기본적인 기능은 아니다. 즉, 개발자의 엔지니어링이 요구되는 부분이다. 어떤 데이터가 어떤 노드에 들어가 있는지 판단하는 것을 개발자가 엔지니어링을 통해 라우팅 처리 해줘야 한다. 이렇게 데이터를 저장하거나 가져올 때 적절한 노드를 찾아주는 흐름을 샤딩 로직 이라고 하는데, 이 로직은 애플리케이션 사이드에 존재할 수도 있고 스토리지 시스템의 미들웨어로 존재할 수도 있다. 애플리케이션 사이드에 이 로직이 있는 것을 Application-level 샤딩이라는 이름으로 보통 불린다. 이는 애플리케이션의 로직에서 라우팅 처리를 해준다는 것을 의미한다. ORM에서 이를 설정한다든지, 직접 데이터값에 따라 어떤 데이터베이스와 연결할지 선택하는 흐름 등을 예로 들 수 있다. 반대로 스토리지 시스템 미들웨어는 솔루션, 샤딩 플랫폼, 프록시 등 여러 이름으로 불리고 있다. 이 방법을 사용하면 어떤 미들웨어인지에 따라 다를 수 있지만, 일반적으로 애플리케이션에서는 데이터베이스의 라우팅 처리에 신경쓰지 않고 마치 하나의 데이터베이스와 상호작용하는 것처럼 동작하게 된다. 샤딩을 하게되면 그 전처럼 RDB를 사용하지 못 할 수 있다. 여러 RDB의 특징들에 제약이 생긴다는 것을 의미한다. 대표적으로 다음과 같은 문제들이 있다. 물리적으로 다른 노드의 데이터베이스와 JOIN 연산을 수행할 수 없는 문제 Auto Increment가 샤드별로 달라지는 문제 하나의 트랜잭션이 두 개 이상의 샤드에 접근할 수 없는 문제 이런 문제가 발생할 수 있으므로 샤딩을 설계하는 과정에서 이 문제들을 고려해서 샤딩을 진행해야 한다. 샤드 키파티셔닝에서도 각 파티션이 어떤 데이터를 가져갈지 결정할 파티션 키가 있었던 것처럼, 분할된 노드(분리된 데이터베이스 서버)는 각각이 가져갈 데이터를 결정해야 한다. 이 기준을 샤드 키라고 부른다. 위에서 파티션 키가 변하지 않는 값으로 설정하는 것이 좋다고 했던 것과 같은 이유로 샤드 키는 변경되지 않는 값을 기준으로 설정해야 한다. 방법샤드 키를 어떻게 설계했는지에 따라 어떻게 라우팅할지도 달라진다. 여러 방법이 있지만 여러 곳에서 소개되고 있는 두 가지 방법을 가져왔다. Range Based Sharding출처: 우아한 형제들 기술 블로그 특정 키의 범위에 따라 샤드에 배분해주는 방법이다. 이름에서 알 수 있듯, 위 파티셔닝 방법 중 Range 방식과 유사하다. 특징은 샤드를 추가하는 과정이 비교적 간단하고 라우팅도 간단하다. 다만 데이터 접근 패턴이나 범위별 데이터양을 고려한 방법은 아니기 때문에 컴퓨팅 자원을 불균형하게 소비하는 케이스가 발생할 수도 있다. Modulus (Key, Hash Based) Sharding출처: 우아한 형제들 기술 블로그 특정 키를 모듈러 연산으로 특정하는 방식이다. 이 방법의 다른 이름에서 알 수 있듯, 위에서 파티셔닝 방법 중 Hash 방식과 유사하다는 점을 알 수 있다. 특징도 Hash 방식의 특징과 유사하다. 데이터가 샤드에 균일하게 분산된다는 장점이 있지만, 샤드를 추가할 때 데이터를 재배열해야 하는 비용이 있다는 단점이 있다. 케이스 스터디샤딩같은 경우는 아무래도 RDB의 자체적인 기능이 아니라서 케이스를 찾아보면 굉장히 다양한 방법으로 샤딩을 진행한 것을 볼 수 있다. 예시로 세 가지 케이스를 확인해보자. Notion 샤딩노션은 애플리케이션 레벨의 샤딩을 결정했다. Vitess, Citus와 같은 미들웨어 서비스를 알아보긴 했는데, 그 솔루션들의 동작이 불투명하다고 판단했고, 본인들 데이터를 직접 컨트롤하길 원했기 때문에 이러한 선택을 했다고 한다. 그 결정 이후에는 어떻게 데이터를 나눌지 결정하는 과정이 있었다. 어떤 데이터를 샤드해야 할까? → 노션의 블록과 FK로 연관된 모든 데이터를 같이 묶어 하나의 샤드에 포함될 수 있도록 함으로써 데이터 부정확성 문제를 방지 어떻게 데이터를 나눠야 할까? → 워크스페이스의 ID를 기준으로 샤딩 함으로써 한 워크스페이스의 데이터들이 같은 데이터베이스 안에 들어갈 수 있도록 분할 더 자세한 내용은 이 링크에서 확인할 수 있다. LINELINE Manga 서비스의 서버 엔지니어링 스토리를 보면 애플리케이션 샤딩을 한 것으로 추측된다. 몇 단계를 거쳐 샤딩을 진행했는데, 4단계에서 코드를 수정해서 샤딩을 적용한다는 얘기가 나온다. 라인 망가의 경우에는 RDB 부하 문제를 초기엔 스케일 업으로 대응했으나, 근본적인 해결책이 아니라고 판단하여 샤딩을 진행했다고 한다. 검토 과정은 생략되었는데, 새로운 컬럼을 구성하고 Range Based Sharding을 진행했다고 한다. NHNNHN의 경우 게임 서버 얘기였는데, 여러 서버로 샤딩을 한 상태에서 애플리케이션 레벨의 샤딩을 했을 때 컨넥션과 관련한 문제가 생길 수 있었다고 한다. 애플리케이션 서버가 200대, DB에 컨넥션이 서버당 300개씩 각자 거는 상황이면, 하나의 DB마다 최대 6만 개의 컨넥션이 생기게 된다. 이 문제를 해결하기 위해 미들웨어를 두고 프록시를 사용하고 있다고 한다. 여기서 사용된 미들웨어는 ProxySQL이고, 이 솔루션을 사용하게 되면 애플리케이션 레벨에서는 하나의 데이터베이스와 통신하는 것과 같이 구성할 수 있었다고 한다. 동시 접속의 개념이 있는 게임 서버에서 컨넥션을 Demultiplexing 하는 과정에 대해 더 자세히 알아보고 싶다면 이 영상을 보자. 마치며사실 애플리케이션의 사용자가 증가하면서 부하를 견디는 설계를 할 때 애플리케이션이 데이터베이스와 접근할 때뿐만 아니라 고려해야 할 사항이 많이 있다. 데이터베이스의 부하를 분산하는 방법으로 미들웨어를 사용한 샤딩을 결정했다면, 이 미들웨어가 애플리케이션으로부터 오는 부하를 강하게 견디는지도 확인해봐야 한다. 이런 전반적인 이야기는 Database HA (High Availability)라는 키워드로 몇 가지 더 알아봐야 한다. 그리고 Replication과 관련된 얘기도 이 글에서는 빠져있는데, 데이터베이스를 복제해 읽기 전용 슬레이브 레플리카를 만들거나, 데이터 분석용, 백업용, 등 여러 이유로 사용할 수도 있다.","link":"/posts/database/rdb-scaling/"},{"title":"Scaling Memcache At Facebook","text":"이 논문은 Planet Scale 서비스 중 하나인 Facebook(이하 Meta, 메타, 페이스북)이 어떻게 Memcache를 사용했는지에 대한 논문인데, 이 글은 이 논문 내용 중 “확장되는 스케일에서 어떻게 Data Consistency를 유지 했는가?”에 집중해 정리했다. 논문에서는 Memcache와 Memcached 용어를 철저히 분리한다. 전자는 분산 시스템을 구성하는 시스템 자체를 의미하고 후자는 실행되는 서버, 바이너리 자체를 의미한다. Overview메타에서는 캐시를 정말 대규모로 사용한다. 생각해 보면 페이스북은 캐시를 쓰기에 가장 적합한 유즈 케이스를 가지고 있다. 일단 논문에서 말하는 용례는 다음과 같다. Query Cache: 데이터베이스 읽기 부하를 줄이기 위해 사용한다. 특히 demand-filled look-aside 캐시로 사용한다. Generic Cache: 굉장히 일반적인 용례를 의미한다. 거의 나머지라고 보면 될 수준. 연산이 오래 걸리는 결과(ex. 머신 러닝 결과 등)라든지 어찌 됐든 무거운 무언가를 해야 하는 걸 담아두고 여러 서비스에서 꺼내서 쓰는 용도이다. demand-filled look-aside 캐시는 흔히 우리가 알고 있는 캐싱 방법이다. 읽어올 때 캐시를 확인 먼저하고 없으면 Origin 데이터 소스로부터 값을 가져오는 방식을 의미한다. 논문과 영상에서 짧게 나오는 내용 중에 look-aside 캐시를 만들기 위해서 Origin과 동기화를 위해 데이터 소스의 변경이 발생하면 캐시의 데이터를 수정하지 않고 삭제하는 방법을 선택했다고 한다. 보통 Cache Invalidation이 이렇게 동작하기 때문에 일반적인 것 같지만, 아무튼 삭제를 선택한 이유는 수정보다 멱등적이기 때문이라고 설명한다. 이 논문에서는 위 용례에 대해 (보통 Query Cache에 대한 내용인 듯) 배포 스케일에 따라 마주한 공학적 어려움을 설명해주고 있다. 이를 크게 세 단계로 나눠서 설명한다. 단일한 클러스터 여러 개의 Front-End(이하 FE, 프론트엔드) 클러스터 세계 단위로 여러 클러스터를 두는 상황 Single Cluster이 수준에서는 지연을 줄이거나 Cache Miss로 인해 발생하는 부하를 줄이기 위해 노력하는 스케일이다. 이 장에서는 지연을 줄이기 위한 방법, 부하를 줄이기 위한 방법, 실패 처리에 대해 자세히 설명한다. 이 글에서 일관성 얘기를 위해 적합한 스테이지가 아니다. 만약 일관성 문제만 궁금하다면 멀티 클러스터 단위로 넘어가도 좋다. 지연 줄이기우선 클러스터로 운영하고 있는 Memcache의 상황을 설명하자면, 수 백대의 Memcached 서버에 데이터가 Consistent Hashing으로 분산되어 있다. 또한 웹서버가 하나의 페이지를 만들기 위해 수많은 Memcached로부터 동시에 값을 읽어오게 된다. 예를 들어 인기 있는 페이지의 결과를 위해 평균적으로 521개의 독립적인 아이템을 Memcached에서 가져온다고 한다. 이러한 이유로 클라이언트는 짧은 시간에 엄청난 양의 데이터 응답을 받을 수 있는 상황이다. 이렇게 되면 다음과 같은 문제가 발생할 수 있다. 하나의 Memcached 병목이 웹서버의 병목 지점이 될 수 있다. 웹서버에 Incast Congestion이 발생할 수 있다. 데이터 복제를 통해 단일 서버 병목을 완화할 수 있지만 데이터 비효율을 감수해야 한다. Incast Congestion은 TCP 응답이 과도하게 몰려 TCP 윈도우를 압도하는 상황을 의미한다. 이 상황이 되면 패킷이 드랍되는 등 느려지는 원인이 될 수 있다. 이러한 문제를 메타에서는 Memcache 클라이언트를 개선해 해결했다. Parallel Requests And BatchingDirected Acyclic Graph(DAG)를 그려 데이터 사이의 의존성을 확인한 후 웹서버가 동시에 Fetching할 수 있는 데이터를 최대로 뽑아낼 수 있게 만들었다. 이 구조로 동작하는 배치는 요청당 평균 24개의 키를 동시에 쿼리하게 되었다. 결과적으로 웹서버의 라운드 트립을 최소화하게 되었다. Client-Server CommunicationMemcached 서버는 각자와 커뮤니케이 하지 않는데, 시스템의 복잡성을 클라이언트에 주입했다. 이로써 Memcached 서버는 굉장히 단순하게 유지된다. 클라이언트는 GET 요청을 보낼 때 지연과 오버헤드를 줄이기 위해 UDP를 사용한다. 클라이언트는 시퀀스 넘버를 통해 UDP 요청에 문제가 있는지 확인할 수는 있지만 Recover 하지는 않는다. 이러한 경우는 그냥 Cache Miss와 동일하게 처리된다. 이러한 방법은 경험적으로는 굉장히 실용적이었다고 한다. UDP 요청은 실제로 20%의 지연을 줄여주었다고 한다. 신뢰성을 위해 PUT &amp; DELETE 요청은 여전히 TCP를 사용한다. 이를 처리하는 컴포넌트로 mcrouter가 있는데, 이는 Proxy로 동작하거나 라이브러리로 클라이언트에 삽입되기도 한다. 이 프록시가 클라이언트와 Memcached의 TCP 컨넥션을 줄여주는 역할을 함으로써 CPU, Memory, Network를 아꼈다. Incast Congestion클라이언트는 TCP Incast Congestion 문제를 보완하기 위해 자체적인 혼잡 제어 메커니즘을 가지고 있었다. 클라이언트는 Sliding Window(슬라이딩 윈도우) 방법을 사용해 요청 숫자를 제어했다. 슬라이딩 윈도우 방법은 말 그대로 TCP 혼잡 제어 방법처럼 천천히 증가하다가 문제가 생기면 줄어드는 구조이다. 윈도우 사이즈를 최적화하는 것도 중요한 문제였는데, 윈도우가 커지면 Incast Congestion을 막을 수 없어 성능 저하가 발생하고, 윈도우가 작아지면 요청들의 대기 시간이 길어졌다. 이것 역시 메타에서는 경험으로 적절한 수치를 찾은 것으로 보인다. 부하 줄이기부하를 줄이기 위한 노력으로 세 가지를 소개하고 있다. Lease Memcache Pools Replication Within Pools Lease캐시를 사용하면서 생길 수 있는 문제로 오래된 데이터를 캐시에서 보관하는 Stale Set 문제와 특정 키가 굉장히 활발히 수정되고 읽히는 Thundering Herds 문제가 있는데, 메타는 위 문제들을 해결하기 위해 Lease 기술을 사용했다. Memcached는 클라이언트가 Cache Missing을 경험했을 때 데이터를 다시 채우기 위해 Lease 토큰을 발급해준다. 이 토큰은 64비트의 키 마다 유일한 값이다. 클라이언트는 캐시에 값을 저장할 때 Lease 토큰을 제공해야 한다. Memcached는 이를 확인하고 데이터가 저장되어야 하는지를 결정한다. 이때 “확인”(Verification) 과정은 말 그대로 “이 토큰이 특정 키에 대해 유효한가”를 보는 것이다. 예를 들어 Memcached가 요청을 받기 전에 해당 아이템을 삭제하라는 요청을 처리한 경우 토큰이 유효하지 않게 된 것이다. 이 동작 방식이 Load-Link/Store-Conditional이라고 하는 방식과 유사하게 동작하여 동시 쓰기로 인한 과거 데이터가 쓰이는 것을 막아준다. Load-Link/Store-Conditional 방법은 쉽게 말해 특정 값에 대한 쓰기 A가 수행되기 전에 다른 요청 B에 의해 처리되어버리면 A가 실패하도록 하는 로직이다. Lease를 통해 Thundering Herds를 완화할 수도 있다. 각 Memcached 서버는 토큰 반환 속도를 조절할 수 있다. 기본으로 페이스북은 Memcached가 토큰을 10초마다 한 번 반환하도록 조정했다. 토큰 발급 후 10초 이내 값을 요구하는 경우 클라이언트에게 잠시 기다리라는 알람을 보낸다. 일반적으로 쓰기는 수 미리 초 안에 수행되기 때문에 10초 뒤의 클라이언트 요청은 데이터가 캐시에 존재하는 상황일 가능성이 높다. 하지만 이 동작은 선택적이고 만약 오래된 데이터를 어느 정도 감안하는 서비스라면 오래된 데이터일 수도 있지만 값을 리턴해준다. Memcache Pools위에서 언급한 Generic Cache로 사용할 때 여러 애플리케이션에 의해 사용되면 각 서비스가 다른 목적으로 접근하고 각 서비스에서 원하는 퀄리티 수준도 모두 다르다. 이는 사용 방법에서도 차이가 크게 생기기 때문에 Cache Hit을 줄이는 결과로 이어질 수가 있다. 이 차이를 해결하기 위해 Memcached 서버들을 다른 성격의 풀로 나눴다. 기본적으로 디폴트에 해당하는 풀이 있는데 이 풀을 WildCard라고 한다. 그리고 이 기본 풀에 있을 때 문제가 되는 키를 복수의 다른 풀에 분배하는 구조이다. 예를 들어서 자주 접근하지만 캐시 미스가 나도 큰 문제가 없는 키를 작은 풀에 할당하고, 자주 접근하고 캐시 미스가 나면 비싼 연산을 수행해야 하는 키를 조금 더 큰 풀에 담을 수 있다. 이로써 보다 적합한 키를 Eviction 처리할 수 있게 된다. Replication Within Pools어떤 풀들은 데이터 복제를 사용하고 있다. 다음과 같은 키는 복제를 사용한다. 앱에서 주기적으로 많은 키를 동시에 가져감 전체 데이터 셋 사이즈가 하나 혹은 두 개의 Memcached 서버에 딱 맞음 요청 속도가 한 대의 서버에서 감당하기 어려운 수준 메타에서는 이런 경우 키를 나눠서 처리하는 방법 보다 복제해버리는 것을 선호한다. 예를 들어서 100개의 아이템이 있는데 다음과 같이 상황이 다른 것을 가정해 보자. 키를 공평하게 둘로 나눠서 가지고 있기 두 개의 서버에 100개를 모두 복제 요청이 1M/s 속도로 들어오고 있고 모든 키를 가져가야 하는 경우를 생각해 보자. 공평하게 둘로 나눈 상태라면 클라이언트는 아이템을 모두 얻기 위해 두 서버 모두에게 요청을 보내게 된다. 즉 두 서버가 모두 1M/s 부하를 받아야 하는 상황이다. 하지만 키가 두 Memcached 서버에 모두 동일하게 전체 셋이 복제되어 들어가 있다면 부하를 두 서버로 분산할 수 있게 된다. 단점이라고 하면 Invalidation을 두 번 해야 하는 것인데, 페이스북의 경우 요청을 분산하는 것이 Invalidation을 여러 번 처리하는 것보다 나은 선택이었다. 장애 복구페이스북은 Memcache 장애를 두 가지 스케일의 장애로 나눠서 처리했다. 작은 장애: 몇 개의 서버가 영향을 받는 장애 큰 장애: 클러스터 내의 꽤 큰 퍼센트의 서버가 영향을 받는 장애 큰 장애는 그냥 다른 클러스터로 요청을 옮기는 형태로 장애를 복구한다. 작은 장애는 보통 자동 복구에 의존하는데 이런 시스템에 의한 복구는 시간이 걸린다. 그런데 이러면 장애가 전파될 수 있는 상황이 발생할 수 있으므로 이를 막기 위한 메커니즘으로 Gutter를 도입했다. Gutter는 장애를 복구하기 위해 사용하는 전용 머신이다. 이 전용 머신은 클러스터 내에 약 1%를 차지한다. 클라이언트가 서버로부터 응답이 없으면 서버가 죽었다고 판단하고 Gutter에게 요청을 보낸다. Gutter에서 캐시 미스가 발생하면 DB에서 값을 쿼리하고 데이터를 Gutter에 집어넣는다. 이는 살아남은 다른 캐시 서버에 Rehashing을 해서 값을 채우는 방식과는 차이가 있다. 살아남은 캐시 서버에 값을 넣는 것은 다른 서버로 장애가 전파될 위험이 있다. 죽은 서버는 내부에 있던 부하가 높은 키를 가지고 있을 수 있는데, 이 키가 다른 서버로 전달되어 다른 서버의 과부하로 이어질 수 있다. 그래서 아예 유휴 서버를 두고 위험을 제한하는 방법을 사용하는 것이다. Multi-Clusters한 클러스터 안에서 Memcached 서버 수를 늘리는 것은 단순해 보이지만 온전한 해결책이 아니다. 장애 전파나 Incast Congestion을 피할 수 없게 될 수 있다. 따라서 Memcached를 복수의 클러스터로 만드는 방법을 선택했다. 이렇게 복수의 FE 클러스터와 하나의 스토리지 클러스터가 합쳐져서 Region을 구성한다. 논문에서 Web Server와 Memcached 클러스터가 있는 것을 FE 클러스터라고 부른다. Regional Invalidations스토리지 클러스터가 FE의 Memcache와 데이터 정합성을 맞추기 위한 Invalidation 책임을 가지고 있다. 이를 위해 메타에서는 mcsqueal이라고 하는 Invalidation Daemon을 사용한다. 이 프로세스는 CDC 형태로 DB의 Delete 요청을 분석해 FE 클러스터에게 알려준다.최적화를 위해 수정 요청을 보낸 웹서버도 자신의 클러스터 안에 있는 Memcache로 Invalidation 요청을 보낸다. 이로써 한 유저가 쓰기 후 읽기 작업을 할 때 보다 유의미한 결과를 전달해 줄 수 있게 된다. Regional Pools여러 클러스터 유저의 요청이 라우팅이 섞이면서 중복된 데이터들이 자동으로 여러 클러스터 안에 속하게 된다. 이는 클러스터 운영을 위한 캐시 중단을 만들었을 때도 다른 클러스터에 의해 Cache Hit가 줄어들지 않게 되는 등, 복제에 의한 장점이 생긴다. 하지만 문제는 메모리 비효율이 크다는 점이다. 이러한 메모리 비효율 문제를 해결하기 위해 Regional Pool를 적용했다. Regional Pool은 같은 Memcached 서버를 갖는 FE 클러스터를 의미한다.복제는 위에서 언급했던 것처럼 Failure Tolerance, 클러스터 안의 낮은 지연 등의 효과를 가지고 있지만 어떤 경우는 이렇게 하나의 캐시 데이터를 사용하는 경우가 나은 경우가 있다. 어떤 데이터와 웹서버를 Regional Pool에 옮겨야 하는지는 경험적인 수작업에 의해 진행된다. 요구되는 데이터 접근 속도, 데이터 사이즈, 특정 아이템에 접근하는 유니크한 유저의 숫자 등 여러 지표를 룰 베이스로 판단해 옮겨 넣는다. 마찬가지로 Regional Pool의 Memcache는 위에서 언급한 Gutter, mcqueal, mcrouter 등의 시스템을 모두 사용한다. Cold Cluster Warm upCold 클러스터를 Warm up 할 때는 Cache Miss 발생 시 스토리지 대신 Warm 클러스터에서 가져온다. 이런 방법으로 앞서 말한 FE 클러스터 간 데이터 복제 효과도 만들 수 있으며 스토리지를 사용한 것보다 빠르게 가져올 수 있다. 하지만 이 방법으로 인한 Race Condition이 발생할 수 있는데, 예를 들어 Cold 클러스터에서 삭제한 다음 곧바로 Cold 클러스터에서 해당 값을 읽는 상황을 생각해 보자. 방금 삭제되었기 때문에 없어야 맞는 값인데 이 Warm 클러스터와 데이터가 동기화되지 않은 상태로 Warm 클러스터에서 값을 가져온다면 Cold 클러스터의 이 값은 언제 끝날지 모르는 불일치가 발생한 상황이 된다. 이를 해결하기 위해 Memcached에서 키 삭제 요청을 처리한 다음 해당 키에 값을 추가하는 작업을 거부하는 기능을 사용했다. 이를 Hold-Off라고 하는데 Cold 클러스터는 2초의 Hold-Off 시간을 가지고 있다. 따라서 만약 어떤 키를 Warm 클러스터로부터 가져오려고 할 때 PUT 요청이 실패한다면 DB에 변경이 발생했다는 것을 알 수 있고, 이 경우는 DB에서 값을 가져오도록 되어있다. 이런 일관성 문제가 발생할 수 있지만 어찌 됐든 Warm up 방식이 그것보다 훨씬 큰 장점을 가져다준다. Cold 클러스터의 Cache Hit이 안정되면 Warm up을 종료하고 다른 클러스터처럼 동작하게 된다. Multi-Regions이전 챕터의 Region은 하나의 데이터 센터이다. 보통 페이스북 사이즈가 되면 데이터 센터를 대륙 혹은 지역 단위로 확장한다. 이를 통해 다음과 같은 장점을 얻을 수 있다. 클라이언트와 데이터 센터를 가까이 두어 지연을 줄인다. 특정 지역의 자연재해, 대규모 정전 등에 영향을 완화한다. 새로운 장소가 더 저렴한 전력, 경제적 장점 등을 줄 수 있다. 각 Region은 스토리지 클러스터와 몇 개의 FE 클러스터로 구성된다. 한 Region을 마스터 데이터베이스를 가진 Region으로 지정하고 다른 Region을 Read-Only Replica(이하 Replica)로 구성한다. 이 구성에서는 Memcache 혹은 스토리지 클러스터에 접근하는 경우 지연이 짧다. 여러 Region을 운영하게 되면 일단 스토리지와 Memcache의 데이터 일관성을 유지하기 어려워진다. 어려운 원인은 마스터 데이터베이스에서 데이터를 가져올 때 발생하는 지연(Lag) 현상이다. 보통 이런 시스템은 일관성과 성능을 어떻게 Trade-Off 할 것인지 광범위한 스펙트럼이 있고 메타 역시 이 스펙트럼의 어떤 한 지점을 고른 것이다. 이는 서비스의 특징 및 규모에 따라 경험적으로 선택되고 논문에서는 꽤 받아들일 수 있는 수준의 Trade-Off를 찾았다고 설명한다. Master Region에서 쓰는 경우Master Region은 이전에 설명한 한 Region에서 쓰기가 발생했을 때 mcsqueal이 동작하는 방법대로 동작한다. 하지만 이 Daemon 프로세스의 동작은 클러스터 안에서 한정된다. 다른 Region의 Memcache에 Invalidation을 전파하는 것은 동시성 이슈를 만들 수 있다. 예를 들어서 데이터가 수정되어 DB Replication이 발생해야 하는데, 이 데이터보다 Cache Invalidation이 먼저 도착하게 되고, 곧바로 클라이언트가 해당 키를 읽었다고 가정해 보자. 그러면 클라이언트는 해당 키에서 값을 못 찾고 Region 안에 있는 스토리지 클러스터에서 데이터를 찾게 된다. 그러면 오래된 데이터가 다시 캐시되고 유저는 오래된 데이터를 보게 된다. Non-Master Region에서 쓰는 경우복제 지연이 발생하고 있는 상황에서 Non-Master Region에서 데이터를 업데이트 한다고 가정해 보자. Region의 Memcache에 Invalidation을 했든 안 했든, Master 데이터베이스가 변경된 값을 Replica로 전달하지 않은 상태라면 업데이트를 요청한 유저가 오래된 데이터를 읽어오게 되는 상황이 생길 수 있다. Name = &quot;changhoi&quot;라고 수정하고 새로고침 된 페이지에서 여전히 &quot;CHANGHOI&quot;라고 보이는 상황을 의미한다. 따라서 Replica에서 데이터를 캐시에 채울 수 있는 순간은 복제 스트림을 따라잡고 난 다음이어야 한다. 만약 복제 스트림을 따라잡지 못한 상태라면 웹서버는 데이터를 Master Region 스토리지 클러스터에서 가져온다. 이 동작을 위해 Remote Marker를 도입했다. 한 서버가 K라는 키에 영향을 주는 업데이트를 한다면 다음과 같은 순서를 따른다. Region 안에 Remote Marker를 R(K)에 둔다. K와 R(K)를 SQL 구문 안에서 Invalidation 될 수 있도록 포함시켜 마스터에 쓰기를 수행한다. Region의 Memcache에서 K를 삭제한다. 2번 단계가 구체적으로 이해가 잘 안되는데, SQL 구문이 Replica에 전파될 때 R(K)를 같이 없앨 수 있게 SQL 구문에 내장한다는 느낌이었다. 이렇게 동작하면 Cache Miss가 발생했을 때 K에 대한 마커가 남아있는 경우 Region의 Replica에서 아직 오래된 데이터를 가지고 있다는 뜻이 되므로 Master Region의 스토리지에서 데이터를 가져온다. 만약 마커가 없다면 Region 안에 있는 Replica에서 값을 가져온다. Reference Scaling Memcache at Facebook Scaling Memcache at Facebook Youtube Scaling Memcache at Facebook 요약 &amp; 해석","link":"/posts/database/scaling-memcache-at-facebook/"},{"title":"Go Package Architecture - 이론편","text":"Go에서 다른 언어와 다르게 Directory(디렉토리)는 굉장히 중요하다. 많은 다른 언어들은 실제 디렉토리의 역할 이상을 하지는 않지만, Go는 Package(패키지)와 밀접한 관계가 있으며 프로그램이 어떻게 작성될지 결정하는 한 부분이다. 그 때문에 Go에서 디렉토리 구조를 어떻게 할지는 패키지를 어떻게 구성할 것인지와 꽤 유관하다. 이번 글에서는 Go를 사용하면서 어떤 형태의 디렉토리와 패키지 구조를 구성하는 것이 좋을지 혼자만의 고민을 정리했다. 일단 패키지 &amp; 디렉토리 아키텍처 얘기를 꺼내기 전에 어떤 점들을 고려하면서 이러한 구조를 생각해 냈는지 정리했다 고민해 볼 Go 특징Directory와 Package의 관계Go에서 Package(패키지)는 go.mod 파일이 있는 모듈의 루트를 기준으로 한 디렉토리에 하나의 패키지만 존재할 수 있다. 또한 패키지의 이름은 기본적으로 디렉토리의 이름을 따르게 되어있다. 만약 패키지의 이름이 디렉토리의 이름과 다른 경우 패키지를 호출할 때 Alias를 붙이는 편이다. 12345678910package main import ( playground &quot;changhoi.kim/playground/pkg&quot; &quot;fmt&quot; ) func main() { fmt.Println(playground.Hello()) } 필자는 Goland를 주로 쓰고 있는데, 이 IDE는 일반적인 기준에 따라 별칭을 붙이는 것 같다. 예를 들어 일반적인 v2 패키지 형태의 경우는 별칭을 붙여주지 않는 게 일반적이다. 123456import ( // fiber는 일반적인 v2 패턴으로 패키지를 제공해 별칭을 붙이지 않는 모습 &quot;github.com/gofiber/fiber/v2&quot; // ...)// ... 별칭이 없어도 사실 잘 동작하지만, Go 생태계에서는 패키지 이름이 디렉토리 이름과 다른 경우 별칭을 사용하는 편이다. 개인적으로 이 생태계의 룰을 지키면서 별칭을 굳이 붙이고 싶지는 않다. 즉, 패키지 이름을 디렉토리 이름과 동일하게 맞추고 싶은 욕망이 기본적으로 있다. Go 블로그 글에서도 Conventionally, 패키지의 경로를 패키지의 이름으로 둔다는 얘기가 나온다. 별칭 얘기를 빼더라도 디렉토리가 항상 단일한 패키지 역할을 하기 때문에 디렉토리 구조가 프로그램의 동작과 유관하다. 그리고 가장 큰 문제로 다른 언어에 비해 꽤 쉽게 순환 참조를 만드는 편이다. 패키지의 어떤 함수만 불러오는 경우가 없고 그냥 통으로 패키지 임포트를 하므로 그렇다. 아마 Go를 사용하면서 모킹을 한 패키지로 모으려고 해본 경험이 있다면 쉽게 순환 참조 문제를 만났을 것 같다. 팀에서도 이러한 문제를 자주 만났었다. Go InterfaceGo의 인터페이스는 Duck Typing(덕 타이핑)으로, 명시적인 구현을 선언할 필요 없이 인터페이스를 구현만 한다면 다형성 조건을 만족하게 된다. 즉, implements 같은 구문이 필요 없다. 이러한 특징으로 인해 굉장히 느슨한 연결이 가능하다. 구현체는 인터페이스를 알 필요도 없기 때문이다. 그래서 개인적으로 패키지 간 연결이 굉장히 매끄럽고 진짜 정확한 의미의 인터페이스를 사용한다고 느껴진다. 예를 들어 A 패키지는 DoSomething이라는 인터페이스를 갖춘 어떤 타입이든 주입하면 사용할 수 있는 어떤 함수를 만들었다고 쳐보자. B 패키지는 이를 구현한 상태라고 했을 때 A 패키지에는 B에 대한 정보가 일절 필요 없다. 1234567891011121314151617181920212223// package domaintype Helloer interface { Hello() string}func HelloPrinter(helloer Helloer) { fmt.Println(helloer.Hello())}// package koreantype Korean struct { // ...}func (k Korean) Hello() string { return &quot;안녕하세요.&quot;}// package mainfunc main() { k := korean.Korean{} domain.HelloPrinter(k) // OK} 고민해 볼 Go ConventionProject Layout Convention공식적인 건 아니지만 이미 꽤 유명한 가장 기본적인 디렉토리 구조가 있다. Go 팀으로부터 오피셜이 아니라고 표기해달라는 요청까지 받은 이 Github Repository가 사실상 표준(de facto)이다. 가장 대표적인 디렉토리는 cmd, internal, pkg 디렉토리인 것 같다. cmd프로젝트의 메인 애플리케이션을 다믄 공간이다. 실행할 수 있는, 즉 메인 패키지와 메인 함수가 들어있으며 여러 Entry Point로 나눠질 수도 있다. 즉, cmd/web, cmd/cli 등 여러 main 함수가 디렉토리로 나눠질 수 있다. 보통 여기 작성되는 메인 패키지 코드는 다른 디렉토리의 코드를 가져와서 실행시키는 역할만 하는 작은 코드를 담는다고 한다. 하지만 “작다”라는 말에 너무 신경 쓰지 않는 것이 좋다. 프로그램을 실행시키기 위해 필요한 동작을 하는 곳이기도 하다. 예를 들어 필요한 의존성을 만들어 주입한 다음 사용하는 것도 일반적으로는 여기서 할 일이다. 보통 “재활용이 가능한 영역인가”를 기준으로 이곳에 둘 코드를 정하면 좋을 것 같다. 예를 들어 동일한 의존성 주입 코드가 여러 cmd 아래의 디렉토리에서 사용된다면 이는 cmd에 있을 필요는 없다. internalPrivate 코드를 담는 공간이다. internal 패키지 아래에 담긴 코드는 상위 디렉토리 혹은 동일 Depth의 다른 디렉토리에 있는 코드에서 사용할 수 없도록 Go 언어 수준에서 강제하고 있다. 예를 들어 SDK를 제공하는 입장에서 안전하지 않은 내부 동작을 숨기고 싶은 경우 이 디렉토리 아래에 패키지를 구성할 수 있다. 가장 최상단의 internal이 아니더라도 어디서든 마찬가지이다. internal/a/internal 패키지는 internal/b 패키지에서 사용할 수 없다. 굳이 내부적으로 internal 패키지를 만드는 경우는 못 봤다. 정말 거대한 팀에서 거대한 프로젝트를 모노리스로 만들고 있다면 필요할 수도 있을 것 같다. 보통 서비스 코드를 작성해야 하는 경우는 이 패키지 아래 많은 코드를 담는 경우가 많다. SDK처럼 굳이 내보내야 할 코드가 없기 때문이다. 약간 다른 언어의 src와 유사하게 사용되는 경향이 있는 것 같다. pkginternal과 반대로 노출하고자 하는 패키지를 이곳에 담는다. 이 패키지를 사용하려는 외부 개발자들은 pkg 디렉토리에 담긴 함수, 타입, 값 등을 안정감 있게 사용하도록 한다. pkg 디렉토리는 과거에 Go Module이 없던 시절에 익숙할 GOPATH의 pkg 디렉토리의 영향을 받은 건가 싶다. 개인적으로 보통 SDK를 제공할 때 굳이 임포트 경로에 pkg를 포함하고 싶지 않은 마음이라, SDK를 개발할 땐 그냥 루트 디렉토리를 사용한다. Package ConventionReference에 적어둔 것처럼 여러 패키지 컨벤션을 확인했다. 하지만 컨벤션은 보통 디렉토리 구조를 어떻게 해야 한다든지, 아키텍처가 어떻게 되어야 한다든지 이런 얘기를 하지는 않는다. 하지만 아예 없는 건 아니고 디렉토리 패스라든지, 어떤 형태로 만들라는 얘기가 조금 나온다. 공통으로 다음과 같은 것들이 있다. 단 하나의 패키지로 모든 API를 다루려고 하지 마라패키지 이름을 interface, model과 같이 만들고 모든 인터페이스나 모델들을 하나의 패키지에서 관리하는 것을 지양하라는 소리다. 대신 책임에 따라 패키지를 구성하라고 말한다. 예를 들어 유저를 관리하는 책임을 지는 패키지 이름을 user라고 만들고 그 안에 UserService 같은 인터페이스를 넣을 수 있다. 이를 다른 표현으로 “Organize by responsibility“라고 하기도 한다. 패키지 경로를 표현으로써 사용하라정확히 패키지 경로를 표현으로 쓰라는 문구를 본 것은 아니지만, Go 블로그 글을 보면 공식 패키지 경로가 다른데 같은 패키지 이름을 갖는 것은 전혀 이상한 게 아니며 오히려 명확한 표현이라는 내용이 나온다. 예를 들어 crypto, image, container, encoding 같은 패키지는 하위 경로에 같은 범위(ex. 이미지를 다루는, 알고리즘을 다루는 범위 등)에서 동작하는 패키지를 다루고 있다. 패키지 자체는 독립적으로 취급되지만, 상위 패키지 경로는 패키지를 불러오는 import 구문에서 패키지 표현의 일부로 활용된다. 이러한 관점에서 runtime/pprof와 net/http/pprof 패키지는 같은 이름을 갖지만, 명확히 다른 동작을 할 것을 예상할 수 있다. 이러한 구체적인 Style Decision은 조금 더 원론적인 내용에 해당하는 Style Guide 측면에서 봤을 때 “반복을 피하라“와 같은 내용과도 연관이 된다. 예를 들어 httppprof라고 패키지 이름을 짓지 않는 이유는 패키지 이름이 쓰기 싫게 생긴 것도 있지만 이미 경로에서 표현하고 있는 정보를 반복 표현하는 것을 피한 것이다. Code Convention코드는 이번 글 주제에서 더 자세한 범위에 속하지만, 어떻게 코드를 작성할지 머릿속에 그릴 수 있어야 패키지를 나눌 수 있다. 공식 라이브러리의 코드들이 어떤 느낌으로 작성되고 있는지 확인해 보려 한다. 인터페이스는 사용하는 쪽에서 작성한다필자는 Go뿐만 아니라 모든 언어에서 “인터페이스”라는 용어를 동일한 맥락에서 사용하는 거라면 인터페이스를 사용하는 쪽에서 해당 인터페이스를 정의하는 방법이 맞다고 생각한다. “이런 동작을 하는 녀석을 데려오면 내가 사용해서 내 목적을 달성해 주지” 형태로 사용하는 방향이 다형성 관점에서 더 적합한 방향이다. 밥을 먹이는 Feeding 동작을 수행하려면 Animal이라는 인터페이스를 구현하고 있어야 한다. 이때 Animal이라는 인터페이스는 Feeding 패키지에 위치해야 자연스럽다는 의미다. 12345678910111213141516171819package tycoontype Animal interface { Eat(string) error}func Feed(animal Animal) error { return animal.Eat(&quot;john mat taeng food&quot;)}// main.gopackage mainfunc main() { duck := new(Duck) if err := tycoon.Feed(duck); err != nil { panic(err) }} 인터페이스는 최대한 작게 작성한다하지만 이런 경우를 생각해 보자. 우리는 Animal이라고 불리는 묵직한 인터페이스를 가지고 있다. 이 인터페이스는 다음과 같이 여러 기능을 수행해야 하며, 여러 패키지에서 이를 사용하고 싶어 한다. 이런 경우는 생각보다 많다. 개발자들은 반복을 싫어하는데, 이런 경우 Animal은 어떻게 되는 걸까? Feeding, Riding, Hunting에 각각 Animal 인터페이스를 만들고 싶지는 않다. 위의 경우는 Animal이라는 인터페이스는 사실 너무 거대한 존재이다. 다음과 같이 쪼개어 인터페이스를 정의하는 것이 더 올바르다. 그래서 Go의 인터페이스 이름 짓는 컨벤션 중에 -er를 붙이고 단일한 수준의 동작만 정의한 아주 작은 인터페이스를 만드는 것이 있다. fmt.Stringer, io.Writer 등을 예로 들 수 있다. 바로 머릿속에 “근데 우리가 SDK 개발만 하는 것도 아니고… 우리는 UserService와 같은 거대한 인터페이스를 어쩔 수 없이 만든다고요…”라는 생각이 들 수도 있다. 이를 위해서 우리는 경로를 패키지 일부로써 활용해야 한다. 의존성이 아예 없는 루트 패키지부터 의존성의 말단에 위치한 패키지까지 계층적으로 패키지를 구성해야 한다. 계층적으로 구성하면 10 Depth가 넘는 패키지 패스가 만들어질 것처럼 느껴지지만 실제로 해보면 그렇지도 않다. 너무 과도하게 깊어지는 것 같다면 적절한 수준에서 패키지를 위로 끌어 올려도 상관없다. 패키지는 어떤 Depth에 있든 독립적인 패키지로서 동작하기 때문이다. 자세한 방법은 예시 패키지를 구성하면서 설명하려고 한다. 고민해 볼 Hexagonal Architecture출처: 헥사고날(Hexagonal) 아키텍처 in 메쉬코리아 필자가 가장 약한 부분이 이런 코드 레벨의 아키텍처 설계라고 생각한다. 여전히 Hexagonal Architecture(육각형 아키텍처)라고 불리는 방법론에서 사용하고 있는 이름들이 헷갈린다. 육각형 아키텍처의 핵심적인 레이어(의 표면)는 Adapter(어댑터)와 Port(포트)라고 생각된다. 어댑터의 바깥쪽은 통신 프로토콜, Socket 등 프로세스의 아예 바깥을 얘기한다. 따라서 어댑터는 프레임워크에서 HTTP 요청을 받아주는 역할이거나 CLI Flag를 받아오거나, 이벤트 메시지를 생산하거나 소비하는 클라이언트 등이 있을 수 있다. 어댑터를 기준으로 육각형 안쪽부터는 프로세스, 즉 우리가 핸들링하는 코드에 해당한다. 어댑터들의 도움으로 받아온 데이터를 애플리케이션의 순수한 비즈니스 코드를 수행할 수 있도록 포트를 통해 밀어 넣어야 한다. 이 아키텍처를 보면 “우리가 어떤 부분을 인터페이스화 해야 하는구나”를 느낄 수 있다. Service, Repository와 같이 도메인 코드를 동작시키는 인터페이스를 만들면 된다. 마무리패키지를 구성하기 위한 배경지식을 모두 정리했다. 말은 쉽지, 이제 코드를 보여주자. Reference The Go Blog: Package names golang으로 만나보는 Duck Typing BenJohnson - Standard Package Layout Style Guide Go Code Review Comments Effective Go Style guideline for Go packages Google Go Style Decisions 헥사고날(Hexagonal) 아키텍처 in 메쉬코리아","link":"/posts/go/go-pkg-architecture-theory/"},{"title":"Go에서 range의 모든 것","text":"Go 언어의 내장 함수들은 많지 않지만, 각각이 일당백 역할을 한다. make, new, range는 Go 프로그램을 작성할 때 넓은 범위에서 사용되는 내장 함수이다. 이번 포스팅에서는 range 활용에 대해서 정리해두었다. range는 for와 함께 사용된다. range의 뒷 부분으로 배열, 슬라이스, 맵, 또는 채널(특히, 값을 받는 역할을 하는)이 들어올 수 있다. 이터레이션 값들 (iteration values라고 표현함)이 있는 경우 그 값을 할당해주고 반복문의 블록으로 진입하게 된다. 1234for i, val := range intSlice { ...}// 가장 많이 사용되는 슬라이스 순환 range 문의 오른쪽 영역을 range expression이라고 하는데, 이 영역에는 다음 표현들이 들어올 수 있다. 배열, 배열 포인터 슬라이스 문자열 맵 채널 (받기 동작을 할 수 있어야 한다.) range 왼쪽의 = 또는 := 기준으로 그 다음 왼쪽 영역은 이터레이션 변수(iteration variables)라고 하는데, range expression이 채널인 경우는 최대 1개의 이터레이션 변수가 가능하고, 나머지는 최대 2개까지 가능하다. 0개 부터 2개까지 사용이 가능하다, 1개만 사용하면, 첫 번째 이터레이션 값이 할당된다. 123for [`iteration variables`] := range `range expression`// `range`는 `range expression`에 맞게 `iteration values`를 반복 생성// `iteration variables`에 할당한다. range expression를 x라고 했을 때, x는 루프가 시작할 때 딱 한 번 평가된다. 하나의 예외가 있는데, 최대 하나의 이터레이션 변수가 있는 상황에서 len(x) 값이 상수인 경우 range expression은 평가 되지 않는다. len(x)가 상수인 경우라는 뜻은, x가 상수 문자열이거나, 배열이거나 배열 포인터 이면서, 그 배열에 받는 쪽 채널이나 함수 호출등이 없는 경우에 해당한다. 자세한 내용은 이 링크에 나온다. range expression을 평가한다는 것은, 어떤 표현식인지를 판단하는 과정이다. 공부하면서 든 생각인데, 슬라이스 중간에 값을 추가하는 동작을 하더라도 이터레이션 횟수가 변하지 않는데, 이유가 표현식을 첫 루프 시작할 때만 평가하기 때문이 아닐까 싶다. 왼쪽에 이터레이션 변수가 있다면, 각 값에 대해 이터레이션 값이 생성된다. Range Expression 1st value 2nd value 배열, 슬라이스, 배열 포인터 인덱스 값 문자열 인덱스 룬 맵 키 값 채널 요소 - 배열, 슬라이스, 배열 포인터인 a가 range expression으로 사용된 경우: 첫 번째 값으로는 인덱스(i) 값이 0부터 오름차순으로 생성된다. 만약 이터레이션 변수가 하나만 제공된 경우, 0부터 len(a) - 1 까지의 값을 생성하게 된다. 두 번째 값으로는 a[i] 값을 생성한다. nil 슬라이스는 이터레이션 횟수가 0이다. 문자열의 경우, 바이트 인덱스 0부터 시작해서 유니코드 절을 반복한다. 연속 반복할 때 인덱스 값은 문자열에서 UTF-8 인코딩 된 코드 포인트의 첫 번째 바이트 인덱스가 되고, 두 번째 값은 해당 코드 포인트의 값이 된다. 반복 과정 중 잘못된 UTF-8 시퀀스를 만나면, 두 번째 값이 0xFFFD가 되고, 다음 반복에서는 바이트 단위로 진행되게 된다. map[K]V 타입의 맵 m의 경우, 첫 번째 값은 K 타입인 키 k이다. 두 번째 값은 V 타입인 값 m[k]이다. 반복 순서는 보장되지 않는다. 반복 도중에 도달하지 않은 엔트리가 제거된다면, 이터레이션 값이 생성되지 않는다. 만약 맵의 값이 반복 중 생성되는 경우엔 이터레이션 값을 만들 수도 있고 생략될 수도 있다. 만들어져 있던 엔트리들과 반복마다 다를 수 있다. 만약 맵이 nil인 경우 이터레이션 횟수는 0이다. 채널의 경우, 이터레이션 값은 채널이 닫히기 까지 받은 연속된 값이다. 만약 채널이 nil이라면, range expression이 영원히 블락되어버린다. 아래는 문자열의 경우 예시이다. 123456789for i, r := range &quot;→👍👎🌮🗂HelloWorld!안녕세상아!😊🚀🔥📝.&quot; { fmt.Print(i, &quot; &quot;) fmt.Println(string(r))}for i, r := range &quot;string, just string&quot; { fmt.Print(i, &quot; &quot;) fmt.Println(string(r))} 아래 문자열을 보면, 룬을 잘라 넣고 이동한 바이트 만큼 인덱스가 이동하는 것을 확인할 수 있다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445460 →3 👍7 👎11 🌮15 🗂19 H20 e21 l22 l23 o24 W25 o26 r27 l28 d29 !30 안33 녕36 세39 상42 아45 !46 😊50 🚀54 🔥58 📝62 .0 s1 t2 r3 i4 n5 g6 ,7 8 j9 u10 s11 t12 13 s14 t15 r16 i17 n18 g 다음은 맵에서 값을 추가하는 경우이다. 123456m := make(map[string]int)m[&quot;key&quot;] = 10for k, v := range m { m[k+&quot;next&quot;] = v + 1 fmt.Println(m)} 아래 결과를 보면, 임의로 추가된 엔트리가 출력되기도 하고, 안되기도 한다. 123456789101112131415$ go run main. gomap[key:10 keynext:11]$ go run main.gomap[key:10 keynext:11]map[key:10 keynext:11 keynextnext:12]$ go run main.gomap[key:10 keynext:11]map[key:10 keynext:11 keynextnext:12]map[key:10 keynext:11 keynextnext:12 keynextnextnext:13]map[key:10 keynext:11 keynextnext:12 keynextnextnext:13 keynextnextnextnext:14]map[key:10 keynext:11 keynextnext:12 keynextnextnext:13 keynextnextnextnext:14 keynextnextnextnextnext:15]map[key:10 keynext:11 keynextnext:12 keynextnextnext:13 keynextnextnextnext:14 keynextnextnextnextnext:15 keynextnextnextnextnextnext:16]map[key:10 keynext:11 keynextnext:12 keynextnextnext:13 keynextnextnextnext:14 keynextnextnextnextnext:15 keynextnextnextnextnextnext:16 keynextnextnextnextnextnextnext:17] Reference https://golang.org/ref/spec#For_statements https://golang.org/ref/spec#Length_and_capacity","link":"/posts/go/about-go-range/"},{"title":"Go Scheduler","text":"Go는 많은 것들을 “알아서” 해준다. 그래서 Go는 빌드 또는 실행 옵션이 다른 언어에 비해서 적은 편이다. Go의 가장 핵심적인 부분이라고 할 수 있는 고루틴 역시 Go의 런타임에서 알아서 관리해주고 있다. Go를 사용하면서 Go의 스케줄러를 알고 있어야만 하는 경우는 많지 않지만, 더 잘 쓰기 위해 조금 디테일한 런타임 동작에 대해 알아보자. Go Runtime Scheduler고루틴은 런타임 스케줄러에 의해 관리된다. 아래 원칙을 기준으로 고루틴을 적절히 스케줄링 한다. OS Thread는 비싸기 때문에 되도록 적은 수를 유지한다. 많은 수의 고루틴을 실행해 높은 동시성을 유지한다. N 코어의 머신에서 N개의 고루틴이 병렬적으로 동작할 수 있게 한다. 스케줄러가 동작하는 4가지 이벤트가 있다. 이 이벤트를 마주하면 스케줄러가 동작할 기회를 얻게 된다. go 키워드를 사용해 새로운 고루틴을 만들고자 할 때 GC가 동작할 때 시스템 콜을 사용할 때 동기화 코드(mutex, atomic, channel)가 동작할 때 GC는 일정 시간마다 트리깅 되도록 되어있기도 하고, 힙 영역을 할당할 때 특정 값을 넘어섰는지 확인하면서 필요한 경우 GC를 트리거 할 수 있다. 링크 Goroutine이 관리되는 방식Go는 G, M, P 구조체를 가지고 M:N 스레딩 모델을 구현하고 있다. 각각은 다음 의미를 갖고 있다. G: Goroutine M: Machine (OS Thread) P: Processor (고루틴을 동작시키는 가상 프로세서) P가 G, M 사이에서 스케줄링 역할을 담당하고 OS Thread가 코드를 동작할 수 있도록 한다. 보통 아래와 같은 이미지로 표현된다. Goroutine의 상태고루틴의 상태는 크게 세 가지로 나눠진다. Waiting: 이벤트 대기 상태. 시스템 콜, 동기화 콜(atomic, mutext, channel)에 의한 정지 상태. Runnable: 실행할 수 있는 상태. M 위에서 돌아가길 원하는 상태이다. Executing: 실행 중 상태. G가 P와 M과 붙어있는 상태를 의미한다. 위 그림을 확인해보면 Local RunQueue 안에 들어가 있는 고루틴이 Runnable 상태, M과 연결된 고루틴이 Executing 상태라고 볼 수 있다. OS 스레드는 필요할 때 만들고, 재사용을 위해 남겨둔다.스케줄러의 목적에 맞게 OS Thread는 최소로 유지된다. 다만 N개의 코어에서 최대 병렬 실행을 위한 수만큼은 생성된다. 그리고 만든 스레드는 스레드 종료 시스템 콜(pthread_exit)을 수행하지 않기 위해 유휴 상태로 남겨둔다. 이를 thread parking이라고 한다. 이렇게 유지되는 스레드를 활용해 빠르게 고루틴을 스레드에 스케줄링할 수 있다. 12345678//1. main-goroutine 실행func main() { ... go g1() //2. g1-goroutine 실행 ... go g2() //3. g1이 완료되고 나서 g2-goroutine 실행되었다고 가정} 위 코드는 아래와 같이 동작하게 된다. 메인 고루틴을 제외하고는 다른 고루틴이 없는 상태이므로, 현재 OS 스레드 상태는 m-main 한 개 g1 고루틴을 생성 후 RunQueue에 담는다. 런타임은 g1을 실행할 OS Thread인 m1 스레드를 만든다. P는 RunQueue에 있는 g1을 m1과 붙여준다. m1은 g1 프로세스가 종료되더라도 사라지지 않고 Parking(idle) 상태가 된다. 새로 g2 고루틴이 RunQueue에 올라간다. (이 시점에서 g1은 종료되었다고 가정한다. 만약 종료되지 않았다면 m2를 생성하고 붙여주는 위와 동일한 작업을 수행함.) 런타임은 Parking 상태인 m1을 Unparking 후 g2를 붙여준다. 이때 2-2의 P는 처음 고루틴을 만들고 RunQueue에 담아준 P일 수도 있고, M이 만들어진 다음 새롭게 붙은 P일 수도 있다. 일단 여러 P 구조체가 접근할 수 있는 Global Level의 RunQueue처럼 이해하고, 이후 P의 Work Stealing을 이해한 다음 다시 생각해보자. 그런데 위에 잠깐 언급된 것처럼, 동시 실행되는 고루틴이 아주 많이 생기면 계속해서 OS Thread를 만드는 상황이 생길 수 있다. 이 문제를 해결하기 위해서는 RunQueue가 접근하는 스레드 수를 제한할 필요가 있다. 스레드 수를 제한한다.스레드 수를 제한하지 않으면 스레드를 계속 생성하는 문제가 발생할 수 있다. 그래서 만약 스레드 수 제한에 도달하면 더 이상 스레드를 생성하지 않고 고루틴을 런큐에서 대기하도록 한다. Go에서는 이 제한값을 설정할 수 있는데 GOMAXPROCS라는 환경 변숫값을 사용한다. 최근 버전에서는 이 값이 머신의 CPU 코어 수로 설정되어 있다. 임의로 수정할 수도 있고 런타임에서는 runtime.GOMAXPROCS 함수를 사용해 설정할 수 있다. 1$ GOMAXPROCS=10 go run main.go 123func main() { runtime.GOMAXPROCS(10)} 예를 들어 GOMAXPROCS 값이 2인 상황에서 g1이 m1 위에서 돌고 있고, g2가 생성되어 RunQueue에 들어가 있는 상황을 생각해보자.12m0 - g0 (메인 고루틴 동작 중) | RQ : [g2]m1 - g1 (g1 고루틴 동작 중)현재 GOMAXPROCS 만큼 M이 있기 때문에 g2는 대기하게 된다. 만약 g0 고루틴에서 동기화 블락이 발생하면 (ex. 동기 채널로 g2가 보낸 데이터를 기다린다든지…) g0은 메인 스레드에서 빠져나오게 되고 g2가 메인 스레드로 가서 실행되게 된다.12m0 - g2 (g2 동작 중) | channel wait queue: [g0]m1 - g1 (g1 동작 중)왜 스레드를 조절하는 이름을 꼭 프로세스 조절 이름처럼 만들었을까? 이유는 이 값의 목적은 위에서 말한 것처럼 “OS Thread 수 조절”이 맞지만 실제 동작은 “가상 프로세서 P 숫자를 제어“하기 때문이다. 말 그대로 “최대 프로세서 수”라는 뜻이다. 무슨 차이가 있는 걸까? 위에서 고루틴이 실행 상태이기 위해서는 P와 M이 붙은 상황이어야 한다고 했다. 스케줄러 역할을 해줄 P와 실제 코드를 실행해줄 M이 필요하다는 뜻이다. 즉, 실행 상태인 고루틴은 P의 숫자에 종속적이다. 따라서 스레드 수는 늘어나도 그 스레드 M이 P와 함께 있는 상황이 아니면 코드를 실행할 수 없다는 뜻이다. M과 P가 붙어있을 수 없는 상황은 바로 시스템 콜을 수행 중인 M인 경우이다. 고루틴에서 시스템 콜을 호출해 OS 스레드가 블락되게 되면 해당하는 M과 G는 P 구조체와 분리되고 P는 새로운 M과 연결되면서 RunQueue에 있는 다른 고루틴을 스케줄링한다. 블락된 고루틴은 시스템 콜 작업이 끝나면 RunQueue로 돌아오게 된다. 이렇게 스레드가 블락 되었을 때 P를 M과 G에서 떼어내는 작업을 handsoff라고 한다. 이 특징 덕분에 P가 멈추지 않고 다른 고루틴을 새로운 M에 붙여줄 수 있게 되므로 고루틴이 기아 상태에 빠지지 않도록 해준다. 고 런타임에서는 블락된 고루틴을 확인하기 위해 백그라운드 모니터 스레드를 별도로 사용하고 있다. 이 스레드는 고루틴들이 블락되는 것을 감지했을 때 유휴 상태 스레드가 없다면 새로운 M을 만들어 P에 붙여주고 만약 유휴 상태의 스레드가 있으면 해당 M과 P를 활성화한다 이러한 구조때문에 Go는 M:P:N 멀티 스레딩 모델이라고도 불린다. 위 내용은 src/runtime/proc.go 파일의 handsoffp 함수 주석에서 자세히 확인할 수 있다. 이런 특징이 코드를 짤 때 어떤 문제를 발생시킬 수 있을까? 우리는 GOMAXPROCS를 가지고 OS Thread 수를 컨트롤할 수 있다고 생각할 수 있지만, 실제로는 그렇지 않다는 점이다. 예를 들어서 파일 100개를 고루틴으로 동시에 열어서 작업을 수행하는 것을 가정해보자. 이 경우 블락된 OS 스레드에서 P를 분리하고 새로운 OS 스레드 M을 만드는 작업을 하므로 이론상 100개가 넘는 스레드가 만들어질 수 있다. 다음 예시 코드는 100개의 고루틴을 돌려서 파일을 만들고 쓰는 작업을 한다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package main import ( &quot;fmt&quot; &quot;os&quot; &quot;runtime/pprof&quot; &quot;sync&quot;) func main() { threadProfile := pprof.Lookup(&quot;threadcreate&quot;) fmt.Printf(&quot;thread count before start: %d\\n&quot;, threadProfile.Count()) var wg sync.WaitGroup wg.Add(100) for i := 0; i &lt; 100; i++ { go func(n int) { defer wg.Done() filename := fmt.Sprintf(&quot;files/%d-file&quot;, n) f, err := os.Create(filename) if err != nil { panic(err) } defer func() { if err := f.Close(); err != nil { fmt.Println(err) } err := os.Remove(filename) if err != nil { fmt.Println(err) } }() var str []byte for j := 0; j &lt; 1000; j++ { str = append(str, byte(j)) } _, err = f.Write(str) if err != nil { panic(err) } }(i) } wg.Wait() fmt.Printf(&quot;threads count aftre program: %d\\n&quot;, threadProfile.Count()) } 123$ go run main.gothread count before start: 5threads count aftre program: 77 시스템 콜 중 Non-Blocking I/O를 사용하는 경우가 있다. 가장 대표적으로 네트워크 I/O의 경우에는 epoll을 사용해 Non-Block으로 응답을 대기한다. 이 경우에는 M이 다른 고루틴을 수행할 수 있다. 네트워크 I/O로 블락이 발생한 고루틴은 Net Poller라고 하는 컴포넌트에서 대기하게 된다. Net Poller는 OS의 알림을 받고 고루틴을 다시 RunQueue로(특히, Local RunQueue로) 보낸다. 분산 RunQueue로 Lock 제거RunQueue가 Global RunQueue 형태였다면 여러 P에서 고루틴을 가져오기 위해 Lock을 사용해야 한다. Go는 Global RunQueue(GRQ) 역시 사용하기는 하는데 일단 기본적으로 지금까지 설명한 내용은 Local RunQueue(LRQ)를 사용한다. 각 P 구조체마다 RunQueue를 가지고 P와 연결된 스레드의 스택을 최대한 사용한다. 또한 P가 가지고 있는 G 안에서 새로운 고루틴을 만들게 되면 이 고루틴 역시 해당 P의 LRQ에 들어가게 된다. GRQ가 사용되는 시점은 몇 가지 있지만 대표적으로 LRQ가 가득 찬 상태에서 또 새로운 고루틴을 생성하려고 할 대 GRQ로 들어가게 된다. P가 만약 G를 M에 붙이지 않은 상태라면 M은 현재 놀고 있는 스레드라는 뜻이다. 이 상태에 있는 P와 M은 “Spinning Thread“라고 한다. 이 상태에서 P는 M에 붙여줄 고루틴을 찾아야 한다. LRQ를 가장 먼저 확인하는데 만약 P가 LRQ에 고루틴을 가지고 있지 않은 상태가 되면 임의의 P의 LRQ에 있는 작업 절반을 훔친다. 이 과정을 Work Stealing이라고 한다. 이 과정을 통해 전체 작업을 고르게 분산할 수 있게 된다. 만약 Work Steal할 대상도 없는 경우에는 GRQ를 바라본다. 그래도 가져올 게 없으면 M과 P는 Parking 된다. Work-Stealing은 작업을 고르게 처리하도록 도와주지만, Locality를 떨어뜨린다. 고루틴은 생성 시 사용된 스레드에서 실행되어야 캐시도 활용하고 같은 메모리 스택을 사용하게 되는데 훔쳐지면 이 이점을 살릴 수 없다. 따라서 LRQ의 구조는 단순히 FIFO 구조가 아니라 맨 앞에는 LIFO 형태로 동작하는 버퍼를 사용한다. 위 이미지처럼 LIFO 버퍼가 비어있는 경우 그 버퍼에 들어가고 만약 새로운 고루틴이 바로 더 들어오면 버퍼에서 밀려 FIFO 큐로 기존 고루틴이 들어가게 되고 새롭게 Enqueue되는 고루틴이 해당 버퍼 자리를 가져간다. 이 우선순위가 있는 버퍼와 함께 새로운 고루틴이 3ms 가량 Work-Steeling 되지 않는다는 규칙이 있어서 어느 정도 Work-Stealing으로 인한 지역성 저하를 보완한다. Fairness스케줄링의 굉장히 중요한 요소 중 하나인 공평성이 보장되기 위해 여러 기법을 적용하고 있다. 이러한 특징을 Fairness라고 부른다. 스레드를 사용하는 고루틴이 10ms 이상 실행되지 않도록 한다. 이 타임 스판을 넘어가면 선점되어 GRQ로 들어가게 된다. LRQ의 구조를 보면 2개의 고루틴이 계속 반복적으로 스레드를 독차지할 수도 있는 구조라는 것을 알 수 있다. 이를 방지하기 위해 버퍼에 들어간 고루틴은 스레드를 반납하더라도 타임 스판이 초기화되지 않는다. 따라서 한 고루틴이 이 버퍼를 차지할 수 있는 시간은 10ms이다. P 구조체가 고루틴을 찾는 과정이 LRQ, Work-Stealing, GRQ 순서이기 때문에 GRQ의 고루틴이 기아 상태에 빠질 수 있다. 이를 방지하기 위해서 스케줄러는 61번마다 한 번씩 LRQ보다 GRQ를 우선해서 확인한다. 61이라는 숫자는 소수 중 경험적 테스트를 통해 나온 값이라고 한다. Net Poller 같은 경우엔 응답을 확인하는 별도의 스레드를 사용한다 이 스레드는 G M P 구조와 별도로 동작하므로 고 런타임에 의한 기아 상태에 빠지지 않는다. Go 스케줄러는 기본적으로 비선점적 방식이기 때문에 10ms, 3ms 등의 이벤트는 Best-Effort에 해당한다. 완전히 정확한 타이밍으로 동작하는 것은 아니다. 다만 1.12 버전 이후로 무거운 Loop가 돌면서 선점되지 않는 고루틴이 발생하는 것을 막기 위해 선점적 스케줄링 방식이 일부 도입되었다. 고루틴 재활용고루틴이 담고 있던 코드 흐름이 모두 완료되고 나면 고루틴을 보관한다. 123456789type p struct { ... // Available G's (status == Gdead) gFree struct { gList n int32 } ...} 위 구조체는 P 구조체인데, gFree에 유휴 상태의 고루틴을 모아둔다. 이 리스트를 유지함으로써 유휴 상태의 고루틴을 저장하거나 뺄 때 Lock같은 동작이 필요 없게 된다. 더 나은 고루틴 관리와 분배를 위해 스케줄러 자체적으로 글로벌하게 관리하는 리스트 두 개가 있는데, 하나는 재활용이 가능한 스택이 할당된 고루틴을 보관하는 리스트와 스택 재활용이 불가능해 스택을 해제한 고루틴을 보관하는 리스트이다. P가 관리하는 유휴 상태의 고루틴이 64개가 넘어가면 고루틴의 절반이 중앙 리스트로 이동하게 된다. 이때 고루틴이 추가적인 메모리를 할당 받아 2KB보다 큰 메모리 사이즈를 가지고 있는 경우가 재활용 불가능한 고루틴으로 판단되어 메모리를 할당 해제 후 보관하고, 그렇지 않은 경우 스택 메모리도 재활용해 사용한다. 이렇게 재활용하는 특성은 OS 스레드를 계속 만드는 것처럼 비슷한 문제를 야기할 수 있다. 즉, 고루틴을 계속 만들어내는 문제가 생길 수 있다. 12345678910111213141516func read(wg *sync.WaitGroup, gid int) { sem &lt;- struct{}{} // semaphore P() (Wait()) processing() // long process fmt.Println(&quot;Done&quot;, gid) wg.Done() &lt;-sem // semaphore V() (Signal())}func main() { var wg sync.WaitGroup wg.Add(100) for i := 0; i &lt; 100; i++ { go read(&amp;wg, i) } wg.Wait()} 위 코드는 일단 고루틴이 생성된 다음 실행 흐름을 판단하기 때문에, 고루틴은 무조건 계속 만들어진다. 따라서 고루틴이 만들어지는 시점과 흐름을 제어해야 하는 시점을 잘 판단해서 코드를 짜야 한다. Overall지금까지의 이야기로 다음 이미지를 이해할 수 있게 되었다. 이 이미지를 이해하기 글에서 각 컴포넌트들을 다시 살펴보자 Reference https://morsmachine.dk/netpoller https://ssup2.github.io/theory_analysis/Golang_Goroutine_Scheduling/ https://www.youtube.com/watch?v=KBZlN0izeiY&amp;ab_channel=GopherAcademy https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html https://rakyll.org/scheduler/","link":"/posts/go/go-scheduler/"},{"title":"Go GC","text":"Go는 메모리 관리를 런타임에서 해주는 프로그래밍 언어이다. 메모리 관리라고 하면 일반적으로 힙 영역에 할당하는 메모리들 더이상 스택에서 접근할 수 없는 상태가 되면, 할당 해제하는 가비지 콜렉팅을 의미한다. 이번 글에서는 GC에 대한 전반적인 이야기와 함께, Go에서는 구체적으로 어떤지 알아보았다. 💡 우선 글은 1.17 버전의 코드를 보면서 작성되었다.💡 글에서 GC라는 말은 “가비지 콜렉터”를 의미하기도 하고 “가비지 콜렉터의 동작”을 의미하기도 한다. 동사로 사용되었으면 콜렉터의 동작, 명사면 콜렉터라고 이해하면 좋을 것 같다. 흔히 알려진 설명GC에 대한 아주 개략적인 Overview이다. 현대 많은 언어는 GC와 함께 메모리 관리를 도와주고 있다. 일반적으로 프로그램에서 동적 할당을 하게 되면 프로세스의 힙(Heap) 영역에 메모리를 할당하게 되어있다. 1Person p = new Person(); // 힙 사용 이때, 힙 영역에 할당된 메모리를 할당 해제 해줘야 하는데, 이 과정을 개발자가 직접 하는 경우가 있고, 언어의 런타임 레벨에서 자동으로 해주는 경우가 있다. 이때 자동으로 해주는 컴포넌트의 이름이 GC이다. GC는 대략 다음과 같은 흐름을 갖는다. GC 수행 시간 동안 GC 스레드를 제외하고 모든 스레드 정지 GC는 참조할 수 없는 객체를 확인하고 메모리 할당 해제 GC가 끝난 후 정지된 애플리케이션 스레드를 다시 재개 1번의 정지되는 순간을 STW (Stop The World)라고 부른다. 이때 STW가 발생하는 순간은 GC 수행의 전체 과정이 아닐 수 있다. 어떤 알고리즘을 사용하는지에 따라 어떤 구간에서 STW가 발생할지 달라진다. 아무튼 GC가 발전하는 과정은 이 STW 시간을 줄이는 과정이고 GC를 튜닝하는 이유도 대부분 STW를 줄이기 위함이다. 알려진 방법들GC의 핵심적인 동작을 수행하는 두 가지 알고리즘을 가져왔다. 첫 번째는 Mark &amp; Sweep 방식이고, 두 번째는 Reference Counting이다. Mark &amp; Sweep이름이 아주 직관적인데, 말 그대로 지워야 하는 오브젝트를 마킹하고 청소하는 방법이다. 스택에서 힙을 참조하고 있는 루트 포인터를 찾아서 해당 루트 노드부터 체이닝 하면서 접근할 수 있는 오브젝트를 제거 대상에서 제외한다. 모두 순회하고 나서는 아직 제거 대상에 있는 오브젝트를 할당 해제하는 방식이다. Go와 JVM, JS에서 이 알고리즘을 사용한다. 이미지 출처: 링크 Reference Counting모든 오브젝트들이 참조 횟수 카운터를 갖고, 카운터가 0이 되는 오브젝트를 GC가 지우는 방식이다. 이 방법은 Python, PHP에서 사용 중인데, 근본적으로 순환 참조하고 있는 오브젝트에 대한 GC가 이루어질 수 없다. 이를 처리하기 위한 추가적인 컴포넌트와 함께 동작해야 한다. 여기 링크에서 여러 GC들의 할당과 해제 모습을 시각화해서 보여주고 있다. 여기 작성된 알고리즘 외, 추가로 몇 가지가 더 설명되어 있으니 궁금하다면 위에서 간략하게 소개된 방법들에 대해 알아보면 좋을 것 같다. GC를 구성하는 것들아! Go, JVM, JS는 Mark &amp; Sweep! 끄덕 끄덕, 하고 끝나면 좋겠지만 편한 프로그래밍의 뒷면은 그렇게 단순하지는 않다. 위에서 “알려진 방법들“로 소개한 방법들은 핵심적인 콜렉터의 동작 알고리즘에 관한 내용이고, GC를 구현한 언어에 따라 추가적인 기술이나 컴포넌트가 존재한다. Java의 GC가 굉장히 대표적이고 유명하다는 생각이 들어서, Go의 GC에 대한 구체적인 내용을 설명하기 전에 JVM에서 사용하고 있는 GC의 구성을 조금 더 살펴보고 이를 Go와 비교해보려고 한다. 세대별 GCGenerational GC라고 불리는 GC 방법이다. 세대별이라는 말은 힙 영역을 세대별로 나눠 관리한다는 것을 의미한다. “세대”는 오래 살아남은 객체와 그렇지 않은 객체를 구분 짓는 것을 의미한다. 이 GC는 다음과 같은 대전제를 바탕으로 설계되었다. 대부분의 객체는 금방 접근 불가능 상태가 된다. 오래된 객체에서 새로운 객체를 참조하는 일은 드물게 발생한다. 위 대전제의 이름은 Weak Generational Hypothesis라고 한다. 이 가설을 이용해 Old 객체를 담는 영역과 Young 영역의 객체를 담는 영역으로 힙을 나눈다. 1234567 &lt;---- Tenured ----&gt;+----------+---+---+---------+---------+| Eden | S | S | | Virtual |+----------+---+---+---------+---------+&lt;----- Young ------&gt;S: Survivor Young 영역: 새롭게 생성된 객체가 위치한다. 가설대로 많은 객체가 이곳에서 새로 만들어졌다가 사라진다. 이곳에서 발생하는 GC는 Minor GC라고 불린다. Old 영역: Young 영역에서 살아남은 객체가 여기 복사된다. Young 영역에 비해 크기가 크고, GC는 덜 자주 발생한다. 이곳에서 발생하는 GC는 Major GC 또는 Full GC라고 한다. 이 방법을 통해 일반적인 상황에서는 Minor GC로 간단하게 GC를 수행하게 된다. 큰 힙 영역을 다 확인할 필요 없이 일부만 확인할 수 있으므로 GC 속도가 빠르다. 💡 따라서 넓은 범위를 확인해야하는 Full GC가 자주 발생하는 상황은 문제가 있는 상황일 수 있다. 만약 2번 전제 상황이 발생하였을 때 GC가 어떻게 Old 영역이 참조하고 있는 Young 영역의 객체를 할당 해제하지 않을 수 있을까? 이를 위해 Old 영역에서 Young 영역의 객체를 참조하고 있는지 기록하는 Card Table을 사용한다. 이 테이블은 512 바이트의 청크로, Old 영역을 모두 확인하지 않고도 이 부분을 확인함으로써 Young 영역의 객체가 지워지는 것을 방지할 수 있다. Compaction힙 영역에 메모리를 할당하고 해제하는 과정이 반복되면 단편화 문제가 발생할 수 있다. 짧게 단편화에 대해 설명하자면, 전체적인 메모리 양은 요청된 메모리를 할당하기에 충분한 양인데, 연속되지 않아서 할당할 수가 없는 상황을 외부 단편화라고 부른다. 메모리가 비효율적으로 사용되고 있는 상황이고, 이런 파편화된 메모리 상태에서는 메모리 할당을 위해 메모리 공간을 찾는 시간도 늘어난다. 💡 Mark-Compact 방식을 쉽게 찾아볼 수 있었는데, 위에서 간단히 설명한 Mark &amp; Sweep 방식에서 컴팩팅을 추가한 방식이다. 마킹 페이즈 이후 컴팩팅 페이즈가 존재해서 데이터들을 압축하고 이동한 오브젝트의 포인터를 업데이트 하는 과정을 거치게 된다. Go의 GC이제 Go에서 어떻게 GC를 구성하고 있는지 확인해보자. Go의 코드 주석으로 설명된 바에 따르면 Go는 비세대별, 비압축, Concurrent Tri-color Mark &amp; Sweep이라고 한다. 비세대별: 힙 영역을 세대별로 관리하지 않는다. 비압축: 힙 영역의 Compaction을 수행하지 않는다. Concurrent Tri-color Mark &amp; Sweep: 마킹과 해제 과정이 STW 없이 애플리케이션과 동시에 동작하고, 삼색 마킹 알고리즘으로 구현되어 있다. CollectorGo GC는 세 개의 페이즈를 수행한다. 이 페이즈들 중 두 개는 STW를 유발하고, 다른 한 페이즈는 애플리케이션의 CPU 처리량을 느리게 만든다. 세 개의 페이즈는 다음과 같다. Mark 준비 - STW Marking - Concurrent Mark 종료 - STW Mark 준비 - STWGC가 시작되면서 가장 먼저 해야 할 일은 Write Barrier가 동작하도록(Enabled) 만드는 것이다. Go에서 Write Barrier는 동시적인 GC 마킹 과정에서도 힙 영역의 데이터 정합성을 유지해주는 장치이다. 위에서 살짝 써놨는데, 마킹 단계는 애플리케이션 고루틴과 GC 고루틴이 동시에 동작한다. 마킹을 하던 도중 애플리케이션 고루틴에서 힙 영역에 대한 변경 작업을 하게 되면 GC도 이를 인지하고 적절한 조치를 취해야 한다. 이것을 가능하게 해주는 것이 Write Barrier이다. 구체적으로 어떻게 해주는지는 이후 설명한다. 💡 Write Barrier라는 용어나 컴포넌트가 Go GC의 특수한 개념은 아니다. 동시적인 힙 영역에 대한 접근을 하기에 앞서 필요한 전처리 작업을 해주는 장치 정도로 사용이 되는 것 같은데, Java에서는 Old 영역에서 Young 영역을 참조할 때 Card Table에 기록하는 역할을 Write Barrier가 한다. Write Barrier가 시작되려면 모든 애플리케이션의 고루틴들이 멈춰야 한다. 일반적으로 이 동작은 아주 빨라서 STW가 거의 발생하지 않는 것처럼 보인다. Marking - ConcurrentWrite Barrier가 켜지고 나면 마킹이 시작된다. GC가 이 단계에서 처음 하는 일은 25% 정도의 CPU 처리량을 가져오는 것이다. 예를 들어 4개의 P가 있으면 그중 하나는 GC를 수행하기 위해 점유(dedicated)된다. 💡 위 이미지는 Go의 고루틴 스케줄링에 대해 알고 있으면 이해가 편한데, 만약 모른다면 사용 중인 스레드 중 하나가 점유된 이미지라고 이해하자. 그러나 엄밀히 말하면 틀린 소리기 때문에 시간이 된다면 Go GMP 구조에 대해 알아보자. 그다음 진짜 마킹을 하게 된다. 일단 현재 존재하는 모든 애플리케이션 고루틴 스택을 확인하면서 힙을 참조하고 있는 포인터를 확인한다. 스택을 스캔하는 과정은 해당 고루틴을 멈추게 한다. 하지만 그 이후 힙 안에서 오브젝트들을 따라가는 과정은 애플리케이션 고루틴과 동시에 동작한다. 다만 25%가량의 CPU 처리량을 사용하지 못하기 때문에 그만큼의 성능 저하가 발생한다. 만약 할당 속도가 너무 빨라서 고루틴이 사용 중인 힙 메모리 한계에 도달 전에 마킹 작업이 완료되지 못한다면 어떻게 될까? 할당이 지속되어 해당 오브젝트를 마킹 하느라 마킹 작업이 끝나지 않는다면? 이 상황이면 고루틴의 할당 속도를 낮출 필요가 있다. GC가 힙 할당 속도를 제어해야 하는 상황이 되면 애플리케이션 고루틴 중에서 마킹 작업을 도와줄 어시스트 고루틴을 선정한다. 이를 Mark Assist라고 부른다. 애플리케이션 고루틴이 Mark Assist 역할을 하는 시간은 힙 영역에 추가되는 데이터 양에 비례한다. Mark Assist가 선정되면 그만큼 애플리케이션의 할당 속도는 줄고, 마킹 작업 속도가 빨라지는 효과가 있다. 그러나 애플리케이션 로직을 수행하는 비율이 더 줄어드는 것이기 때문에 속도 저하의 원인이 되기도 한다. Tri-color Mark &amp; Sweep에 대해 자세히 알아보자. 아래 이미지가 알고리즘 방식이다. 이미지 출처: 링크 먼저 모든 오브젝트는 하얀색 집합에서 시작한다. 루트 오브젝트를 회색 마킹한다. 회색으로 마킹된 오브젝트를 순회하면서 참조하고 있는 오브젝트들을 회색으로 칠한다. 순회를 마친 회색 오브젝트는 검은색으로 마킹한다. 3, 4번 스탭을 회색 오브젝트가 없어질 때까지 반복한다. 여전히 흰색 집합에 있는 오브젝트를 할당 해제한다. 위 과정은 STW 상태가 아니기 때문에 동시에 오브젝트 변경이 지속해서 발생한다. 위에서 언급한 것처럼 GC가 동작하는 도중에 애플리케이션 고루틴이 힙에 변경을 가하면 Write Barrier가 적절한 조치를 취한다. 예를 들어서 GC 도중 스택에서 새롭게 할당하는 오브젝트는 바로 검은색으로 마킹한다. 이미 존재하는 오브젝트 트리 구조에서 변경점이 생기면 Write Barrier에서는 변경이 생기기 전 Original Pointer와 변경이 생긴 New Pointer를 기록하고 두 포인터 모두 마킹 처리를 한다. Original Pointer에 마킹처리를 하는 이유는 포인터 값을 스택이나 레지스터에 복사해두는 경우, Write Barrier를 거치지 않기 때문이다. Write Barrier는 힙 영역을 대상으로 발생하는 변경 점에 대한 전처리 작업을 하는 것이기 때문에, 로컬 스택이나 레지스터에 복사가 발생했는지 알 수 없다. 1234567[go] b = obj[go] oldx = nil[gc] scan oldx...[go] oldx = b.x // b.x를 Write Barrier를 거치지 않고 로컬 변수 oldx에 복사한다.[go] b.x = ptr // Write Barrier는 원래 b.x 값 역시 체크한다.[gc] scan b...//만약 Write Barrier가 원래 값을 마킹하지 않는다면 oldx가 스캔 되지 않는다. 위와 같은 상황처럼, 스택에 복사된 상태로 사용할 때, 스캔하면서 할당 해제되는 상황을 막아준다. New Pointer 역시 마킹 처리하는 이유는 다른 고루틴에서 포인터의 위치를 바꿀 수 있기 때문이다. 1234567[go] a = ptr[go] b = obj[gc] scan b...[go] b.x = a // Write Barrier는 새로운 b.x 값을 마킹하도록 한다.[go] a = nil[gc] scan a...//만약 새로운 값을 마킹하지 않는다면, ptr 값은 스캔 되지 않는다. 위 상황처럼 만약 Write Barrier가 없다면 이미 스캔을 진행한 오브젝트에 아직 스캔을 진행하지 않은 포인터가 붙고 기존의 포인터를 담던 변수에서 제거되면 해당 힙 오브젝트가 스캔 되지 않을 수 있다. 이런 이유로 Write Barrier가 Original Pointer, New Pointer 모두 마킹 작업을 수행하도록 만들어주고, 동시적인 상황에서도 안전하게 힙 마킹을 유지할 수 있다. Mark 종료 - STW마킹 작업이 끝나면 Write Barrier와 Mark Assist를 종료하고 다음 GC가 동작할 목표치를 계산하게 된다. 이 과정은 STW 없이 동작할 수 있는데, 구현 시 코드 복잡성이 과하게 증가하는 반면 그에 비해 얻는 이점이 너무 작아 STW 상태로 진행된다고 한다. 다음 GC 수행을 위한 목표치 계산 알고리즘을 Pacing Algorithm이라고 부른다. 알고리즘은 콜렉터가 실행 중인 애플리케이션의 힙 사이즈 정보와, 힙에 가해지는 강도(Stress)에 의해 정의된다. Go에서는 GC Percent 값을 Go 환경 변숫값으로 설정해 GC가 동작하는 속도를 조절할 수 있다. 이 환경 변수 이름은 GOGC인데, 기본값은 100이다. 이는 현재 정리된 이후 힙 메모리보다 100% 커지면 다시 GC가 동작한다는 것을 의미한다. 즉, 기본값으로는 대략 2배 사이즈가 될 때마다 GC가 동작한다. Sweep 과정?어떤 글에서는 Sweep 페이즈에 대해 따로 페이즈로 나눠서 설명하기도 하는데, 이는 GC 사이클과 조금 독립적으로 동작하기 때문에 GC의 페이즈로 설명하지 않았다. Sweep은 애플리케이션과 함께 동시적으로 동작하는데, 애플리케이션에서 힙 영역에 할당을 요청했을 때 필요한 경우 삭제 처리된 오브젝트를 게으르게 할당 해제한다. 즉, 할당 시점에 Sweep이 발생하고 GC 수행 시간과는 무관하다. 그리고 다음 GC가 수행되기 전까지 아직 청소되지 않은 메모리 영역이 있다면, 모두 클린업 처리해주면서 다음 GC가 시작된다. 비압축 방식압축을 통해 단편화 문제를 해결할 수 있는데, Go는 이 방법을 사용하고 있지 않다. 그렇다면 이 문제는 어떻게 해결하고 있을까? 이 문제는 현대 메모리 할당 방식에서 많이 해결해주고 있다고 한다. 전통적으로 프로세스 안에서 힙을 공유해 메모리를 할당해주는 방식은 멀티 스레드 프로그래밍에서는 그다지 적합한 방식이 아니다. 힙에 접근해 할당하는 과정에 Lock이 필요하기 때문이다. Go는 Google에서 만든 TCMalloc이라는 메모리 할당 방식을 활용하고 있다. 💡 “TCMalloc Like”라고 표현하던데, TCMalloc 방법을 사용했다고 이해해도 무방할 것 같다. 메모리 할당 방법 Overview조금 개괄적으로 설명하자면, TCMalloc은 중앙 힙과 함께 스레드마다 로컬 스레드 캐시를 가지고 있고, 작은 할당은 로컬 스레드 캐시에서 해결한다. 필요에 따라 로컬 스레드 캐시에 새로운 메모리 영역을 할당해주거나, 중앙 힙에서 직접 큰 메모리 덩어리를 떼어 사용하기도 한다. 로컬 스레드 캐시로 인해 Lock이 필요 없는 할당이 빠르게 진행되기도 하고, 힙의 파편화된 영역을 최소화할 수 있는 원리로 작용하는 것 같다. 아래 구체적인 내용은 몰라도 남은 내용들을 이해하는 데 문제가 없다. 궁금한 사람들은 보기로 하자. 작은 메모리 할당위에서 짧게 설명했지만, 작은 메모리를 할당하는 전략과 큰 메모리를 할당하는 전략이 다르다. 작은(32kb 이하) 할당을 할 때는 로컬 캐시인 mcache라고 불리는 메모리를 가져오려고 한다. 이 캐시는 32kb 짜리 청크 리스트인 mspan 리스트를 가지고 있다. 이미지 출처: 링크 고루틴 G를 처리하는 P에서 물고 있는 mspan 중 하나의 캐시를 사용해서 작은 범위의 할당을 한다. 이 과정은 힙 영역이 아니라서 Lock이 불필요하다. mspan은 32kb를 여러 사이즈로 나눈 여러 종류로 가지고 있다. 8bytes부터 32kb까지 클래스가 나눠진다. 이미지 출처: 링크 그럼 만약 할당하려고 할 때 이 mspan 리스트에 충분한 슬롯이 없다면 어떻게 될까? Go는 중앙에 mcentral이라고 하는 메모리 공간을 관리한다. mcentral에는 두 가지 종류의 스판 리스트가 있다. 하나는 꽉 찬 스판과 다른 하나는 그렇지 않은 스판 리스트이다. 이미지 출처: 링크 mcentral에서는 스판 리스트가 양방향 연결 리스트로 되어있다. mcache에서 mspan이 꽉차게 되면 mcentral에서 빈 스판 리스트를 가져온다. 이미지 출처: 링크 만약 mcentral에서 제공할 수 있는 리스트가 없으면 힙에서 새로 할당받는다. 이미지 출처: 링크 힙이 메모리가 더 필요한 경우 OS로부터 메모리를 가져온다. 이때 새롭게 할당하는 영역은 arena라고 불리는 커다란 메모리 덩어리이다. 64bits 아키텍처일 때 64MB를 할당받고, 32bits인 경우 4MB를 할당받는다. 큰 메모리 할당32kb보다 큰 메모리를 할당하게 되면 로컬 캐시를 사용하지 않는다. 할당되는 메모리 사이즈는 페이즈 사이즈로 올림 처리해 힙에 직접 할당한다. 대략적인 전체 흐름 이미지는 다음과 같다.이미지 출처: 링크 비세대별 GC힙 메모리를 스캔하는 범위를 좁히는 방법으로 비세대별 GC에 관해 설명했었다. Go에서는 이 부분이 도입되면 충분히 장점이 있을 것이라고 하지만, 현재는 도입된 상태가 아니라고 한다. Go에서는 컴파일 최적화 과정인 Escape Analysis 단계에서 다른 언어와 다르게 실제 동적 할당하는 많은 부분을 스택에 할당하도록 한다. 세대별 알고리즘의 대전제인 “많은 오브젝트들은 수명이 짧다”에 해당하는 부분을 스택에 할당함으로써 GC의 대상이 아니게 만들어준다. 따라서 다른 언어에 비해 세대별 GC를 사용하는 것으로 생길 수 있는 장점이 비교적 작다. 일단 여기까지 내용이 Go의 GC가 어떻게 동작하는지, 그리고 왜 이런지에 관한 내용이다. 이후는 GC를 컨트롤하려는 케이스를 예시로 가져왔다. 위 내용을 모두 포함하고 있어서, 잘 이해했다면 아래 내용이 재밌다. Case StudyGC Tuning 옵션에 관한 이야기dotGo 2019 컨퍼런스에서 Go GC를 어떻게 쓸 수 있는지 설명한 얘기가 있다. Go는 GC 관련 설정을 할 수 있는 방법이 위에서 언급한 GOGC 환경 변숫값 하나뿐이다. 다음 두 가지 상황에서 GOGC가 어떻게 될지 설명하고 있다. 상황 1: 안정적인 큰 데이터셋이 있다면?예를 들어서 20GB가 고정된 사이즈의 데이터라고 해보자. GOGC=100이라면 다음 GC는 40GB가 될 때 발생한다. 메모리 낭비가 굉장히 심한 상황인데 GOGC=50으로 바꾸면 30GB에 동작하게 바뀐다. 상황 2: 고정된 데이터 사이즈가 없는 애플리케이션 (작은 힙을 가지고 시작)10MB의 힙 사이즈를 들고 시작했다고 가정해보자. GC는 20MB에 발생할 것이고, 정리되고 나서도 금방 다음 GC 사이클이 돌아온다. 이런 경우 GOGC 사이즈를 조금 여유있게 잡아주면 GC가 덜 발생한다. 위 컨퍼런스의 내용을 대충 요약하면 고정 메모리 소비량이 많으면 메모리 효율성을 위해 GOGC 값을 줄이고, 그 반대 상황에서는 GC 사이클을 줄이기 위해 GOGC 값을 크게 만들자는 내용이다. 굉장히 단순한 방법. Twitch에서 Go 애플리케이션의 힙 사이즈를 수동으로 조절해 GC OPS를 줄인 이야기Twitch는 Visage라는 프론트앤드가 바라보고 있는 API Gateway 앱을 가지고 있다. 이 앱은 EC2 + LoadBalancer 위에서 돌고있는 Go 애플리케이션이다. AWS 컴포넌트로 기본적인 스케일링 처리가 가능하지만, 애플리케이션 자체적으로 CPU 처리량이 급격히 떨어지는 상황이 있었다고 한다. Twitch에서는 이를 “리프레시 스톰”이라고 불렀다. 인기 있는 방송인의 인터넷 상태가 안 좋아지는 경우 시청자들이 다 같이 새로고침을 연타하는 경우 생기는 문제이기 때문이다. 이 경우에는 평소보다 약 20배가 넘는 트래픽을 유발한다고 한다. 트위치는 Go 프로파일링 옵션을 프로덕션에서도 켜놔서 쉽게 프로파일링 결과를 얻을 수 있었는데, 다음과 같은 보고를 얻었다고 한다. 안정적인 상태에서는 GC가 초당 8 - 10회 발생 (8 ~ 10 OPS) 30%의 CPU 사이클이 GC와 유관한 함수를 호출하기 위해 사용 리프래시 스톰 상황에서는 GC OPS 급증 평균적인 힙 사이즈는 450MiB 💡 프로파일링 옵션을 켜두는 것이 그렇게 오버헤드가 있지는 않다고 한다. Excution tracer는 오버헤드가 있을 수 있는데 시간당 몇 초 정도 수행할 정도로 수행 빈도가 별로 안된다고 한다. GC OPS를 줄이고 STW 시간을 줄일 목적으로 밸러스트(바닥짐, Ballast)를 수동으로 만들어줬다. 앱이 시작할 때 아주 큰 메모리 사이즈를 힙에 할당해버리는 방법이었다. 123456func main() { // 10 GiB 할당 해버리기 ballast := make([]byte, 10&lt;&lt;30) // 앱 실행 진행 // ...} 기본 GOGC를 유지한 상태였기 때문에, 밸러스트를 만듦으로써 약 10GB의 할당이 더 발생해야 GC가 동작했다. 결과적으로는 GC OPS가 99% 감소했다. 이미지 출처: 링크 CPU 활용도 30%가량 내려갔다. 이미지 출처: 링크 GOGC를 설정하지 않고 직접 밸러스트를 만든 이유는 다음과 같다. GC 발생 비율은 관계가 없고, 총 메모리 사용량이 더 중요한 상황 밸러스트와 같은 효과를 발생시키려면 아주 큰 GOGC가 필요한데, 그렇게 하면 힙에 유지되는 메모리의 크기 변경에 아주 민감해짐 라이브 메모리와 변화하는 비율을 추론하는 것 보다, 전체 메모리를 추론하는 것이 훨씬 쉬움 그렇다면 소중한 10GiB 메모리가 그대로 소비되는 것은 아닐까? 실제 시스템 메모리는 OS에 의해 페이지 테이블을 통해 가상 주소가 지정되고 물리 메모리와 매핑된다. 위 밸러스트를 설정하는 코드가 실행되면 가상 메모리에 배열이 할당되고 실제 읽기 쓰기를 시도하면 페이지 폴트가 발생하면서 실제 메모리에 적재하는 과정이 발생한다. 따라서, 밸러스트가 물리 메모리를 차지하고 있지는 않다. API 레이턴시 역시 많이 향상되었는데, Twitch는 처음에는 STW 자체가 줄어서라고 생각했지만, 실제로 STW가 줄어든 절대적인 시간 자체는 아주 짧았다. 실제로 성능 향상에 많은 영향을 줬던 것은 Mark Assist가 줄었기 때문이다. 위에서 언급했던 것처럼 Mark Assist가 동작하면 애플리케이션 입장에서는 CPU 처리량을 더 뺏기는 것이기 때문에 처리량이 줄어든다. Reference https://deepu.tech/memory-management-in-programming/ https://spin.atomicobject.com/2014/09/03/visualizing-garbage-collection-algorithms/ https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/generations.html https://d2.naver.com/helloworld/1329 https://en.wikipedia.org/wiki/Mark-compact_algorithm https://cs.opensource.google/go/go/+/refs/tags/go1.17.8:src/runtime/mgc.go https://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html https://programming.vip/docs/deep-understanding-of-go-garbage-recycling-mechanism.html http://goog-perftools.sourceforge.net/doc/tcmalloc.html https://medium.com/a-journey-with-go/go-memory-management-and-allocation-a7396d430f44 https://groups.google.com/g/golang-nuts/c/KJiyv2mV2pU/m/wdBUH1mHCAAJ https://en.wikipedia.org/wiki/Escape_analysis https://youtu.be/uyifh6F_7WM https://blog.twitch.tv/en/2019/04/10/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap/","link":"/posts/go/go-gc/"},{"title":"Golang + Lambda로 신규 유저 정보 이메일로 보내기 삽질기","text":"이번 글은 실패한 사례를 공유해두려고 한다. 실패라기 보다는 몰랐던 사실 때문에 방법을 수정하게 되었다. 결론적으로 말하자면, EC2에 스케줄링 하는 방식으로 수정되었다. 다만 람다에 배포하고 CloudWatch Event를 사용해 스케줄링 하는 과정까지는 진행했고, 해당 과정을 담았다. 서비스 중인 앱 중에서는 회원가입 신청한 유저의 신원을 직접 확인한 후 Activate를 해줘야 하는 부분이 있다. 회원 가입 후, 비개발인력이 데이터베이스에서 새롭게 가입한 유저를 확인하고, 몇 가지 확인과 등록 절차를 통해 유저를 등록시켜야 하는데, 비개발 인력이 하기 어려운 작업이라 매일 오전 9시에 전날 새로 가입한 유저 정보를 CSV로 만들고 메일로 보내는 스케줄링 작업을 Go로 만들어보려고 한다. 먼저 데이터베이스에서 내용을 가져와 CSV로 만들어내는 부분을 만든 다음, 메일 보내기 작업을 한 다음 RDS에 연결한 Lambda 배포까지 진행해보려고 한다. 서비스 코드 작성하기우선 Go 언어 사용이 생소하기 때문에 디렉토리 구조나 개발 환경에 대한 고민을 많이 했다. 가장 상위 src 디렉토리 아래, 테이블을 정의하는 models 디렉토리, ORM 로직을 담는 orm 디렉토리, 서비스 로직을 담는 services, 실행되는 main.go 형태로 구성했다. 먼저 만든 부분은 ORM에서 조건에 맞춰 쿼리를 넣는 작업이다. 데이터베이스는 PostgreSQL을 사용하고 있었고, 우리에게 필요한 부분은 전날 가입한 사람들이다. 먼저 이미 데이터베이스에 정의된 모델을 아래와 같이 가져오도록 정의했다. 123456789101112131415161718192021// src/models/models.gopackage models// Pharm pharm modeltype Pharm struct { ID int `gorm:&quot;Column:id&quot; json:&quot;id&quot;` Name string `gorm:&quot;Column:name&quot; json:&quot;name&quot;` Username string `gorm:&quot;Column:username&quot; json:&quot;username&quot;` JibunAddr string `gorm:&quot;Column:jibunAddr&quot; json:&quot;jibunAddr&quot;` RoadAddr string `gorm:&quot;Column:roadAddr&quot; json:&quot;roadAddr&quot;` DetailAddr string `gorm:&quot;Column:detailAddr&quot; json:&quot;detailAddr&quot;` Location string `gorm:&quot;Column:location&quot; json:&quot;location&quot;` PharmacistName string `gorm:&quot;Column:pharmacistName&quot; json:&quot;pharmacistName&quot;` Tel string `gorm:&quot;Column:tel&quot; json:&quot;tel&quot;` Phone string `gorm:&quot;Column:phone&quot; json:&quot;phone&quot;`}// TableName pharm table namefunc (Pharm) TableName() string { return &quot;pharm&quot;} Go의 ORM으로 잘 알려진 gorm을 사용하고 있다. 우리에게 필요한 모델을 설정했으니, 이제 쿼리를 작성해보자. 우선 데이터베이스를 연결하고, 연결된 데이터베이스를 넘겨주는 함수를 만들었다. 1234567891011121314151617181920212223// src/orm/layer.gopackage ormimport ( &quot;github.com/jinzhu/gorm&quot; // orm driver _ &quot;github.com/jinzhu/gorm/dialects/postgres&quot;)// DBORM ormtype DBORM struct { *gorm.DB}// NewORM get new orm settingfunc NewORM(config string) (*DBORM, error) { db, err := gorm.Open(&quot;postgres&quot;, config) return &amp;DBORM{ DB: db, }, err} 로직은 메인함수에서 진행이 되도록 작성했다. 12345678910111213141516171819202122232425262728293031323334// src/main.gopackage mainimport ( &quot;bdygMailScheduler/src/orm&quot; &quot;fmt&quot; &quot;log&quot; &quot;os&quot; &quot;github.com/joho/godotenv&quot;)func main() { godotenv.Load() DB_HOST := os.Getenv(&quot;DB_HOST&quot;) DB_USER := os.Getenv(&quot;DB_USER&quot;) DB_NAME := os.Getenv(&quot;DB_NAME&quot;) DB_PASSWORD := os.Getenv(&quot;DB_PASSWORD&quot;) ormConfig := fmt.Sprintf(&quot;host=%s port=5432 user=%s dbname=%s password=%s&quot;, DB_HOST, DB_USER, DB_NAME, DB_PASSWORD) db, err := orm.NewORM(ormConfig) if err != nil { panic(err) } defer db.Close() pharms, err := orm.GetYesterdayNewbie(db) log.Print(pharms, err)} ORM 로직을 아래와 같이 작성했다. 전날 가입한 유저의 정보를 가져오는 로직이다. 12345678910// src/orm/orm.gopackage ormimport &quot;bdygMailScheduler/src/models&quot;// GetYesterdayNewbie 어제 가입한 약사 유저 리스트func GetYesterdayNewbie(db *DBORM) (pharms []models.Pharm, err error) { return pharms, db.Select([]string{`name`, `username`, `&quot;jibunAddr&quot;`, `&quot;roadAddr&quot;`, `&quot;detailAddr&quot;`, `&quot;pharmacistName&quot;`, `location`, `tel`, `phone`, `&quot;createdAt&quot;`}).Find(&amp;pharms, `date(&quot;createdAt&quot;) = current_date - 1`).Error} 위와 같이 메인함수와 ORM로직을 작성한 다음, 서비스 코드를 작성했다. 핵심적인 서비스를 다루는 로직은 src/services 아래 두었다. csv를 만드는 서비스와 메일을 보내는 서비스를 따로 분리했다. 123456789101112131415161718192021222324// src/services/csvService.gopackage servicesimport ( &quot;bdygMailScheduler/src/models&quot; &quot;os&quot; &quot;github.com/gocarina/gocsv&quot;)// GetCsv create csv datafunc GetCsv(newbieList []models.Pharm) string { key := &quot;/tmp/newbie.csv&quot; file, err := os.Create(key) if err != nil { panic(err) } if err := gocsv.MarshalFile(newbieList, file); err != nil { panic(err) } return key} 위에서 한 가지 삽질했던 것이 있는데, 람다에서는 /tmp 임시적으로 파일을 생성할 수 있다고 한다. 123456789101112131415161718192021222324252627282930313233// src/services/mailService.gopackage servicesimport ( &quot;fmt&quot; &quot;time&quot; &quot;gopkg.in/gomail.v2&quot;)// From email sendertype From struct { Email string Password string}// SendMail send datafunc SendMail(from From, to []string, attachment string) { current := time.Now().Local() subject := fmt.Sprintf(&quot;%s 새로운 약국 가입자&quot;, current.Format(&quot;2006-01-02&quot;)) m := gomail.NewMessage() m.SetHeader(&quot;From&quot;, from.Email) m.SetHeader(&quot;To&quot;, to...) m.SetHeader(&quot;Subject&quot;, subject) m.Attach(attachment) d := gomail.NewDialer(&quot;smtp.gmail.com&quot;, 587, from.Email, from.Password) if err := d.DialAndSend(m); err != nil { panic(err) }} 서비스 코드를 작성했으니, 메인 함수에서 동작하게 호출해줬다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344// src/main.gopackage mainimport ( &quot;bdygMailScheduler/src/orm&quot; &quot;bdygMailScheduler/src/services&quot; &quot;fmt&quot; &quot;os&quot;)func main() { DB_HOST := os.Getenv(&quot;DB_HOST&quot;) DB_USER := os.Getenv(&quot;DB_USER&quot;) DB_NAME := os.Getenv(&quot;DB_NAME&quot;) DB_PASSWORD := os.Getenv(&quot;DB_PASSWORD&quot;) EMAIL := os.Getenv(&quot;EMAIL&quot;) PASSWORD := os.Getenv(&quot;PASSWORD&quot;) ormConfig := fmt.Sprintf(&quot;host=%s port=5432 user=%s dbname=%s password=%s&quot;, DB_HOST, DB_USER, DB_NAME, DB_PASSWORD) db, err := orm.NewORM(ormConfig) if err != nil { panic(err) } defer db.Close() pharms, err := orm.GetYesterdayNewbie(db) if err != nil { panic(err) } count := len(pharms) if count == 0 { return } csvKey := services.GetCsv(pharms) defer os.Remove(csvKey) emailSender := services.From{Email: EMAIL, Password: PASSWORD} to := []string{EMAIL} services.SendMail(emailSender, to, csvKey)} 이 정도로 작성한 뒤, 로컬에서 동작시켜보니 올바르게 작동했다. 이제 람다에 올리고 CloudWatch Event에 스케줄링을 해두면 되겠다고 생각했다. Gsuite의 보안 수준이 낮은 앱에서 로그인이 안되는 문제가 있었지만 2단계 인증 이후 앱 비밀번호?를 설정해 해당 비밀번호를 사용하게 했다. 람다에 올리는 과정은 공식 문서를 참고 했는데, 람다 함수의 진입점으로 설정해줘야 하는 부분이 있어서 아래와 같이 메인함수를 수정해줬다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// src/main.gopackage mainimport ( &quot;bdygMailScheduler/src/orm&quot; &quot;bdygMailScheduler/src/services&quot; &quot;fmt&quot; &quot;os&quot; &quot;github.com/aws/aws-lambda-go/lambda&quot;)// HandleRequest Go lambda functionfunc HandleRequest() { DB_HOST := os.Getenv(&quot;DB_HOST&quot;) DB_USER := os.Getenv(&quot;DB_USER&quot;) DB_NAME := os.Getenv(&quot;DB_NAME&quot;) DB_PASSWORD := os.Getenv(&quot;DB_PASSWORD&quot;) EMAIL := os.Getenv(&quot;EMAIL&quot;) PASSWORD := os.Getenv(&quot;PASSWORD&quot;) ormConfig := fmt.Sprintf(&quot;host=%s port=5432 user=%s dbname=%s password=%s&quot;, DB_HOST, DB_USER, DB_NAME, DB_PASSWORD) db, err := orm.NewORM(ormConfig) if err != nil { panic(err) } defer db.Close() pharms, err := orm.GetYesterdayNewbie(db) if err != nil { panic(err) } count := len(pharms) if count == 0 { return } csvKey := services.GetCsv(pharms) defer os.Remove(csvKey) emailSender := services.From{Email: EMAIL, Password: PASSWORD} to := []string{EMAIL} services.SendMail(emailSender, to, csvKey)}func main() { lambda.Start(HandleRequest)} 람다에 배포하기서버리스 프레임워크도 Golang을 지원하지만, 이번에는 간단하기도 하고 하나의 함수만 넣는 작업이기 때문에, 직접 콘솔을 사용했다. 함수는 zip으로 압축한 다음, 콘솔에서 직접 업로드 했다. 12GOOS=linux go build src/main.gozip function.zip main 일반적으로 람다를 만들듯 함수 생성하기를 통해 함수를 만든 다음, RDS를 사용하기 위해서 VPC 내부로 설정을 해줬다. VPC로 연결하기 위해서는 다음과 같은 권한을 요구로 한다. (역할 정책에 다음과 같은 권한을 추가해줘야 한다.) 12345678910111213141516{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ec2:DescribeNetworkInterfaces&quot;, &quot;ec2:CreateNetworkInterface&quot;, &quot;ec2:DeleteNetworkInterface&quot;, &quot;ec2:DescribeInstances&quot;, &quot;ec2:AttachNetworkInterface&quot; ], &quot;Resource&quot;: &quot;*&quot; } ]} EventBridge를 통해 크론잡을 추가해준다. 크론잡에 대해서는 이 글에서 한 번 다룬 적이 있다. 우리는 매일 오전 아홉시에 해당 이메일이 오길 원하기 때문에 아래와 같은 표현식을 사용했다. 1cron(0 18 * * ? *) 발생한 문제문제가 발생한 곳은 외부 네트워크로 요청을 보내는 것에 있었다. 메일을 보내는 부분에서 timeout이 발생했는데, VPC의 Public subnet에 위치시켜도 마찬가지였다. 친절하기로 소문난 AWSKRUG 슬랙 채널에 질문을 드린 결과 VPC 내부에 위치시켰을 때 Public, Private 상관 없이 외부 네트워크 요청을 할 때 IGW를 통하지 못한다는 답변을 받게 되었다. 그래서 해결 방안으로는 NAT Gateway를 만들거나, VPC Endpoint를 사용해서 SES를 쓰는 것이다. (또는 이번 태스크에서 선택한 람다를 포기하는 것…) 결론아쉽게도 람다를 포기했다. 이번 태스크에서 사용하기 위해 Go와 GORM을 공부하면서 시간 소비가 좀 있기도 했고, NAT Gateway로 간단하게 해결하자니 추가되는 금액을 부담할만한 사항이 아닌 것 같고… 해서 EC2에서 스케줄링을 넣기로 했다. 후기좋은 삽질이었다. Go언어로 만든 첫 프로젝트라서 재미도 느꼈다. 다른 프로젝트도 Go를 사용해보고 싶다. Reference https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/golang-handler.html","link":"/posts/backend/go-lambda-try/"},{"title":"gRPC를 지탱하는 기술","text":"gRPC는 쉽게 말해서 HTTP/2.0 위에서 동작하는 RPC이다. 국내에서도 당근마켓, 뱅크샐러드, 데브시스터즈 같은 조직들이 gRPC를 활용하기 시작하면서 이미 많이 알려진 커뮤니케이션 방법이 되었다. 이번에 gRPC를 사용하게 되면서 어떤 기술들로 구성되어 있고, 어떤 대안들과 비교했을 때 어떤 장점이 생길 수 있는지 공부했던 내용을 정리했다. gRPC의 특징유명한 오픈소스 기술들은 공식문서에 특징을 명확히 정리해서 보여준다. gRPC도 당연히 구글이 만든 잘된 프로젝트기 때문에 이러한 특징들이 간단명료하게 정리 되어있다. gRPC is a modern open source high performance Remote Procedure Call (RPC) framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in last mile of distributed computing to connect devices, mobile applications and browsers to backend services. 정리하자면 다음과 같은 특징이 있다. gRPC는 High Performance 여러 데이터 센터들 사이에 효율적인 연결 트레이싱, 헬스 체킹, 인증, 로드 밸런싱 포함 플랫폼 독립적으로 백엔드 서비스와 연결할 수 있음 조금 더 구체적인 설명으로는 Protocol Buffer를 사용해서 Binary Serialization(직렬화)를 했고, HTTP/2.0을 사용해서 Bi-Directional 스트리밍이 가능하다는 내용과, 기타 크로스 플랫폼, 스텁을 생성해줘서 간단하게 개발할 수 있다는 내용 등이 나온다. 이런 특징들은 쉽게 말해서 “우리 기술을 사용하면 이런 장점이 있어”를 설명한 내용이다. 전반적인 벤치마크를 가지고 이런 특징을 내세우고 있겠지만, gRPC 같은 경우 생각해보면 대안은 대충 두 가지 갈래로 나눠질 수 있다. 첫 번째는 REST API를 대체하는 것이고, 두 번째는 Binary 인코딩을 하는 다른 구현체, 예를 들어 Thrift 같은 것들이다. 사실 그런데 후자의 경우 보통 gRPC로 통일되고 있는 분위기이기도 하고, 보통 고민하고 있는 갈래가 보다 전통적인 방식의 HTTP/1.1 + JSON을 사용하는 REST API vs gRPC이기 때문에 이번 글 역시 REST API에 비해 어떤 장점을 설명할까에 집중해서 내부적인 부분들을 파헤쳐보려고 한다. 일단 설명 전이지만, REST API는 gRPC와 비교 범주가 조금 다르긴 하다고 생각한다. gRPC는 아주 구체적인 구현체이고 REST API는 이론에 가까운 단어이기 때문이다. 그런데 일단 전통적인 방식과 비교하자는 의미이다. REST API가 JSON을 사용해야 한다는 것도 아니고 HTTP/1.1을 써야만 한다는 것도 아니다. 그냥 일반적인 상황을 얘기하는 것이다. “HTTP/1.1 프로토콜 위에서 JSON 바디를 보내는 방법”을 줄여 간단히 REST API라고 작성한 것으로 이해하자. 위 장점 중 트레이싱, 헬스 체킹 등 에코 시스템 관련된 장점과 크로스 플랫폼이라는 특징은 REST API를 타겟으로 한 소리는 아니라고 볼 수 있다. 따라서 gRPC가 어떻게 REST API에 비해 High Performance, 효율적인 연결을 이룰 수 있는지 위주로 풀어보자. CORE 1: HTTP/2.0gRPC는 HTTP/2.0 위에서 동작하는 RPC라고 가장 처음 설명했다. gRPC는 HTTP/2.0 프로토콜을 구현하는 것으로 다음과 같은 이점을 가지게 된다. 컨넥션 수 감소 Long-live Connection Bi-Directional Concept HTTP/2.0의 기술적 목표기존 HTTP/1.X에서는 다음 문제들이 있었다. Plain Text (ASCII) 프로토콜이기 때문에 Human-Readable 하므로 디버깅이 쉽지만, 아스키 코드 특성상 비슷한 다른 코드들이 여럿 존재한다. 예를 들어 다양한 White Space 종류들과 Termination을 의미하는 여러 코드, New Line을 의미하는 여러 코드 등 파싱 단계에서 생길 수 있는 문제가 있기도 하고 보안상 문제가 되기도 했다. Plain Text라는 사실은 곧 불필요한 공간을 많이 사용하고 있다는 것을 의미하고, 이는 네트워크 친화적이지 않다는 것을 의미한다. HOL(Head Of Line) Blocking 문제를 발생시킬 수 있다. Parallel Connection을 통해 큰 오브젝트를 전달받을 때 비효율성을 해결하려고 했으나, 결국 컨넥션을 많이 사용해야 한다는 것을 의미한다. 서버의 구현 레벨에서 일반적으로 Connection은 스레드를 의미한다. 스레드는 메모리 등 컴퓨팅 자원을 소모하게 된다. 위 문제들은 현재까지도 어느 정도 감안하며 사용하고 있을 정도로 크리티컬하다고 판단되지는 않는다. 그런데 구글 정도의 사이즈는 이런 문제를 해결했을 때 오는 효용 역시 엄청나기 때문인지, 새로운 프로토콜을 개발했다. 처음 만들어진 것은 SPDY인데, 역사 얘기를 하지는 않을 것이고 결국 발전해서 HTTP/2.0이 되었다. HTTP/2.0의 디자인과 기술적 목표는 다음 같다. 네트워크 리소스를 더 효율적으로 쓸 수 있고, 지연을 줄일 수 있어야 한다. 기존 HTTP/1.1을 대체하는 방식이 아니라 확장하는 방법이어야 한다. Binary Framing LayerHTTP/2.0의 성능 향상과 확장을 담당하는 레이어이다. 위에서 언급한 것처럼 HTTP/1.X의 확장적 개념으로 도입되기 위해서 기존 애플리케이션 레이어에 별도로 추가된 새로운 레이어이다. 이 레이어에서는 클라이언트와 서버 사이 HTTP 메시지가 HTTP/2.0 방식으로 캡슐화 된 것인지 확인하고 변환해준다. 출처: Oreilly 위 그림처럼 애플리케이션 레이어에서 바이너리 인코딩 및 디코딩을 담당한다. HTTP/1.X에서 Plain Text를 사용해 \\n을 기준으로 작성하는 프로토콜이라면, HTTP/2.0에서는 프레임이라고 불리는 바이너리 포맷으로 전송된다. 따라서 HTTP/2.0을 사용하려면 클라이언트와 서버 모두 애플리케이션 레이어에 Binary Framing Layer를 지원하고 있어야 한다. 이렇게 새로운 레이어 도입이 필수적이라는 측면에서 기존 인프라와 Incompatible 하기 때문에 HTTP/1.2가 될 수 없다. HTTP/1.1 버전의 헤더 (메소드, 상태 코드, URI 등)은 그대로 유지하기 때문에 애플리케이션 레벨에서는 변경점이 필요 없다. Binary Framing Layer를 양측에 도입하기만 하면 HTTP/2.0으로 동작할 수 있다는 뜻이다. Stream, Message, FrameHTTP/2.0은 바이너리로 인코딩된 프레임을 주고받는다. 커뮤니케이션에 사용되는 컴포넌트들은 아래와 같다. Frame(프레임): HTTP/2.0 커뮤니케이션의 가장 작은 유닛이다. 각 프레임은 하나의 프레임 헤더와 바디가 있는데, 헤더에는 자신이 속한 Stream의 식별자 정보와 우선순위 정보가 있다. 이 프레임들은 Message를 구성하는 일부이거나 전체이다. Message(메시지): 온전한 메시지 시퀀스이다. 이 메시지는 논리적으로 HTTP/1.X 버전의 요청 또는 응답의 메시지와 같다. Stream(스트림): 성립된 TCP 컨넥션 위에서 양방향 바이트 플로우를 보낼 수 있는 통로이다. 하나 이상의 메시지를 운반할 수 있다. 출처: Oreilly 모든 커뮤니케이션은 하나의 TCP 컨넥션 위에서 발생하고, 컨넥션은 양방향 스트림 몇 개든 동작시킬 수 있다. 각 스트림은 유니크한 식별자가 있고, 추가적으로 우선순위에 대한 정보가 있다. 그리고 해당 정보들은 메시지를 전달할 때 메시지를 구성하는 각 프레임 헤더에 담기게 된다. Google이 병렬 처리를 더 잘하도록 만드는 방법으로 기존 자원을 더 작게 나눠 관리하는 레이어를 추가해주는 방법을 많이 사용하는 것 같다는 생각을 했다. Go의 Goroutine도 Thread를 나눠 사용함으로써 더 가볍게 병렬로 동작하는 구현을 했다고 느꼈는데, 이 방법도 기존 컨넥션 활용을 더 작은 단위의 스트림이라는 단위로 나눠 사용하는 것이라고 느꼈다. 본인이 이해한 느낌은, 실제 구현 레벨에서는 컨넥션으로 전달되는 데이터는 Frame이다. 나머지는 논리적으로만 Binary Framing Layer에서 처리를 한다. 기존에 텍스트 메시지를 전달하듯, 메시지 순서와 상관 없이 메시지를 프레임으로 나눠 전송하는 형태이다. 그런데 도착해서는 프레임 헤더의 Stream 식별 정보로 데이터를 재조립할 수 있다. gRPC가 HTTP/2.0을 활용한 방법gRPC는 HTTP/2.0을 구현하면서 확장적으로 사용하고 있다. gRPC에서 사용하고 있는 커뮤니케이션을 위한 컴포넌트는 다음과 같다. Channel(채널) RPC Message(메시지) 위 HTTP/2.0처럼 각 컴포넌트들은 포함 관계를 가지고 있다. 채널은 여러 RPC를 가질 수 있고 RPC는 여러 메시지를 가질 수 있다. 묘하게 연결되는 구석들이 느껴지긴 하는데 구체적으로 다음 그림처럼 표현할 수 있다. 채널들은 gRPC에서 핵심적인 컨셉이다. HTTP/2.0에서 Stream 여러 개를 하나의 컨넥션 위에서 동작시킬 수 있었는데, gRPC의 채널은 컨넥션을 여러 개 활용해서 마치 하나의 전송 통로처럼 사용하도록 추상화하고 있다. 이 부분이 구체적으로 어떻게 구성되고 있는지는 후술하고 있다. 아무튼, 표면적으로는 RPC를 올리는 하나의 간단한 인터페이스를 제공하는 것 같지만 이면에는 여러 컴포넌트들이 여러 컨넥션을 묶어 Alive 상태를 유지한다. 즉, 채널은 하나의 엔드포인트와 연결해주는 가상의 컨넥션이다. RPC는 이 컨넥션과 함께 본인이 해야 할 일을 수행한다. 해야 할 일이라고 하면 요청을 처리해 응답을 보내주는 역할을 하는 것인데, 커뮤니케이션 입장에서는 메시지를 주고받는 역할을 의미한다. RPC는 실제로 단순히 Stream 형태로 구현된다. 메시지는 HTTP/2.0의 메시지와 동일하고 RPC를 통해 전송된다. 조금 더 구체적으로는 프레임에 메시지를 “적재”하는 방법으로 동작한다. 약간 프레임을 경제적으로 사용한다는 느낌인 것 같다. layered 한다고 표현을 하는데, 프레임이 하나 이상의 메시지를 담을 수도 있고, 만약 메시지가 HTTP/2.0 스펙상 기본 프레임 사이즈인 16KB보다 크면 두 개 이상의 프레임을 사용할 수도 있다는 내용이었다. 정리하자면, Channel은 Connection(복수의 컨넥션을 추상화), RPC는 Stream, Message는 Message와 연결되는 개념이다. Resolver &amp; Load Balancer채널이 결국 여러 컨넥션을 활용하고 있는 가상의 컨넥션인데, 어떻게 컨넥션들을 관리하고 유지하고 있을까? 이를 위해서 여러 컴포넌트들이 사용되는데 핵심적으로 name resolver(resolver, 리졸버), load balancer(로드 벨런서)이다. 리졸버는 DNS 리졸버로부터 호스트 이름을 가지고 IP 주소를 질의한 다음 넘겨받은 IP 주소 리스트를 로드 벨런서에게 넘겨주는 역할을 한다. 로드 벨런서는 넘겨받은 주소들을 가지고 컨넥션들을 만들어 하나의 채널로 구성한다. 그리고 RPC가 채널에 들어오면 정해진 로드 벨런스 전략에 따라 RPC를 실제 컨넥션에 연결해준다. 일단 22년 2월 기준으로 기본은 Round Robin 방식인 것 같고 다른 방식으로는 pick_first 방식이 있다. 이 링크에서 로드 벨런싱을 설정하는 예시를 볼 수 있다. Channel과 유관한 코드들은 다른 언어에서는 Channel이라는 키워드를 쓰고 있는 것 같은데, Go에서는 이미 채널이라는 개념이 언어 레벨에 있다 보니, ClientConn이라는 이름을 사용하고 있다. 동작 관련된 내용을 보면 클라이언트에서 찾아볼 수 있겠구나라고 생각할 수 있고, 예시 역시 클라이언트 코드를 보면 확인할 수 있다. 예를 들어서, DNS 리졸버로부터 호스트에 대한 13개의 IP 주소를 알려줬다면, 라운드 로빈 벨런서가 13개의 컨넥션을 만들고 RPC를 배분해준다. Connection Management한 번 설정된 이후 gRPC는 리졸버와 로드 벨런서에 의해 정의된 대로 컨넥션 풀을 유지하게 된다. 그렇다면 컨넥션 실패를 경험하면 어떻게 될까? 일단 실패를 인지하는 단계가 있어야 하지만 구체적으로 어떻게 실패를 인지하는지는 나중에 얘기하고, 일단 실패를 발견하면 로드 벨런서는 현재 가지고 있는 주소 리스트를 기반으로 컨넥션을 새로 구성하려고 한다. 한편 리졸버는 호스트 이름의 주소 리스트를 새롭게 업데이트한다. 이유는 기본적으로 IP가 유동적인 개념이기도 하고, 특정 IP가 프록시 서버였다고 가정했을 때 이 서버에 문제가 생겨 내려간 상태라고 한다면 재시도를 할 필요 없기 때문이다. 이렇게 업데이트한 주소 목록을 다시 로드 벨런서에게 넘겨주면 로드 벨런서는 불필요한 컨넥션을 내리고, 필요한 컨넥션은 새롭게 만든다. 이런 방법으로 Long-live Connection을 안정적으로 유지되도록 한다. 실패한 컨넥션 찾기컨넥션 관리 사이클의 시작은 실패를 인지하는 것부터라고 했다. 실패한 컨넥션은 다음과 같은 케이스가 있다. Clean Failure: 실패에 대한 커뮤니케이션이 상호 진행됨 Less-clean Failure: 실패에 대한 커뮤니케이션이 진행되지 않음 첫 번째 케이스는 엔드포인트에서 의도적으로 컨넥션을 끊었을 때 발생할 수 있다. Graceful Shutdown이 진행 중이라든지, 타임아웃이 발생한 경우 그럴 수 있다. 이런 경우 TCP 레이어에서 FIN Handshake가 발생할 것이고, gRPC는 즉각 실패를 인지할 수 있다. 두 번째 케이스는 엔드포인트가 의도치 않게 죽었거나, 클라이언트에게 알리지 않고 종료한 케이스이다. 이 경우 클라이언트는 TCP 컨넥션 연결 유지를 최대 10분 동안 하고 있는다. 10분 동안 컨넥션의 사용 가능 여부를 판단하는 것은 말이 안되니까, gRPC는 HTTP/2.0의 Ping Frame을 사용해 이런 케이스를 판단한다. 다이얼 옵션 중 KeepAlive 옵션이 켜진 경우 gRPC는 주기적으로 HTTP/2.0 Ping Frame을 전송하는데, 이 프레임들은 HTTP/2.0의 흐름 제어 플로우에서 바이패싱(무조건 통과)된다. 그리고 컨넥션이 살아있는 경우만 응답을 받게 되므로, 만약 핑의 응답을 못 받으면 gRPC는 이를 실패로 인지한다. 두 케이스 모두 실패로 인지된 이후는 위에서 설명한 대로 로드 벨런서의 재연결 시도가 시작된다. KeepAlive 옵션은 지금 버전 기준으로 기본값이 True인 것으로 알고 있다. 위에서 언급한 기능 외 추가적으로 장점이 있는데, 프록시들에게 컨넥션의 라이브니스(Liveness)를 알려주는 용도로 많이 사용된다. 만약 서버와 클라이언트 사이 프록시 서비스가 사용되고 있을 때, 많은 프록시 서비스들이 일정 시간 동안 사용되지 않은 컨넥션을 불필요한 것으로 간주하고 닫아버린다. 예를 들어서 AWS ELB 서비스는 TCP 컨넥션이 1분간 사용되지 않으면 해당 컨넥션을 닫고, GCP의 경우 10분이면 닫는다고 한다. KeepAlive 옵션은 위에서 언급한 Ping을 보내는 방식으로 프록시 호스트에게 컨넥션이 사용 중임을 알리는 역할도 한다. CORE 2: Protocol Buffer지금까지 gRPC가 어떻게 HTTP/2.0을 활용하는지 얘기해봤다. 이제는 gRPC가 바디 사이즈를 줄이기 위해 활용한 바이너리 인코딩 메커니즘인 Protocol Buffer에 대해 얘기해보려고 한다. 프로토콜 버퍼는 구조화된 데이터를 바이너리로 직렬화하기 위한 구글의 메커니즘이다. 어떻게 사용하지에 대한 내용은 이 글에서 담지 않았다. 기본적인 얘기를 하자면 proto 확장파일을 가진 IDL을 정의한 다음 필요에따라 각 언어로 컴파일함으로써 쉽게 직렬화·역직렬화를 할 수 있는 Stub을 생성해 사용하는 구조이다. 메시지 구조1234message Example { int32 id = 1; string msg = 2;} 메시지 구조는 이렇게 생겼다. int32 id = 1;이라는 라인에서 int32에 해당하는 부분이 필드 타입, id에 해당하는 곳이 필드(필드 이름), 1에 해당하는 부분을 필드 넘버라고 한다. 어떤 글에서는 필드 넘버를 필드 아이디라고도 하고… 조금씩 표현이 다른 것 같다. 아무튼 이 글에서는 이렇게 표기하고 있다. 필드 넘버는 유니크한 값이다. 실제 인코딩 디코딩 시점에서 Key 역할을 하기 때문이다. JSON 타입은 문자열로 구성된 타입을 키로 사용하지만 Protocol Buffer는 간단히 숫자로만 표현한다. {&quot;id&quot;: Value}를 {1: Value}로 변환하는 느낌으로 보면 된다. 디코딩하는 시점에서는 개발의 편리를 위해 설정된 필드 이름으로 매핑된다. 이렇게 전송 과정에서 불필요한 텍스트를 다 버려 일차적으로 압축되는 효과가 있다. 인코딩프로토콜 버퍼 사이즈가 줄어들게 되는 이유로 가장 길게 설명하는 부분은 Varints이다. Varints라고 표현하는 이 인코딩 방법은 스칼라 정수형 값을 인코딩하는 방법이다. 이 방법은 정수를 하나 이상의 유동적인 바이트 수로 표현한다. 숫자가 더 작을 수록 더 적은 바이트만 사용하게 된다. 인코딩된 바이트의 첫 번째 비트는 MSB(Most Significant Bit)라고 한다. 이 비트는 현재 바이트가 마지막 바이트인지 알려주는 역할을 한다. 나머지 7비트는 숫자를 표현하기 위해 사용한다. 인코딩은 2진수의 작은 숫자 비트부터 시작해서 위로 7비트를 잘라 표현하는 리틀 엔디안 방식이다. 예를 들어 1을 인코딩하면 다음과 같다. 10000 0001 -&gt; MSB = 1 &amp; 나머지 비트로 1 표현 300을 표현해보자 11010 1100 / 0000 0010 1010 1100과 0000 0010 두 개의 바이트로 표현된다. 먼저 앞의 바이트의 MSB가 1이기 때문에, 그 뒤 바이트 역시 같은 수를 표현하기 위해 사용되었다는 것을 의미한다. 그리고 그다음 바이트의 MSB가 0이니까 그다음 바이트에서 끝나는 수이다. 두 바이트의 뒤 7비트를 뒤집어서 합쳐주면 원래 숫자가 나온다. (앞의 7비트가 더 아랫자리 수니까 뒤집어서 합친다) 1000 0010 ++ 010 1100 = 0001 0010 1100 = 256 + 32 + 8 + 4 = 300 이 방법은 사실 신박한 프로토콜 버퍼의 인코딩 방법이 아니라 과거에서부터 사용되던 압축 방법이다. “대규모 시스템을 지탱하는 기술”이라는 책에서 이 방법을 vbcode라는 이름으로 소개하고 있다. 과제로도 있어서 구현한 적이 있다. 필드 타입 &amp; 필드 넘버 인코딩위 방법은 메시지 타입이 Varint 타입인 경우 적용되는 방법이다. “메시지 타입”은 Wire Type이라고 문서에서 말한다. 위 표처럼 Wire Type은 0부터 5까지 총 6개 정수형으로 구성되어있다. 이를 표현하는 건 3개의 비트만 있으면 되고, 필드 넘버도 위에서 설명했던 것처럼 정수형으로 인코딩되기 때문에 둘이 합쳐서 메시지의 맨 첫 번째로 나오게 된다. 즉, 데이터 스트림 시작의 첫 바이트는 MSB(1) + Field Number(4) + Wire Type(3)으로 구성되어있다. 따라서 필드 넘버는 1 ~ 15까지의 숫자만 쓰는 것이 좋다. 그 이상 사용하게 되면 바이트를 추가로 써야 한다. 사실 1바이트 더 쓰는 게 뭐 그렇게 대수냐? 라고 생각할 수 있다. 그 말도 맞다고 생각하기 때문에 그냥 숫자 자체는 15이하로 쓰고 안되면 넘겨 쓰자는 정도로 이해하면 될 것 같다. 인코딩된 데이터를 받았는데 아래와 같이 생겼다고 가정해보자. 각 띄어쓰기로 구성된 부분이 바이트이고, 16진수로 표현된 숫자이다. 108 96 01 위 데이터 중 맨 앞부분의 바이트는 필드 정보를 나타내고 있다. 1234508 = 0000 1000 -&gt; MSB = 0, MSB 제거000 1000 -&gt; 뒤 세 비트가 0임을 확인 -&gt; varint 타입. 타입에 맞게 이후 바이트들을 디코딩0001 -&gt; 남은 비트는 1을 표현 -&gt; 필드 넘버는 1 따라서, 필드 넘버가 1이고 값은 Varint 타입인 값에 150 (96 01을 Varint 방식으로 디코딩하면 150)이 들어온 데이터라고 인식하게 된다. 작은 음수 인코딩음수가 나올 가능성이 확실히 있는 경우, 2의 보수 표현법으로 바이너리를 채우는 Varint 방식을 그대로 적용하면 불필요하게 너무 많은 바이트를 쓰게 될 수 있다. 특히 작은 숫자일 수록 1로 큰 수 쪽이 채워지기 때문에 그렇다. 이런 경우 sint(sint32, sint64)로 필드 타입을 설정하는 것이 효율성 측면에서 더 좋다. Signed Integer 타입은 음수값을 지그재그 인코딩으로 먼저 변환한 다음 Varint 인코딩을 수행한다. 지그재그 인코딩은 다음 표처럼 음수를 양수 사이사이에 껴두어 치환한 테이블을 가지고 인코딩을 하는 방식이다. 기타 다른 인코딩 방식은 단순히 필요한 바이트 길이를 미리 전달해주고 뒷부분을 바이트로 변환해 집어 넣는 length-delimited 방식의 인코딩과, double, float 처럼 고정된 바이트를 사용하는 경우 그대로 바이너리 값을 넣어주는 등, 큰 압축 효과 없이 데이터를 전달한다. 결론지금까지 gRPC를 지탱하는 핵심 기술에 대해 알아봤다. 실제로 FAQ에는 “gRPC가 HTTP/2 위로 바이너리를 보내는 것보다 나은 게 있는가?”라는 질문이 있다. 이 질문에 결국 gRPC가 하는 역할은 크게 보면 그걸 하고 있다고 답변한다. 물론 결국 답변은 +a가 있기 때문에 더 낫다는 답변이지만, 본질적으로 우리는 핵심 두 가지를 살펴본 것이다. 어떻게 쓰는 것이 좋지?그렇다면 gRPC는 앞으로 어떻게 써야 할까? 우선 HTTP/2.0이라는 특징과, 구체적으로 어떻게 구현해 사용하고 있는지를 알게 되었으니 Dial 옵션을 더 꼼꼼하게 알아볼 필요가 있다. 우리 상황에 맞게 더 잘 쓸 수 있는 방법이 있을지 확인해보고 프로젝트 환경에서 컨넥션을 효율적으로 사용할 수 있는 옵션이 무엇일지 찾아보고 적용해보면 좋을 것 같다. 그리고 Protocol Buffer를 사용하는 것 자체에서 주의해야 할 점 몇 가지가 있다. IDL에서 메시지에 새로운 필드를 추가하는 작업은 기본값 처리만 해두면 이전 시스템과 호환이 되기 때문에 비교적 자유롭게 추가할 수 있다. 그러나 삭제하는 작업은 고민해볼 필요가 있다. 삭제 자체는 문제 되지 않지만, 만약 삭제한 다음 해당 필드 넘버로 새로운 필드를 추가한다면, 과거 IDL을 사용하고 있던 시스템과 문제를 일으킨다. 따라서 값을 지울 때는 미래에도 이 값을 사용하지 않도록 키워드로 DEPRECATED 처리를 하든, reserved 키워드로 방어하든 조금 보수적으로 접근할 필요가 있다. 살짝 언급했지만, 프로토콜 버퍼에서 값이 없는 필드는 디코딩할 때 제로값(기본값)을 사용한다. 따라서 실제 값인지, 기본값인지 판단할 수 있는 로직이 필요하고 만약 그게 어렵다면 WKT의 Wrapper를 사용해 null (Go 에서는 nil) 타입이 제로값으로 들어가도록 한 번 감싸주는 것도 좋은 방법이 될 수 있다. IDL을 관리해야한다는 이슈도 있다. 보통 프로토콜 버퍼를 사용하는 조직은 전사적인 IDL 관리를 진행하고 있다는 얘기를 많이 들었다. 뱅크샐러드글에서도 그 내용이 나온다. IDL 관리, 버저닝 등 미리 팀과 얘기해볼 것들이 많이 있다. REST API는 대체되는가?결론 먼저 말하면 당연히 아니다. REST API에 비해 결론적으로 갖게 되는 장점은 다음과 같은 것들이 있을 수 있다. TCP 컨넥션 사용 효율성 증가 메시지 Header 압축으로 네트워크 사용 감소 Long-live Connection으로 Handshake Overhead 감소 페이로드 압축률 증가 그러나 클라이언트와 서버 모두 gRPC가 애플리케이션 레이어 앞쪽에서 동작해야 하는 영역이 있기 때문에, 그 부분을 자유롭게 손볼 수 없다면 사용이 어렵다. 대표적으로 Web 클라이언트는 온전히 gRPC 스텁을 동작시키지 못하기 때문에 HTTP/1.1에서 동작하는 방식으로 다운그레이딩되어 사용된다. 위에서 말한 장점들 역시 온전히 가져가지 못할 것이고, 그렇다면 굳이 그 환경에서도 gRPC를 써야 할지 의문이다. 프로토콜 버퍼의 공식문서를 보면 프로토콜 버퍼는 큰 데이터를 다루기에는 부적합한 방법이라고 설명한다. 메시지가 메가바이트 단위라면 다른 대안을 찾을 것을 권고하고 있다. 아마 인코딩 디코딩의 CPU Bound가 문제가 되는 것인가 싶다. 예를 들어 이미지 파일의 경우 스트림을 통해 보내주는 케이스가 있을 수도 있는데, 그냥 저장하고 있는 CDN 링크를 요구사항에 맞게 보내주는 것이 한 방법이 될 수 있다. Reference https://grpc.io/ https://grpc.io/blog/grpc-on-http2/ https://www.cncf.io/blog/2018/07/03/http-2-smarter-at-scale/ https://www.oreilly.com/library/view/http-the-definitive/1565925092/ch04s04.html https://www.oreilly.com/library/view/http2-a-new/9781492048763/","link":"/posts/backend/grpc-internals/"},{"title":"HTTPS 설명하기","text":"HTTPS를 잘 이해해보자. HTTP는 TCP 연결 이후 평문으로 요청을 전달한다. 즉, 중간에 탈취되었을 때 내용이 모두 노출된다는 것이다. 이런 문제를 해결하기 위해서 HTTPS를 사용한다. 어떻게 암호화 하는 건지 정리해봤다. HTTP Over Secure Socket Layer 의 약자이다. 평문을 암호화 해서 보내는 것이 핵심 기술이다. 여기에 사용되는 방식은 비대칭키, 대칭키 암호화 방식이 사용된다. 비대칭키 (공개키 암호 방식)공개키 암호화 방식이라고도 하는 비대칭키 방식은, 두 개의 키를 가지고 암호화 및 복호화가 이루어진다. 하나는 공개키, 다른 하나는 비밀키라고 불린다. 공개키는 누구나 알아도 되지만, 비밀키는 소유자만이 알고 있어야 한다. 공개키 암호 방식은 두 가지 역할을 할 수 있다. 첫 번째는 암호화, 두 번째는 인증이다. 암호화는 암호화 방식이라는 이름에 맞게, 컨텐츠를 암호화 하고, 그 내용을 특정 사람만 확인할 수 있게 하는 방식이다. 공개키로 암호화 한 내용은 개인키로 복호화 할 수 있고, 개인키로 암호화 한 내용은 공개키로 복호화 할 수 있다. 인증은 해당 내용을 특정 대상이 맞음을 확인하는 것이다. 비밀키로 암호화 한 내용은 공개키로만 복호화 가능하기 때문에, 이 내용을 특정인이 보냈음을 확신할 수 있다. 이 두 가지 기능이 HTTPS에서 모두 적절하기 잘 활용된다. 대칭키 방식은 암호화 하는 키와 복호화 하는 키가 같다. 인증HTTPS에서 인증이란, 자신이 받은 정보가, 올바른 사람이 전송한 것이 맞는지 확인하는 것을 인증이라고 할 수 있다. 공개키 방식은 누구나 공개키를 획득할 수 있기 때문에, 비밀키로 암호화한 정보는 암호화의 의미가 없다. 그러나 비밀키로 암호화한 정보를 공개키로 복호화 할 수 있다는 것은, 다시 말해 암호화 한 주체가 틀림없이 비밀키를 가지고 있음을 인증한다고 말할 수 있다. 이것이 “인증서”의 원리이다. SSL 인증서인증서는 다음 두 가지 역할을 하게 된다. 클라이언트가 접속한 서버가 신뢰할 수 있는 서버임을 보장한다. SSL 통신에 사용할 공개키를 클라이언트에게 제공한다. CA (Certificate Authority)클라이언트가 접속한 서버가 클라이언트가 의도한 서버가 맞는지를 보장하는 역할을 하기 위해서, 특정 민간 기업이 이 역할을 해준다. 이런 기업들을 CA, 혹은 Root Certificate라고 한다. 이 CA는 브라우저에 탑재되어 있다. 브라우저는 CA의 리스트와 함께 인증서를 복호화 하기 위한 CA의 공개키를 가지고 있다. 사설 인증 기관개발이나 사적인 목적을 위해, SSL의 암호화 기능을 이용할 때, 직접 CA 역할을 할 수 있다. 이 경우에는 브라우저가 경고를 표시한다. 서비스의 보증인증서 안에는 다음과 같은 내용이 포함되어있다. 서비스의 정보(CA, 서비스 도메인 등) 서버의 공개키 (공개키 내용 및 암호화 방법 등) 브라우저는 처음 서버에 접근할 때, 인증서를 받게 되고, 해당 인증서가 공인된 (브라우저에 저장된) CA 리스트 안에 있는 회사의 인증서임을 확인한다면, 알고 있는 공개키로 인증서를 복호화한다. 복호화가 가능하다는 것은 해당 인증서를 발급한 기관이 틀림 없이 보증된 회사임을 증명하는 것이다. 그렇다면 이제 브라우저는 위 “서비스의 정보”와 “서버의 공개키”가 생겼다. 이제 브라우저는 서버의 공개키를 통해 HTTP 평문을 암호화 할 수 있게 되었다. 이하 내용은 구체적으로 SSL이 어떻게 동작하게 되는 것인지를 담고 있다. SSL 동작실제로는 HTTP 내용을 비대칭키 방식으로 데이터를 암호화 하는 것이 아니다. 공개키 방식으로 암, 복호화 하는 과정은 컴퓨팅 파워의 소비가 필요하기 때문에, 실제 내용은 비교적 컴퓨팅 파워가 덜 소모되는 대칭키를 통해 암, 복호화 하게 된다. 우선 서버와 클라이언트가 연결되면서, Handshake 과정을 거치게 된다. 다음과 같은 과정을 지난다. Client Hello 클라이언트가 서버에 접속하고 다음 정보를 가져온다. 클라이언트에서 생성한 랜덤 데이터 클라이언트가 지원하는 암호화 방식: 클라이언트와 서버측이 사용할 암호화 방식에 대한 협상을 위해 클라이언트가 사용 가능한 암호화 방식을 전송한다. 세션 아이디: 이미 SSL Handshaking이 이루어진 상태라면, 기존 세션을 재활용하게 된다. 이때 사용할 세션 아이디를 서버에 전송한다. Server Hello 서버는 클라이언트 Hello를 받고 다음 응답을 보내준다. 서버측에서 생성한 렌덤 데이터 서버가 선택한 클라이언트 암호방식: 클라이언트가 전달한 암호화 방식 중, 서버에서도 사용할 수 있는 암호화 방식을 선택해 클라이언트로 전달한다. 인증서 클라이언트가 가지고 있는 CA 리스트에서 인증서를 확인한다. CA 리스트 중에 있다면, 가지고 있는 해당 CA의 공개키를 통해 복호화한다. 클라이언트는 서버의 랜덤 데이터와 클라이언트의 랜덤 데이터를 조합해 pre master secret이라는 키를 생성한다. 이 키는 이후 데이터를 암호화 하기 위해 사용된다. 위에서 언급했듯, 데이터를 암호화 하는 기법은 대칭키이므로 외부에 노출되어선 안된다. 이제 만든 pre master secret을 전달해야 한다. 이 값을 안전하게 전달하기 위해서 인증서에서 획득한 공개키로 암호화해 서버로 전송하게 된다. 서버는 pre master secret을 개인키로 복호화한다. 이제 클라이언트와 서버 모두 pre master secret 키 값을 가지고 있는데, 일련의 과정을 통해 master secret을 만든다. 그리고 이 키값을 통해 세션 키를 만들게 된다. 이 세션 키를 이용해 서버와 클라이언트는 데이터를 대칭키 방식으로 암호화한 후 주고 받는다. 클라이언트와 서버는 핸드쉐이크 단계의 종료를 서로에게 알린다. Reference https://brunch.co.kr/@sangjinkang/38 https://commons.wikimedia.org/wiki/File:Full_TLS_1.3_Handshake.svg https://opentutorials.org/course/228/4894 https://ko.wikipedia.org/wiki/%EB%8C%80%EC%B9%AD_%ED%82%A4_%EC%95%94%ED%98%B8","link":"/posts/backend/https-dive/"},{"title":"Nest + TypeORM TODO API 만들기","text":"우선 빠르게 배워봤으니 Nest와 TypeORM을 사용해서 국민 데모 앱 TODO API를 만들어보자. 앤드포인트는 아래와 같이 설계할 생각이다. GET /todos: 투두 리스트 POST /todos: 투두 만들기 GET /todos/:id: 투두 디테일 데이터 가져오기 PUT /todos/:id: 투두 디테일 상태 변경 (완료, 미완료) DELETE /todos/:id: 투두 삭제 TypeORM 붙이기먼저 프로젝트를 시작해보자. 아래 명령어로 간단하게 공식적으로 제공해주는 보일러플레이트를 사용할 수 있다. npx @nestjs/cli new nest-todo-api-demo TypeORM을 붙이기 위해서 이 링크와 이 링크를 참조했다. 먼저 필요한 데이터베이스 세팅을 해줬다. docker-compose.yml로 PostgreSQL과 pgAdmin을 띄우고 데이터베이스도 만들어줬다. 그 다음 필요한 패키지를 인스톨 해줬다. 1yarn add @nestjs/typeorm typeorm pg 투두 데이터는 아래처럼 설계했다. 123456789101112131415161718192021222324252627282930313233// src/models/todo.entity.tsimport { BaseEntity, PrimaryGeneratedColumn, Entity, Column, CreateDateColumn, UpdateDateColumn} from &quot;typeorm&quot;;@Entity()class Todo extends BaseEntity { @PrimaryGeneratedColumn() id: number; @Column({ type: &quot;varchar&quot;, length: 50 }) shortDesc: string; @Column({ type: &quot;varchar&quot;, nullable: true }) longDesc?: string; @Column({ type: &quot;boolean&quot;, default: false }) isDone: boolean; @CreateDateColumn() createdAt: Date; @UpdateDateColumn() updatedAt: Date;}export default Todo; 이후, app.module.ts에 TypeOrmModule을 임포트 시켰다. 123456789101112131415161718192021// src/app.module.tsimport { Module } from &quot;@nestjs/common&quot;;import { TypeOrmModule } from &quot;@nestjs/typeorm&quot;;import Todo from &quot;./models/todo.entity.ts&quot;;@Module({ imports: [ TypeOrmModule.forRoot({ type: &quot;mysql&quot;, host: &quot;localhost&quot;, port: 5432, username: &quot;postgres&quot;, password: &quot;postgres&quot;, database: &quot;todo-database&quot;, entities: [Todo], synchronize: true }) ]})export class AppModule {} forRoot() 메서드는 createConnection() 함수에 들어가게 되는 설정 값을 넣어줄 수가 있다. 게다가 몇 가지 추가적인 설정 값이 있는데, 아래와 같다. retryAttempts: database 연결 시도 횟수 (기본 10) retryDelay: 연결 시도 사이 딜레이 (기본 3000ms, 3초) autoLoadEntities: true인 경우, 엔티티들이 자동으로 불러와짐 (기본 false) keepConnectionAlive: true인 경우, 연결이 앱이 꺼져도 유지됨 (기본 false) 엔티티들을 자동으로 불러오지 않는 다는 건 무슨 말일까? 자동으로 불러오는 것이 기본이 아닌 이유는 뭘까? express에서 TypeORM을 사용하듯, 프로젝트 최상단에 ormconfig.json을 두고, forRoot() 메서드에 옵션을 안 넘겨도 된다. 위처럼 연결해주는 작업을 하고 나면, TypeORM Connection과 EntityManager 객체는 전체 프로젝트에서 주입 가능해진다. Todo 모듈 만들어주기모듈별로 구분해주고 싶으니까 src/modules/todo 디렉토리를 만들고 이 안에서 만들어보려고 한다. 우선 todo.controller.ts와 src/modules/todo/todo.service.ts를 만들었고, src/modules/todo/interfaces/dto.interface.ts도 만들었다. 123456// src/modules/todo/interfaces/dto.interface.tsexport interface ICreateTodoDto { shortDesc: string; longDesc?: string;} 1234567891011121314151617181920212223242526272829303132333435363738// src/modules/todo/todo.service.tsimport { Injectable } from &quot;@nestjs/common&quot;;import Todo from &quot;src/models/todo.entity&quot;;import { ICreateTodoDto } from &quot;./interfaces/dto.interface&quot;;import { Repository } from &quot;typeorm&quot;;import { InjectRepository } from &quot;@nestjs/typeorm&quot;;@Injectable()export class TodoService { constructor( @InjectRepository(Todo) private readonly todo: Repository&lt;Todo&gt; ) {} createOneTodo(createTodoDto: ICreateTodoDto) { return this.todo.create(createTodoDto).save(); } getTodoList() { return this.todo.find({ select: [&quot;shortDesc&quot;, &quot;isDone&quot;, &quot;createdAt&quot;], order: { createdAt: -1 } }); } getDetailTodo(id: number) { return this.todo.findOne(id); } async toggleTodo(id: number) { const before = await this.todo.findOne(id); await this.todo.update(id, { isDone: !before.isDone }); } removeOneTodo(id: number) { return this.todo.delete(id); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// src/modules/todo/todo.controller.tsimport { Controller, Post, Body, Get, Param, Put, Delete} from &quot;@nestjs/common&quot;;import { ICreateTodoDto } from &quot;./interfaces/dto.interface&quot;;import { TodoService } from &quot;./todo.service&quot;;@Controller(&quot;todos&quot;)export class TodoController { constructor(private readonly todoService: TodoService) {} @Post() async create(@Body() createTodoDto: ICreateTodoDto) { const ret = await this.todoService.createOneTodo(createTodoDto); return ret; } @Get() async getList() { const ret = await this.todoService.getTodoList(); return ret; } @Get(&quot;:id&quot;) async getOne(@Param(&quot;id&quot;) todoId: number) { const ret = await this.todoService.getDetailTodo(todoId); return ret; } @Put(&quot;:id&quot;) async toggleDone(@Param(&quot;id&quot;) todoId: number) { const ret = await this.todoService.toggleTodo(todoId); return ret; } @Delete(&quot;:id&quot;) async removeOne(@Param(&quot;id&quot;) todoId: number) { const ret = await this.todoService.removeOneTodo(todoId); return ret; }} 원래 계획했던 엔드포인트에 맞춰 컨트롤러를 만들어줬다. 아래는 service와 controller를 추가해준 src/modules/todo/index.ts와 src.modules/todo를 추가해준 src/app.module.ts이다. 1234567891011121314// src/modules/todo/index.tsimport { Module } from &quot;@nestjs/common&quot;;import { TodoController } from &quot;./todo.controller&quot;;import { TypeOrmModule } from &quot;@nestjs/typeorm&quot;;import Todo from &quot;src/models/todo.entity&quot;;import { TodoService } from &quot;./todo.service&quot;;@Module({ imports: [TypeOrmModule.forFeature([Todo])], providers: [TodoService], controllers: [TodoController]})export class TodoModule {} nest g module todo로 모듈을 만들 수 있는데, 이렇게 만들면 src/todo/todo.module.ts가 만들어진다. 다른 nest g service todo, nest g controller todo 모두 같은 공간에 만들어진다. 공식적으로는 모듈별로 디렉토리를 만드는 걸 제안하고 있는 것 같다. 123456789101112131415161718192021import { Module } from &quot;@nestjs/common&quot;;import { TypeOrmModule } from &quot;@nestjs/typeorm&quot;;import { TodoModule } from &quot;./modules/todo&quot;;import Todo from &quot;./models/todo.entity.ts&quot;;@Module({ imports: [ TypeOrmModule.forRoot({ type: &quot;mysql&quot;, host: &quot;localhost&quot;, port: 5432, username: &quot;postgres&quot;, password: &quot;postgres&quot;, database: &quot;todo-database&quot;, entities: [Todo], synchronize: true }), TodoModule ]})export class AppModule {} 결과 확인결과적으로 Postman으로 살펴보면 다음과 같이 나타난다. 전체 리스트. 처음엔 하나도 없다. 만들기 시도 isDone: true 만들기 디테일 결과 확인하기 삭제하기 진행한 TODO API 코드는 여기 깃헙 레포지토리에서 확인할 수 있다. 후기국민 Todo API 튜토리얼은 간단하게 끝났다. 해보면서 아직 아리송 한 것들이 분명 있긴 한데, 프로젝트 해 가면서 더 확실해질 것 같으니, GraphQL 사용하는 걸 공부한 다음 프로젝트를 시작하면 될 것 같다. Reference https://docs.nestjs.com/techniques/database https://docs.nestjs.com/recipes/sql-typeorm NestJS 빨리 배우기 시리즈","link":"/posts/backend/nest-todo-api-demo/"},{"title":"NestJS 빠르게 배우기 01","text":"사이드 프로젝트를 하기 위해서 백앤드를 여러가지로 고민하고 있었는데, 선택한 바로는 NestJS를 사용하기로 했다. 7.0.0 버전이 릴리즈 됐다고도 하고(현재는 7.0.3이다), star도 많이 받고 있어서 확인해봤다. 그리고 우연하게 직방에서 어떤 스택을 사용하고 있는지 확인할 일이 있었는데, 직방에서도 NestJS를 일부 도입하고 있는 것 같았다. 사이드 프로젝트답게, 개인적으로 도전적인 스택을 고민했는데, 백앤드를 Golang으로 구성해보고 싶었지만, Golang은 조금 더 사용해보면서 코드 스타일에 대해서 연습해보고 싶었다. (솔직히 Golang 코드로 작성되는 모양새가 마음에 쏙 들지는 않음) 지금 마음 속으로는 NestJS, GraphQL, Electron 이렇게 생각하고 있다. 우선 이 글은 NestJS를 도입하기 위한 간단한 NestJS 개념을 정리해보는 글이다. 시작하기이 글은 NestJS의 Overview를 확인하고 작성하고 있다. 우선 NestJS가 제공해주는 보일러플레이트로 프로젝트를 시작하는 방법은 이 링크에서 간단하게 확인할 수 있다. 시작해보면 간단히 아래와 같은 모습을 프로젝트에서 취하고 있다. 1234src | main.ts # 앱의 시작점 | app.module.ts # 앱의 루트 모듈 | app.controller.ts # 앱의 컨트롤러 이번 글은 Controller에 대해서 확인해보자. Controllercontroller는 흔히 알려져 있듯, Request, Response를 처리하는 로직이다. 특정 라우터에 붙여서 구체적인 요청을 받아서 처리한다.Nest에서는 기본 컨트롤러를 만들려면 class와 decorators를 사용한다. 공식 문서에서 제공하는 이미지 라우팅@Controller()를 사용해서 컨트롤러를 지정할 수 있는데, 데코레이터를 통해서 연관된 라우터를 그룹 지을 수 있다. Nest 문서에서 나타난 코드는 아래와 같은데 라우트 패스 접두사를 cats로 정의한 것이다. 이러한 prefix를 지정해서 앞서 말한 관련된 라우터로 그룹 지을 수 있는 것이다. 123456789101112cats.controller.tsJS;import { Controller, Get } from &quot;@nestjs/common&quot;;@Controller(&quot;cats&quot;)export class CatsController { @Get() findAll(): string { // 여기는 유저가 정의하는 부분이다. return &quot;This action returns all cats&quot;; }} @Get()은 Nest에게 HTTP request들의 앤드포인트를 구체적으로 지정하는 역할을 한다. @Get()을 사용해서 위에 컨트롤러에 정해진 prefix와 합쳐서 엔드 포인트를 정할 수 있다. 지금 예시는 GET /cats에서 동작하는 컨트롤러라고 볼 수 있다. request Method 데코레이터에서도 구체적인 엔드 포인트를 지정할 수 있는데, 예를 들어서 GET /cats/spinks 엔드 포인트를 원한다면 해당 메서드에 @Get('spinks')를 사용하면 된다. 위 예시는 200 코드와 응답을 줄 것이다. 일반적인 Express app처럼 res.json을 사용하지 않는데 이건 Nest가 사용하는 두 가지의 Response를 다루는 옵션과 관련 되어 있다. Standard: Nest 문서에서 추천하는 방식이고, 내장된 매서드를 사용하는 것이다. JS Object나, Array로 값을 리턴하면 자동적으로 JSON으로 시리얼라이징 해준다. 하지만 나머지 JS 리터럴 값들은 시리얼라이징 하지 않고 보낸다. 그리고 status code는 항상 200이다 (단, POST 매서드 같은 경우는 201). 당연히 Default 값이라는 뜻이고 이를 변경하려면 @HttpCode(...)를 붙여줘야 한다. 이와 관련해서는 나중에 천천히 알아보자 Library-specific: 쉽게 말해서 Express같은 라이브러리의 Response 스타일을 사용하도록 만드는 것이다. 이건 사용하고 싶지 않기 때문에 구체적인 얘기를 건너 뛰도록 하겠다. 주의: 두 가지를 동시에 사용할 수는 없다고 한다. 스탠다드를 추천하고 있으니, 프로젝트나 글에서도 스탠다드 형태로 많이 정리해볼 예정이다. Request ObjectNest는 Request 객체에 접근하는 방식을 마찬가지로 데코레이터로 제공한다. Default인 Request 객체는 Express의 객체를 따른다. 12345678910import { Controller, Get, Req } from &quot;@nestjs/common&quot;;import { Request } from &quot;express&quot;;@Controller(&quot;cats&quot;)export class CatsController { @Get() findAll(@Req() request: Request): string { return &quot;This action returns all cats&quot;; }} Express의 request 객체를 사용하지만, Express처럼 직접 query나 body를 꺼내오지 않아도 된다. (물론 할 수는 있는 것 같다.) @Body(), @Query()를 사용하면 request 객체에서 가져올 수 있다. 그 밖에 아래와 같은 데코레이터를 지원한다. 12345678@Request() -&gt; req@Response(), @Res() -&gt; res // 위에서 언급한 것처럼, Library-specific한 응답을 보낼 때 사용한다.@Next() -&gt; next@Session() -&gt; req.session@Param(key?: string) -&gt; req.params / req.params[key]@Body(key?: string) -&gt; req.body / req.body[key]@Query(key?: string) -&gt; req.query / req.query[key]@Headers(name?: string) -&gt; req.headers / req.headers[name] 리소스Nest는 일반적인 HTTP request 엔드포인트 데코레이터를 @Get()과 동일하게 제공하고 있다. - @Post(), @Put(), @Delete(), @Patch(), @Options(), @Head(), @All() 123456789101112131415// cats.controller.tsimport { Controller, Get, Post } from &quot;@nestjs/common&quot;;@Controller(&quot;cats&quot;)export class CatsController { @Post() create(): string { return &quot;This action adds a new cat&quot;; } @Get() findAll(): string { return &quot;This action returns all cats&quot;; }} 한 가지 개인적으로 궁금한 게 있는데, @Get()과 @Post() 처럼 여러 메서드를 데코레이터로 설정해둘 수 있을까? 또는 여러 엔드포인트에서 동작하도록 @Get(&quot;something&quot;), @Get(&quot;another&quot;)을 붙여도 동작할까? 프로젝트 하면서 시도해봐야겠다. 와일드카드 라우팅패턴 기반 라우터도 제공하고 있다. 예를 들어서 아래 애스터리스크 *는 패턴으로 사용된다. abcd, ab_cd, abecd 모두 매칭된다. 1234@Get('ab*cd')findAll() { return 'This route uses a wildcard';} ?, +, *, ()가 라우터 패스에 사용될 수 있다. Status code위에서 말한 대로, 기본은 200과 201(201은 POST 요청일 때)이다. 기본을 바꾸기 위해서 @HtppCode(...)를 사용하면 된다. 1234567import {HttpCode} from &quot;@nestjs/common&quot;@Post()@HttpCode(204)create() { return 'This action adds a new cat';} 다만, 상황 별로 다르게 status code를 보내줘야 하는 경우에는 @Res를 쓰거나, 에러를 throw 해야 한다. 커스텀 헤더커스텀한 Response Header를 만들기 위해서 @Header()를 사용할 수 있다. 1234567import {Header} from &quot;@nest/common&quot;@Post()@Header('Cache-Control', 'none')create() { return 'This action adds a new cat';} 리다이렉션리다이렉트 리스폰스를 주기 위해, @Redirect() 데코레이터를 사용하거나, library-specific 리스폰스 오브젝트를 사용할 수 있다. @Redirect()는 url를 인자값으로 넘겨줘야 한다. 그리고 선택적으로 statusCode를 넘겨줘야 하는데 기본 값은 302이다. 12@Get()@Redirect(&quot;https://changhoi.github.io&quot;, 301) 만약 statusCode와 리다이렉트 할 URL을 동적으로 설정해주고 싶다면, 메서드 안에서 아래와 같은 형태로 리턴을 해주면 된다. 1234{ &quot;url&quot;: string, &quot;statusCode&quot;: number} 이 값이 @Redirect() 데코레이터 인자값으로 넘어간 값을 덮어 쓰게 된다. 1234567@Get(&quot;docs&quot;)@Redirect(&quot;https://docs.nestjs.com&quot;, 302)getDocs(@Query(&quot;version&quot;) version) { if (version &amp;&amp; version === &quot;7&quot;) { return {url: &quot;https://docs.nestjs.com/v5/&quot;}; }} 파라메타 라우팅라우터에 파라메타를 넣는 방법은 Express와 동일하다. 그리고 파라메타를 얻는 방법도 마찬가지로 데코레이터를 사용해서 받아 올 수 있다. 메소드의 파라메타 뿐 아니라 라우터에서 지정된 파라메터도 받아 올 수 있다. 12345678910@Get(':id')findOne(@Param() params): string { console.log(params.id); return `This action returns a #${params.id} cat`;}/**findOne(@Params(&quot;id&quot;) id): string {} 과 같이 id를 구체적으로 가져올 수도 있다.*/ 서브도메인 라우팅@Controller 데코레이터는 host 옵션을 가져올 수 있는데, 이 옵션으로 들어오는 요청의 HTTP 호스트를 구체적으로 맞춰 줄 수 있다. 1234567@Controller({ host: &quot;admin.example.com&quot; })export class AdminController { @Get() index(): string { return &quot;admin page&quot;; }} route와 마찬가지로, host 옵션은 동적인 값을 호스트 이름에 위치시키기 위해서 토큰처럼 사용할 수도 있다. host 파라미터 토큰은 @Controller 에서 @HostParam() 데코레이터를 통해서 접근할 수 있다. 1234567@Controller({ host: &quot;:account.example.com&quot; })export class AccountController { @Get() getInfo(@HostParam(&quot;account&quot;) account: string) { return account; }} 비동기성데이터 추출은 JS에서 대부분 비동기로 작동한다 (I/O, Network). Nest에서는 async 함수를 잘 지원한다. 모든 비동기 함수는 Promise를 리턴한다. 즉, Nest가 스스로 resolve 할 수 있는 연기된 값 (미뤄진, Promised된 값)을 전달할 수 있다는 뜻이다. 아래와 같이 작성할 수 있다. 1234@Get()async findAll(): Promise&lt;any[]&gt; { return []} 위의 코드는 완전 유효하지만, Nest route handler에서는 RxJS의 Observable streams를 리턴함으로서 더 강력하게 위 동작을 해낼 수 있다. Nest는 자동적으로 해당 소스를 subscribe하고, 값이 나오면 가져간다. 1234@Get()findAll(): Observable&lt;any[]&gt; { return of([])} 개인적으로 RxJS를 모르기 때문에 비동기를 기쁘게 활용하도록 하려고 한다. Request payloadsPOST 라우트 핸들러가 클라이언트의 payload를 사용하려면 먼저 DTO(Data Transfer Object)를 정의해야 한다. DTO는 어떻게 데이터가 보내지게 될 지 정의하는 객체이다. DTO 스키마를 Typescript의 interface나 class를 사용해서 정의할 수 있다. Nest에서는 이 객체를 클래스로 정의할 것을 추천한다. 이유는 클래스는 ECMA의 표준에 속하고, 컴파일된 JS에서도 실제 entities로서 다룰 수 있게 된다. 반면 interface는 컴파일 과정에서 삭제되기 때문에 런타임에서 Nest가 참조할 수가 없게 된다. 이러한 차이는 런타임에 이러한 메타데이터(class또는 interface로 정의된 DTO와 같은)에 접근해야 하는 Pipe와 같이 추가적인 가능성들을 더해주는 기능들이 있기 때문에 중요하다. 아래와 같이 DTO를 정의한 다음에 컨트롤러 안에서 사용할 수 있다. 12345678910export class CreateCatDto { readonly name: string; readonly age: number; readonly breed: string;}@Post()async create(@Body() createCatDto: CreateCatDto) { return 'This action adds a new cat';} abstract class는 어떨까? abstract class는 컴파일 할 때 삭제되는 것이 아니라 abstract가 지워지는 것으로 알고 있는데, 프로퍼티를 initializing 하지 않아도 되니 타입스크립트의 옵션을 끄지 않아도 될 것 같은데. 에러 핸들링별도의 파트가 있고, 관련된 내용은 이 글에서 확인할 수 있다. 실행해보기아래는 완전한 예시 샘플이다. 123456789101112131415161718192021222324252627282930313233343536373839import { Controller, Get, Query, Post, Body, Put, Param, Delete} from &quot;@nestjs/common&quot;;import { CreateCatDto, UpdateCatDto, ListAllEntities } from &quot;./dto&quot;;@Controller(&quot;cats&quot;)export class CatsController { @Post() create(@Body() createCatDto: CreateCatDto) { return &quot;This action adds a new cat&quot;; } @Get() findAll(@Query() query: ListAllEntities) { return `This action returns all cats (limit: ${query.limit} items)`; } @Get(&quot;:id&quot;) findOne(@Param(&quot;id&quot;) id: string) { return `This action returns a #${id} cat`; } @Put(&quot;:id&quot;) update(@Param(&quot;id&quot;) id: string, @Body() updateCatDto: UpdateCatDto) { return `This action updates a #${id} cat`; } @Delete(&quot;:id&quot;) remove(@Param(&quot;id&quot;) id: string) { return `This action removes a #${id} cat`; }} 위와 같이 컨트롤러를 설정한 다음엔, Nest에게 컨트롤러가 있다는 걸 알려줘야 한다. 컨트롤러는 항상 module에 속한다. 그래서 @Module() 데코레이터 안에 controllers Array가 있는 것이다. 루트 모듈에 아래와 같이 작성하면 된다. 1234567import { Module } from &quot;@nestjs/common&quot;;import { CatsController } from &quot;./cats/cats.controller&quot;;@Module({ controllers: [CatsController]})export class AppModule {} 후기Controller 부분은 GraphQL을 사용하게 되면 하나만 설정하게 되는 건가? 그렇다고 해도 라우팅과 컨트롤러에서 많은 매력점을 가진 프레임워크인 것 같다. Reference https://docs.nestjs.com/controllers","link":"/posts/backend/nestjs-quicklearn-01/"},{"title":"NestJS 빠르게 배우기 02","text":"지난 글에서는 Controller에 대해서 알아봤다. 이번 글에서는 Provider가 뭔지 알아보도록 하자. ProviderProvider를 확인해보자. 문서에서 말하기로는 Provider가 가장 핵심적인 역할을 하는 곳이라고 한다. 많은 Nest의 기본 클래스들은 Provider로서 취급된다. (services, repositories, factories, helpers 등) Provider의 기본적인 아이디어는 의존성을 주입하는 것이다. 즉, 객체들이 다른 것들과 다양한 관계를 만들 수 있다는 것이다. 하나의 Provider는 @Injectable() 데코레이터로 간단하게 클래스에 annotated된 형태이다. 공식 문서에서 제공해주는 이미지 Services앞에 예시에서 사용한 것처럼 Cat과 관련된 예시로 시작해보자 (Nest는 고양이를 좋아한다. 이미지도 고양이임). CatsService는 데이터 저장과 검색에 대한 일을 한다. 그리고 이것은 CatController에서 동작하도록 설계되었다. 그래서 이러한 경우는 provider로서 정의되기 좋다. 따라서, 우리는 @Injectable 데코레이터로 이 클래스를 데코레이트 하면 된다. service를 CLI를 통해서 만들 수 있는데, 아래와 같은 서비스는 nest g service cats 명령어로 실행 가능하다. 1234567891011121314151617// cats.service.tsimport { Injectable } from &quot;@nestjs/common&quot;;import { Cat } from &quot;./interfaces/cat.interface&quot;;@Injectable()export class CatService { private readonly cats: Cat[] = []; create(cat: Cat) { this.cats.push(cat); } findAll(): Cat[] { return this.cats; }} 123456// interfaces/cats.interface.tsexport interface Cat { name: string; age: number; bread: string;} 위와 같이 CatsService는 두 메서드와 하나의 프로퍼티를 가지고 있는 클래스이다. 새로운 특정은 @Injectable() 데코레이터를 사용했다는 것인데, 이 데코레이터는 메타데이터를 붙여준다. 그 메타데이터는 Nest에게 이 클래스가 Provider라는 점을 알려준다. 이 서비스를 CatsController에 담아보면 아래와 같다. 12345678910111213141516171819import { Controller, Get, Post, Body } from &quot;@nestjs/common&quot;;import { CreateCatDto } from &quot;./dto/create-cat.dto&quot;;import { CatsService } from &quot;./cats.service&quot;;import { Cat } from &quot;./interfaces/cat.interface&quot;;@Controller(&quot;cats&quot;)export class CatsController { constructor(private catsService: CatService) {} @Post() async create(@Body() createCatDto: CreateCatDto) { this.catsService.create(createCatDto); } @Get() async findAll(): Promise&lt;Cat[]&gt; { return this.catsService.findAll(); }} CatsService는 class의 생성자를 통해서 주입된다. @Injectable()를 사용하면, 뱔도의 작업 없이(constructor에서 담아주는 것을 빼고) 클래스 내부에서 this로 접근해 사용할 수 있게 된다. typedi를 사용했을 때와 유사하다. ScopesProvider는 일반적으로 애플리케이션의 라이프타임과 동일한 라이프타임을 갖는다. 애플리케이션이 시작되면, 모든 의존성들이 주입되고, 그리고 모든 Provider가 인스턴스화된다. 마찬가지로, 애플리케이션이 종료되면, 각 Provider들은 파괴된다. 하지만, Provider의 라이프타임을 request-scope로 만들 수도 있다. 해당 내용은 이 글에서 읽어볼 수 있다. Custom providersNest는 provider 사이의 관계를 정리해주는 내장된 제어 역전 (IoC) 컨테이너가 있다. 제어의 역전과 의존성 주입과 관련된 OOP와 연관된 개념은 이 링크에서 확인해보자. 이러한 특징들은 위에서 언급했던 의존성 주입과 관련된 특징들에 기반해 있다. @Injectable() 데코레이터는 사실 Provider를 정의하는 유일한 방법은 아니고, 일반적인 값, 클래스들, 비동기 또는 동기 factories를 사용할 수도 있다 더 자세한 내요은 이 링크에서 확인할 수 있다. 선택적(Optional) Providers의존성 중에서는 반드시 필요 없는 의존성이 존재할 수도 있다. 즉 옵셔널하게 사용되는 경우가 있다는 뜻인데 이러한 경우에서는 @Optional()데코레이터를 생성자에 사용해줄 수 있다. 123456import { Injectable, Optional, Inject } from &quot;@nestjs/common&quot;;@Injectable()export class HttpService&lt;T&gt; { constructor(@Optional() @Inject(&quot;HTTP_OPTIONS&quot;) private httpClient: T) {}} Property-based Injection위에서 계속 사용된 기술들은 생성자 기반의 주입이라고 (class-based injection) 볼 수 있다. 특별한 경우에는 프로퍼티 기반의 주입 (Property-based injection)을 사용할 수도 있는데, 예를 들어서 최상위 클래스가 하나 또는 다수의 providers에게 의존성을 가지고 있다면, 이러한 경우에 하위 클래스에서 super를 통해 넘겨주는 것은 그렇게 좋은 방법은 아니다. 이러한 경우를 피하기 위해서 @Inject() 데코레이터를 프로퍼티에게도 사용할 수 있다. 1234567import { Injectable, Inject } from &quot;@nestjs/common&quot;;@Injectable()export class HttpService&lt;T&gt; { @Inject(&quot;HTTP_OPTIONS&quot;) private readonly httpClient: T;} Provider 등록위 과정을 통해서 CatsService라는 Provider를 만들어낸 것으로 볼 수 있다. 그리고 이는 CatsController에서 소비될 것이다. 따라서 이 서비스를 Nest에 등록하고 주입되도록 해야하는데, 이는 app.module.ts에서 providers 배열에 서비스를 더해주면 된다. 123456789import { Module } from &quot;@nestjs/common&quot;;import { CatsController } from &quot;./cats/cats.controller&quot;;import { CatsService } from &quot;./cats/cats.service&quot;;@Module({ controllers: [CatsController], providers: [CatsService]})export class AppModule {} 지금까지 예시에서 프로젝트의 디렉토리 구조는 다음과 같다. 12345678910src/ cats/ dto/ create-cat.dto.ts interfaces/ cat.interface.ts cats.service.ts cats.controller.tsapp.module.tsmain.ts 후기DI와 IoC등 OOP관련 개념이 많이 등장하는데, 코드를 직접 써봐야 체감할 수 있을 것 같다. 무슨 말을 하는지는 알겠다. 그리고 디렉토리 구조가 마음에 들지 않는다. 최상위 app.module.ts에 Provider가 모이고, Controller에서는 생성자에 가져오는 코드만 작성하면 Nest가 알아서 Inject를 해주는 것 같은데, 만약 내가 model 부분의 코드를 분리해서 작성한다면? 그것도 service와 마찬가지로 provider에 등록하게 되는 건가? 그리고 app.module.ts에다가 몰빵해둬야 하는 구조인지는 모르겠지만 (여러 모듈로 나눠서 관리할 수 있을 것 같지만), 몰빵해야 하는 거는 좀 별로 같기도? 아마 다음에 Module에 대해서 보게 되니까 확인해보면 될 것 같다. Reference https://docs.nestjs.com/providers https://docs.nestjs.com/fundamentals/injection-scopes https://develogs.tistory.com/19 https://docs.nestjs.com/fundamentals/dependency-injection","link":"/posts/backend/nestjs-quicklearn-02/"},{"title":"NestJS 빠르게 배우기 03","text":"지난 글에서는 Provider의 Overview를 살펴봤다. 이번 글에서는 Modules에 대해서 확인해보자. ModulesModule은 @Module이라는 데코레이터에게 어노테이트 된 클래스이다. @Module 데코레이터는 Nest가 앱의 구조를 조직할 수 있는 메타데이터를 제공해준다. 공식 문서에서 제공해주는 이미지 각 앱은 적어도 하나의 root module이 있다. 이 모듈은 Nest가 application graph를 만들어낼 시작 포인트를 잡아준다. application graph는 Nest가 Module과 Provider 관계와 의존성을 결정할 때 사용하는 내부적인 데이터 구조이다. 아주 작은 앱들은 이론적으로 root module만 필요할 수 있지만, 이런 경우가 보통 일반적인 것은 아니다. 대부분의 앱에서는 여러가지의 모듈들을 사용하게 되고 각각은 연관된 capabilities 들의 모음으로 캡슐화 되게 된다. @Module()이 인자에서 갖는 객체는 아래 모습과 같다. providers: Nest의 injector (typedi와 같은)에 의해 인스턴스화되고, 인스턴스들은 이 모듈 안에서 최소한으로 공유된다. controllers: 해당 모듈에서 정의된, 인스턴스화 되어야 하는 컨트롤러의 모음이다. imports: 임포트된 모듈들의 리스트이다. 이 리스트의 모듈들은 데코레이터에 사용 중인 모듈에서 필요한 providers를 export 하고 있어야 한다. exports: providers의 하위 집합으로, 데코레이터를 사용 중인 모듈이 제공받은 Provider의 일부를 내보낼 수 있다. 이는 다른 모듈에서 import 할 때 사용된다. 특징 단위의 모듈 (Feature module)CatsController와 CatsService는 같은 application 영역이다. 서로 연관이 깊기 때문에, feature module로 묶을 수 있다. feature module은 간단하게 특정한 특징들과 연관된 코드를 함께 조직화한다. 이렇게 함으로써, 코드를 조직적이게 유지하고, 명확한 경계를 세울 수 있다. 이는 SOLID 원칙과 함께 개발을 할 때 복잡성을 줄여준다. 1234567891011// cats/cats.module.tsimport {Module} from &quot;@nestjs/common&quot;;import {CatsController} from &quot;./cats.controller&quot;;import {CatsService} from &quot;./cats.service&quot;;@Module({ controllers: [CatsController], providers: [CatsService]})export class CatsModule {} CLI를 사용해서 만들려면 nest g module cats로 만들 수 있다. 위에서 정의한 cats.module.ts와 연관된 모든 모듈을 cats 디렉토리 아래에 둔다. 마지막으로, 루트 모듈에 CatsModule을 임포트 시켜주면 된다. 123456789// app.module.tsimport {Module} from &quot;@nestjs/common&quot;;import {CatsModule} from &quot;./cats/cats.module&quot;;@Module({ imports: [CatsModule]})export class AppModule{} 결과적으로 디렉토리 구조는 아래와 같이 생겼다. 1234567891011src/ cats/ dto/ create-cat.dto.ts interfaces/ cat.interface.ts cats.service.ts cats.controller.ts cats.module.ts app.module.ts main.ts 공유되는 모듈Nest에서는 Module이 기본적으로는 Singleton이다. 따라서 같은 어떠한 Provider의 인스턴스든 여러 모듈에서 공유할 수 있다. 공식 문서에서 제공해주는 이미지 모든 모듈은 자동적으로 shared module이 된다. 한 번 만들어지면 어떤 모듈에서든 사용할 수 있다. 예를 들어서 CatsService의 인스턴스를 몇 다른 모듈들에서 사용하고 싶다고 가정하자. 이렇게 하려면, CatsService를 먼저 module의 export 배열에 담아서 내보내줘야 한다. 1234567891011// cats.module.tsimport {Module} from &quot;@nestjs/common&quot;;import {CatsController} from &quot;./cats.controller&quot;;import {CatsService} from &quot;./cats.service&quot;;@Module({ controllers: [CatsController], providers: [CatsService], exports: [CatsService]})export class CatsModule {} 이렇게 해두면, CatsModule을 임포트 하고 있는 어떤 모듈에서든 CatsService 인스턴스를 사용할 수 있다. 모듈 다시 내보내기위에서 본 것 처럼, 모듈들은 모듈 내부적인 provider들을 내보낼 수 있다. 게다가 모듈은 import 해온 것들을 export 할 수도 있다. 아래의 CommonModule은 CoreModule을 임포트해와서 export 하고 있다. 이렇게 함으로써, CommonModule을 임포트한 다른 모듈에서도 CoreModule을 사용할 수 있게 된다. 의존성 주입모듈에서 providers를 넣어줄 때, providers를 주입하는 방식으로도 가능하다. 123456789101112// cats.module.tsimport {Module} from &quot;@nestjs/common&quot;;import {CatsController} from &quot;./cats.controller&quot;;import {CatsService} from &quot;./cats.service&quot;;@Module({ controllers: [CatsController], providers: [CatsService]})export class CatsModule { constructor(private catsService: CatsService) {}} 그렇지만 모듈 클래스 자체는 환 의존성 문제로 provider로서 주입할 수 없다. 글로벌 모듈들만약 같은 모듈들 세트를 모든 곳에 임포트 하고 싶다면, 하나씩 하기는 귀찮은 일이다. Nest에서는 Provider들을 모듈 범위 안에서 캡슐화 한다. 따라서 앞서서 캡슐화된 모듈을 임포트 하지 않는다면, 모듈의 Provider를 사용할 수 없다. 여러 Provider들 집합을 어디서든 제공해주고 싶다면, 모듈을 global로 만들어야 한다. @Global() 데코레이터를 사용하게 되면 이를 가능하게 해준다. 1234567891011import {Module, Global} from &quot;@nestjs/common&quot;;import {CatsController} from &quot;./cats.controller&quot;;import {CatsService} from &quot;./cats.service&quot;;@Global()@Module({ controllers: [CatsController], providers: [CatsService], exports: [CatsService]})export class CatsModule {} @Global() 데코레이터는 모듈을 글로벌 범위로 만들어준다. 글로벌 모듈은 한 번만 등록될 수 있고, 일반적으로 루트나 코어 모듈에 의해 등록된다. 위 예시에서, CatsService는 어디서나 쓸 수 있고, CatsService를 사용하고 싶은 모듈들은 CatsModule을 임포트 할 필요 없이 의존성 주입할 수 있다. 모든 것을 글로벌화 시키는 것은 좋은 디자인이 아니다. 글로벌 모듈들은 불필요한 보일러플레이트를 줄여주는 역할을 한다. 일반적으로 모듈을 소비하고 싶다면 import 배열에 담아주는 것이 좋다. 동적 모듈들Nest의 모듈 시스템은 동적 모듈이라고 하는 강력한 기능을 포함하고 있다. 이 기능은 커스텀 가능한 모듈들을 만들 수 있게 해주는데, 커스텀 가능한 모듈은 providers를 동적으로 설정하고 등록할 수 있게 해준다. 동적 모듈은 이 링크에서 더 자세하게 확인할 수 있다. 이번 챕터에서는 간단한 오버뷰만 보여준다. 1234567891011121314151617import {Module, DynamicModule} from &quot;@nestjs/common&quot;;import {createDatabaseProviders} from &quot;./database.providers&quot;;import {Connection} from &quot;./connection.provider&quot;;@Module({ providers: [Connection]})export class DatabaseModule { static forRoot(entities = [], options?): DynamicModule { const providers = createDatabaseProviders(options, entities); return { module: DatabaseModule, providers, exports: providers } }} 이 모듈은 Connection Provider를 기본으로 @Module() 데코레이터 안에 메타데이터에서 정의하고 있다. 하지만 forRoot에 넘어온 entities와 options에 따라서 추가적으로 provider들을 expose하고 있다(예를 들어서 createDatabaseProviders에서는 repositories가 생성된다고 하자). 동적 모듈에 의해 반환된 프로퍼티들은 override되는 것이 아니라, @Module() 데코레이터에 정의된 기본 module 메타데이터에서 extend된다. 이 방법으로 정적으로 선언된 Connection Provider와 동적으로 생성된 repository provider가 모듈에서 exported 된다. 만약 동적 모듈을 글로벌하게 등록하고 싶으면, 반환값에 global 프로퍼티를 true로 설정하면 된다. 123456{ global: true, module: DatabaseModule, providers, exports: providers} DatabaseModule은 아래와 같은 방법으로 임포트되고 설정될 수 있다. 12345678import {Module} from &quot;@nestjs/common&quot;import {DatabaseModule} from &quot;./database/database.module&quot;;import {User} from &quot;./users/entities/user.entity&quot;;@Module({ imports: [DatabaseModule.forRoot([User])]})export class AppModule{} 만약 동적 모듈을 re-export(위에서 다시 내보내기와 같다) 하고 싶다면, forRoot 메서드를 호출하는 것을 빼고 다시 내보내주면 된다. 123456789import {Module} from &quot;@nestjs/common&quot;;import {DatabaseModule} from &quot;./database/database.module&quot;;import {User} from &quot;./users/entities/user.entity&quot;;@Module({ imports: [DatabaseModule.forRoot([User])], exports: [DatabaseModule]})export class AppModule {} 후기모듈 단위로 개발하고 조립하는 식으로 해야 하는 것 같고, 이전에 주로 내가 구현하던 방식의 아키텍쳐는 아니다. 그래도 위에서 소개해주던 방식으로 구현할 수 있도록 의도하는 Nest의 구조 덕분에 어렵지 않게 만들 수 있을 것 같기는 하다. 익숙하지 않음이 있어서 마음에 들지는 않지만 사용할만 한 것 같다. Reference https://docs.nestjs.com/modules","link":"/posts/backend/nestjs-quicklearn-03/"},{"title":"NestJS 빠르게 배우기 04","text":"지난 글에서는 Module에 대해서 알아봤다. 이번 글에서는 Middleware에 대해서 알아보자. Middleware미들웨어는 컨트롤러 바로 전에서 동작하는 함수이다. 미들웨어 함수들은 애플리케이션의 request-response 사이클 중에 request와 response 오브젝트에 접근할 수 있고, next() 미들웨어 함수에 접근할 수 있다. 공식 문서에서 제공해주는 이미지 Nest의 미들웨어는 기본적으로 express의 미들웨어와 동일하다. express의 공식 문서에서는 미들웨어의 기능을 아래와 같이 기술하고 있다. Middleware 함수는 아래 기능들을 수행할 수 있다. 어떠한 코드도 실행할 수 있음 request-response 사이클을 종료할 수 있다. 다음 미들웨어 함수를 가져올 수 있다. 현재 미들웨어 함수가 request-response 사이클을 종료하는 기능을 하지 않는다면, 그 미들웨어는 반드시 next()함수를 다음 미들웨어에게 통제권을 넘기기 위해서 사용해야 한다. 그렇지 않으면 request는 계속 응답을 기다리는 상태가 된다. 커스텀 Nest Middleware는 함수나 Injectable() 데코레이터와 함께 사용한 클래스로 사용할 수 있다. 클래스의 경우, NestMiddleware 인터페이스를 implement 해야 한다. 반면 함수는 어떤 특별한 요구사항은 없다. 1234567891011// logger.middleware.tsimport { Injectable, NestMiddleware } from &quot;@nestjs/common&quot;;import { Request, Response } from &quot;express&quot;;@Injectable()export class LoggerMiddleware implements NestMiddleware { use(req: Request, res: Request, next: Function) { console.log(&quot;Request...&quot;); next(); }} 의존성 주입Nest 미들웨어는 온전하게 의존성 주입을 지원하고 있다. Provider나 Controller처럼 같은 모듈에서 사용할 수 있는 의존성을 주입하는 것이 가능하다. 그 전과 마찬가지로 constructor에 명시해주는 것으로서 사용할 수 있다. (주: 이 부분은 그 전 글을 읽고 오면 좋을 것 같다.) 미들웨어 적용하기@Module 데코레이터에는 미들웨어를 넣을 부분이 없다. 대신 미들웨어를 Module 클래스의 configure 메서드를 만들어서 세팅할 수 있다. 미들웨어를 사용하는 모듈들은 NestModule 인터페이스를 implements 해야 한다. 123456789101112import { Module, NestModule, MiddlewareConsumer } from &quot; @nestjs/common&quot;;import { LoggerMiddleware } from &quot;./common/middleware/logger.middleware&quot;;import { CatsModule } from &quot;./cats/cats.module&quot;;@Module({ imports: [CatsModule]})export class AppModule implements NestModule { configure(consumer: MiddlewareConsumer) { consumer.apply(LoggerMiddleware).forRoutes(&quot;cats&quot;); }} 위 예시에서는 LoggerMiddleware를 /cats 라우터 핸들러에 달았다. 더 디테일한 메서드에서 사용하려면 forRoutes에서 메서드를 설정해줄 수 있다. 아래에 RequestMethod는 요구되는 요청 메서드 타입을 참조하고 있는 enum이다. 12345678910111213141516171819import { Module, NestModule, RequestMethod, MiddlewareConsumer} from &quot;@nestjs/common&quot;;import { LoggerMiddleware } from &quot;./common/middleware/logger.middleware&quot;;import { CatsModule } from &quot;./cats/cats.module&quot;;@Module({ imports: [CatsModule]})export class AppModule implements NestModule { configure(consumer: MiddlewareConsumer) { consumer .apply(LoggerMiddleware) .forRoutes({ path: &quot;cats&quot;, method: RequestMethod.GET }); }} configure() 메서드는 비동기적으로도 만들어질 수 있다. 와일드 카드 라우팅라우터에서 적용되던 패턴 베이스의 라우팅이 미들웨어에서도 마찬가지로 적용된다. *를 와일드 카드로 사용할 수 있다. 1forRoutes({ path: &quot;ab*cd&quot;, method: RequestMethod.ALL }); 미들웨어 ConsumerMiddlewareConsumer는 헬퍼클래스이다. 이 클래스는 몇 가지 내장된 미들웨어를 관리해주는 메서드를 가지고 있다. 이 모든게, fluent style에서 체인 형태로 적용할 수 있다. forRoutes() 메서드는 하나의 문자열, 다수의 문자열, RouteInfo 객체, 컨트롤러 클래스, 심지어 다수의 컨트롤러 클래스들을 받을 수 있다. 대부분의 경우 몇 개의 컨트롤러들의 리스트를 넘기게 될 것이다. 12345678910111213import { Module, NestModule, MiddlewareConsumer } from &quot;@nestjs/common&quot;;import { LoggerMiddleware } from &quot;./common/middleware/logger.middleware&quot;;import { CatsModule } from &quot;./cats/cats.module&quot;;import { CatsController } from &quot;./cats/cats.controller&quot;;@Module({ imports: [CatsModule]})export class AppModule implements NestModule { configure(consumer: MiddlewareConsumer) { consumer.apply(LoggerMiddleware).forRoutes(CatsController); }} apply() 메서드는 아마 하나의 미들웨어나 여러 미들웨어를 특정하기 위한 다수의 인자값을 받을 수도 있다. 라우터 제외하기특정 라우트에는 미들웨어 적용을 하고 싶지 않을 수도 있다. 이러한 경우에는 exclude 메서드를 사용해서 쉽게 제외할 수 있다. 이 메서드는 하나의 스트링, 다수의 스트링, 또는 제외될 라우팅을 결정해주는 RouteInfo 객체를 받는다. 12345678consumer .apply(LoggerMiddleware) .exclude( { path: &quot;cats&quot;, method: RequestMethod.GET }, { path: &quot;cats&quot;, method: RequestMethod.POST }, &quot;cats/(.*)&quot; ) .forRoutes(CatsController); 위 예시들로 exclude() 메서드에서 정의된 세 가지 예외사항을 제외한 LoggerMiddleware는 CatsController 의 라우터들에게 적용된다. 함수형 미들웨어위 예시에 나온 LoggerMiddleware 클래스는 아주 간단했다. 멤버도 없고 추가 메서드도 없고 의존성도 없다. 이러한 경우는 그냥 함수형 미들웨어를 정의할 수도 있는데, 아래와 같이 작성하면 된다. 123456// logger.middleware.tsexport function logger(req, res, next) { console.log(`Request...`); next();} 12consumer.apply(logger);forRoutes(CatsController); 별다른 의존성이 필요 없다면 언제든 함수형 미들웨어를 적용할 것을 고민해보자. 다수의 미들웨어 적용위에서 언급한 것처럼, 여러 미들웨어를 연속적으로 동작하도록 묶어주기 위해서 쉼표(,)로 분리된 리스트가 apply() 메서드 안에 필요하다. 1consumer.apply(cors(), helmet(), logger).forRoutes(CatsController); 글로벌 미들웨어미들웨어를 등록된 모든 라우터에 한 번에 적용하려면, INestApplication인스턴스에 의해 제공되는 use() 메서드를 사용해라. 123const app = await NestFactory.create(AppModule);app.use(logger);await app.listen(3000); 후기미들웨어는 익숙한 방식으로도 작성할 수 있고, 컨트롤러나 특정 라우터마다 적용할 수도 있다고 해서 좋긴 한데, 의존성을 주입하는 예시 부분이 없어서 조금 아쉽다. 다음 장이 예외 처리 인데, 이 부분 까지만 하면 대충은 필수적인 부분은 다 확인한 것 같고, Nest의 특징적인 부분들은 프로젝트를 해가면서 배우는 것도 좋을 것 같다. Reference https://docs.nestjs.com/middleware https://en.wikipedia.org/wiki/Fluent_interface","link":"/posts/backend/nestjs-quicklearn-04/"},{"title":"NestJS 빠르게 배우기 05","text":"지난 글에서는 Middleware에 대해서 알아봤다. 이번 글에서는 Exception filters에 대해서 알아보자. Nest는 핸들링 되지 않은 모든 예외들에 대해서 반응하도록 하는 내장된 예외 레이어가 있다. 예외가 핸들링 되지 않는다면, 이 레이어에서 잡아서 자동으로 적절한 유저 친화적 응답을 보내준다. 공식 문서에서 제공해주는 이미지 이 동작은 HttpException 타입의 예외를 처리하는 내장된 글로벌 예외 처리기에 의해 동작하는 것이다. 예외가 인식되지 않는 경우 (HttpException이 아니거나, HttpException을 상속하지도 않는 경우) 내장된 예외 처리기가 아래 기본 응답을 만들어준다. 1234{ &quot;statusCode&quot;: 500, &quot;message&quot;: &quot;Internal server error&quot;} 표준 예외 스로잉하기Nest는 내부장된 HttpException 클래스를 제공한다고 했다. 일반적인 HTTP REST/GraphQL API 기반의 앱을 위해서, 특정한 에러 상황이 발생했을 때 표준 HTTP 응답 객체를 보내는 가장 좋은 방법일 것이다. 예를 들어서, CatsController에서 findAll() 메서드를 가지고 있는 상황을 가정해보자. 이 라우트 핸들러가 몇 가지 이유로 예외를 스로잉 할 때를 가정해보면 아래와 같이 작성할 수 있다. 123456// cats.controller.ts@Get()async findAll() { throw new HttpException(&quot;Forbidden&quot;, HttpStatus.FORBIDDEN)} HttpStatus는 @nestjs/common 패키지 안에 있는 enum을 담고 있는 헬퍼이다. 이 엔드포인트를 요청한다면, 응답은 아래와 같이 갈 것이다. 1234{ &quot;statusCode&quot;: 403, &quot;message&quot;: &quot;Forbidden&quot;} HttpException 생성자는 두 개의 인자 값을 요구하는데, 이것들로 응답 값을 결정한다. response: JSON 응답의 바디르 결정하는데 쓰인다. 이 부분은 문자열이 될 수도 있고, 객체가 될 수도 있다. 아래에 무엇을 넘기는지에 따라 어떻게 달라지는지 나온다. status: HTTP 상태 코드 (status code)를 결정하는데 사용된다. 기본 값으로 JSON 응답의 바디 부분은 두 가지 프로퍼티를 가지고 있는 객체이다. statusCode: status 기본적으로는 인자 값으로 넘어온 상태 코드가 들어가게 된다. message: status에 맞는 짧은 문장이 들어가게 된다. message 부분을 덮어 씌우기 위해서는 response 인자 값에 문자열을 제공해야 한다. 전체 JSON 응답 값을 덮어 씌우려면 response 인자 값으로 객체를 넘기면 된다. Nest는 객체를 시리얼라이즈 하고 JSON의 응답 바디 값으로 넘긴다. status 인자 값은 유효한 HTTP 상태 코드가 들어가야 한다. 가장 좋은 방법은 HttpStatus enum을 @nestjs/common에서 임포트해와서 사용하는 것이다. 아래 예시는 응답 바디 전체를 덮어 씌우는 것이다. 1234567@Get()async findAll() { throw new HttpException({ status: HttpStatus.FORBIDDEN, error: &quot;Custom Message&quot; }, HttpStatus.FORBIDDEN)} 1234{ &quot;status&quot;: 403, &quot;error&quot;: &quot;Custom Message&quot;} 내장된 HTTP 예외들Nest는 일반적인 표준 예외들을 HttpException을 기본 베이스로 해서 제공하고 있다. 그리고 이것들은 모두 @nestjs/common 패키지에 속해있다. BadRequestException UnauthorizedException NotFoundException ForbiddenException NotAcceptableException RequestTimeoutException ConflictException GoneException PayloadTooLargeException UnsupportedMediaTypeException UnprocessableEntityException InternalServerErrorException NotImplementedException ImATeapotException MethodNotAllowedException BadGatewayException ServiceUnavailableException GatewayTimeoutException 예외 필터들내장된 기본 예외 필터들은 자동적으로 많은 경우를 커버하지만, 예외 레이어의 온전한 제어를 필요로 할 때도 있다. 예를 들어서 로그를 기록하는 툴을 달거나, 다양한 JSON 스키마를 동적인 요소들에 기반해서 사용하는 경우가 잇을 수 있다. 예외 필터(Exception filters)들은 이러한 목적에 맞게 사용할 수 있다. 이 필터들은 명확한 흐름의 통제와 응답 컨탠츠의 통제를 제공해준다. HttpException 클래스의 인스턴스를 처리하는 예외 필터를 만들어보고, 커스텀한 응답 로직을 실행해보자. 이 작업을 위해서 Request와 Response 객체에 접근해야 한다. Request 오브젝트에서는 오리지날 url과 로깅 정보를 포함하고 있다. Response 객체는 직접적인 응답 컨트롤을 위해서 사용한다. (response.json 메서드를 사용하겠다는 의미이다.) 12345678910111213141516171819202122232425// http-exception.filter.tsimport { ExceptionFilter, Catch, ArgumentsHost, HttpException} from &quot;@nestjs/common&quot;;import { Request, Response } from &quot;express&quot;;@Catch(HttpException)export class HttpExceptionFilter implements ExceptionFilter { catch(exception: HttpException, host: ArgumentsHost) { const ctx = host.switchToHttp(); const response = ctx.getResponse&lt;Response&gt;(); const request = ctx.getRequest&lt;Request&gt;(); const status = exception.getStatus(); response.status(status).json({ statusCode: status, timestamp: new Date().toISOString(), path: request.url }); }} 모든 예외 필터들은 제네릭 ExceptionFilter&lt;T&gt; 인터페이스를 implements 해야 한다. implements 함으로써, catch(exception: T, host: ArgumentsHost) 메서드를 제공하도록 한다. T는 예외의 타입을 가리킨다. Arguments hostcatch() 메서드의 파라메타를 살펴보자. exception 파라메타는 현재 처리되고 있는 예외 객체이다. host 파라메타는 ArgumentsHost 객체이다. ArgumentsHost는 이 링크(Excution context)의 장에서 더 자세하게 다룰 예정이다. 이번 샘플에서는 Request와 Response 객체를 얻는 용도로 사용된 것만 알고 잇으면 된다. 코드에서 ArgumentsHost에서 헬퍼 메소드를 사용해서 Request Response를 얻을 수 있었다고 정도만 알자. 더 많은 ArgumentsHost에 대해서 보려면 이 링크 (위 링크와 같다)에서 확인해보자. 이 단계에서 추상적인 이유는 ArgumentsHost 함수들이 모든 context 안에 있기 때문이다. (즉, 지금은 HTTP 서버의 컨텍스트를 사용하고 있지만, MSA라든지, WebSockets에서도 사용된다.) 실행 맥락 챕터에서 어떻게 ArgumentsHost와 그것의 헬퍼 메소드들을 사용해서 어떤 실행 환경에서든 적절한 arguments에 접근하는지 볼 수 있다. 이러한 특징은 제네릭 예외 필터를 어떤 맥락에서든 동작하게 작성할 수 있게 해준다. Binding filters위에서 만든 HttpExceptionFilter를 CatsController에 create() 메서드에 묶어보도록 하겠다. 1234567// cats.controller.ts@Post()@UserFilters(new HttpExceptionFilter())async create(@Body() createCatDto: CreateCatDto) { throw new ForbiddenException()} @UseFilters() 데코레이터도 @nestjs/common에 있다. 적용할 때는 @UseFilters() 데코레이터를 사용했다. @Catch() 데코레이터와 유사하게, 하나의 필터 인스턴스나, 쉼표로 구분된 필터 인스턴스들을 넣을 수 있다. 위에서는 HttpExceptionFilter의 인스턴스를 넣어줬다. 근ㄷ 인스턴스 말고 클래스를 넣어줘도 된다. 클래스를 넣는 것을 선호하는게 좋다. 메모리 사용을 줄여준다. Nest가 쉽게 전체 모듈에서 같은 클래스의 인스턴스를 쉽게 사용할 수 있게 해준다. 위 예시에서 HttpExceptionFilter는 하나의 create() 라우터 핸들러 메서드에만 적용되어있다. 예외 필터는 컨트롤러 스코프, 글로벌 스코프 등으로도 사용될 수 있다 (위 예시에서는 메서드 스코프로 사용된 거임). 예를 들어서 컨트롤러 스코프로 만들어보자면, 아래와 같이 사용하면 된다. 12@UseFilters(new HttpExceptionFilter())export class CatsController {} 이 작업은 HttpExceptionFilter를 CatsController에서 정의한 라우터 핸들러 모두에게 적용하도록 한다. 글로벌 스코프의 필터를 만들려면 아래와 같이 사용하면 된다. 1234567async function bootstrap() { const app = await NestFactory.create(AppModule); app.useGlobalFilters(new HttpExceptionFilter()); await app.listen(3000);}bootstrap(); 글로벌 스코프 필터는 앱 전반적으로 모두 사용된다. 의존성 주입 관점에서, 어떤 모듈의 외부로부터 등록된 글로벌 스코프 필터는 의존성을 주입할 수 없다. 왜냐하면 어떤 모듈의 맥락 밖에서 이미 실행된 것이기 때문이다 (?). 이러한 이슈를 해결하려면 글로벌 스코프 필터를 아래 처럼 생성된 모듈로부터 직접적으로 등록할 수 있다. 1234567891011121314// app.module.tsimport { Module } from &quot;@nestjs/common&quot;;import { APP_FILTER } from &quot;@nestjs/core&quot;;@Module({ provider: [ { provide: APP_FILTER, useClass: HttpExceptionFilter } ]})export class AppModule {} Catch everything모든 핸들링되지 않은 예외를 캐치하려면, @Catch() 데코레이터의 파라메타 리스트를 빈 상태로 두면 된다. 아래 예시는 각 예외들이 던져질 때 클래스의 타입과 관계 없이 캐치하는 필터이다. 123456789101112131415161718192021222324252627import { ExceptionFilter, Catch, ArgumentsHost, HttpException, HttpStatus} from &quot;@nestjs/common&quot;;@Catch()export class AllExceptionFilter implements ExceptionFilter { catch(exception: unknown, host: ArgumentsHost) { const ctx = host.switchToHttp(); const response = ctx.getResponse(); const request = ctx.getRequest(); const status = exception instanceof HttpException ? exception.getStatus() : HttpStatus.INTERNAL_SERVER_ERROR; response.status(status).json({ statusCode: status, timestamp: new Date().toISOString(), path: request.url }); }} 상속일반적으로, 완전하게 커스터마이징 된 예외 필터를 앱의 요구 사항에 맞춰 만들 것이다. 하지만 내장된 기본 global exception filter를 확장할 일도 있을 거다. 기본 필터가 예외 처리하는 것을 대신하기 위해서 BaseExceptionFilter를 확장하고, catch() 메서드를 상속받으면 된다. 12345678910// all-exception.filter.tsimport { Catch, ArgumentsHost } from &quot;@nestjs/common&quot;;import { BaseExceptionFilter } from &quot;@nestjs/core&quot;;@Catch()export class AllExceptionFilter extends BaseExceptionFilter { catch(exception: unknown, host: ArgumentsHost) { super.catch(exception, host); }} 위의 실행은 쉘에서 접근을 보여주는 것이다. 확장된 예외 필터의 실행은 비지니스 로직을 포함하고 있어야 한다. 글로벌 필터는 베이스 필터를 확장할 수 있다. 두 가지 방법 모두 가능하다. 첫 번째 메서드는 HttpServer 참조를 주입하는 것이고, 두 번째 방법은 위에서 사용한대로 APP_FILTER 토큰을 사용하는 것이다. 아래는 HttpServer 참조를 주입하는 방식이다. 1234567async function bootstrap() { const app = await NestFactory.create(AppModule); const { httpAdapter } = app.get(HttpAdapterHost); // HttpServer의 참조를 가져오는 방법인 것 같다. app.useGlobalFilters(new AllExceptionFilter(httpAdapter)); await app.listen(300);} 후기사실 내용 중에 이해 못 한 부분도 몇 개 있다. 원하는 내용은 소수고 별로 안 쓰겠는데 싶은 내용은 많았다. 뒤에 내용은 FP를 위한 도구이거나, 권한과 관련해서 조금 더 Nest를 잘 사용하는 방법, Interceptor ?, 커스텀 데코레이터가 남아 있긴 한데, 지금까지 공부한 내용과 TypeORM 붙이는 방법만 확인해보면 간단한 TODO List API 서버를 만드는 데 문제가 없을 것 같다. Quicklearn 시리즈는 이쯤 정리하고, 간단한 데모를 만들어보고, Nest에서 GraphQL을 잘 사용하는 방법에 대해서 공부해봐야겠다. Reference https://docs.nestjs.com/exception-filters https://docs.nestjs.com/fundamentals/execution-context","link":"/posts/backend/nestjs-quicklearn-05/"},{"title":"Monolithic 서버사이드 타입스크립트 세팅 01","text":"자바스크립트 대신, 타입스크립트를 사용하는 건 많은 부분에 있어서 장점이 있다. 그렇지만 필자 입장에서는 타입스크립트로 개발을 할 때 가장 공을 들이는 부분은 다름 아닌 세팅이다… (타입스크립트 세팅 너무 고민할 게 많아…) 이 글에서 보통 프로젝트를 제대로 프론트 개발자와 함께 시작하는 상황에서의 백앤드 개발자의 입장에서, 정리도 해둘 겸, 보통 고민하는 순서대로 한 번 세팅을 해보려고 한다. 완전히 모든 부분을 커버한 글이 아님을 미리 알립니다. 어느 정도는 JS, TS 기반 서버 구성을 해본 적 있는 분들이 보시기에 적합합니다. 일단 Typescript 서버사이드를 구성할 때, 아래 내용을 준비하는 편이다. 아래 내용들은 나름 독립적인 케이스가 많아서, 순서도 뭐 상관 없을 것 같고, 필요에 따라 선택해서 볼 수 있을 것 같긴 하다. Express 기본 아키텍처 구성 ESLint &amp; Prettier Swagger tsconfig, webpack 설정 개발 환경, 운영 환경 분리 도커라이징 이번 글에서는 ESLint &amp; Prettier 까지 설정해보려고 한다. Express 기본 아키텍처 구성이 부분은 타입스크립트가 아니더라도, 필자가 Node 앱을 만들 때 보통 비슷하게 하는 경향이 있어서 맨 처음 구성에, Node 백앤드 앱이라면 타입스크립트와 상관 없이 해야 하는 내용들 위주로 담았다. 타입스크립트 프로젝트 시작은 yarn init과 tsc --init으로 한다(typescript가 global에 설치 되어있어야 한다.). 그 다음, 보통 앱은 여기 링크에 나온 아키텍처를 선별적으로 사용하는 편이다. 최근에 커뮤니티에서 한글로 번역도 해주신 것 같은데, 링크를 어디에 저장해뒀는지 모르겠어서 일단 원래 보던 링크를 올렸다. 프로젝트 최상단에 src 디렉토리를 만들고 .gitignore를 만든다. .gitignore에는 여기 링크에 있는 .gitignore를 사용한다. 그 다음 .envs 디렉토리를 만들고 local.env, dev.env, prod.env를 만든다. 현재 프로젝트 상태는 아래와 같다. 글을 쓰면서 네이밍을 수정했습니다. 글에 나온 대로 .envs로 진행했습니다. 기본적으로 필요한 의존성 모듈을 먼저 설치하도록 한다. 그 아래부터는 디렉토리 구조와 역할을 간단하게 설명했다. 1234yarn add express cookie-parser @babel/polyfill helmet morgan dotenv# 아래는 타입스크립트로 구성할 때 해당 글에 특수한 경우? 선택하기 나름인 경우에 설치하는 부분이다.yarn add typedi typeorm reflect-metadata pg @types/{...}에 해당하는 타입 모듈들은 사용하면서, 없으면 -D 옵션을 붙여서 설치하도록 하자. @types이 부분은 type과 interface를 정의하는 곳이다. 오픈 소스 중에서도 정의가 부실한 경우에도 사용하고, 빌드 중인 앱에서 주로 타입이 필요한 경우 이곳에 정의한다. [project] 부분에는 현재 프로젝트를 적는다. 1234567// src/@types/[project]/return.d.tsexport interface ErrorSafety&lt;T&gt; { success: boolean; error?: any; result?: T;} 1234567// src/@types/[project]/index.d.tsimport { ErrorSafety } from &quot;./return&quot;;declare global { export interface Mutation&lt;T = any&gt; extends ErrorSafety&lt;T&gt; {} export interface ServiceData&lt;T = any&gt; extends ErrorSafety&lt;T&gt; {}} configsconfigs 디렉토리에서는 설정값을 모두 관리한다. 보통 index.ts에서 process.env 값을 관리하게 되어있다. process.env에 환경 변수들을 넣는 건 dotenv를 사용할 것이다. 대충 아래처럼 사용하는 편이다. 12345678910111213141516// src/configs/index.tsimport dotenv from &quot;dotenv&quot;;if (!process.env?.ENV) { dotenv.config();}export default { ENV: String(process.env.ENV), SECRET: String(process.env.SECRET), APP: { PORT: Number(process.env.APP_PORT), LOGSTAGE: String(process.env.APP_LOGSTAGE) }}; errors에러를 정의한다. 특히 response로 돌려줘야 하는 에러인 경우 statusCode라는 프로퍼티를 담아서 next() 함수에 모아준다. 아래 loaders에서 마지막 에러처리 하는 함수에서 해당 코드를 인지해서 에러를 반환한다. 12345678910111213141516171819202122232425262728// src/errors/errRequest.tsimport { BAD_REQUEST, INTERNAL_SERVER_ERROR, NOT_FOUND} from &quot;http-status-codes&quot;;abstract class RequestError extends Error { statusCode: number; constructor(message?: string) { super(message); }}export class BadRequest extends RequestError { constructor(message: string = &quot;유효하지 않은 요청입니다.&quot;) { super(message); this.statusCode = BAD_REQUEST; }}export class NotFound extends RequestError { constructor(message: string = &quot;리소스가 존재하지 않습니다.&quot;) { super(message); this.statusCode = NOT_FOUND; }} loadersloaders는 Express Application이 앱을 시작하기 전에, 올려야 하는 부분들이다. 앱을 설정해주는 부분이라고 볼 수도 있는데, Database를 연결해주는 것, 애플리케이션 설정해주는 것 등을 나눠서 코드를 짜는 편이다. 1234567891011// src/loaders/index.tsimport { Express } from &quot;express&quot;;import appLoader from &quot;./appLoader&quot;;import dbLoader from &quot;./dbLoader&quot;;const loaders = async (app: Express) =&gt; { appLoader(app); await dbLoader();};export default loaders; 123456789101112131415161718192021222324252627282930// src/loaders/appLoader.tsimport express, { Express, Request, Response, NextFunction } from &quot;express&quot;;import helmet from &quot;helmet&quot;;import logger from &quot;morgan&quot;;import cookieParser from &quot;cookie-parser&quot;;import configs from &quot;@/configs&quot;;import routers from &quot;@/routers&quot;;const appLoader = (app: Express) =&gt; { app.use(helmet()); app.set(&quot;port&quot;, configs.APP.PORT); app.use(logger(configs.APP.LOGSTAGE)); app.use(cookieParser(configs.SECRET)); app.use(express.urlencoded({ extended: true })); app.use(express.json()); app.use(&quot;/api&quot;, routers); app.use((err: any, req: Request, res: Response, next: NextFunction) =&gt; { console.log(err); res.status(err.statusCode || 500).json({ message: err.message, statusCode: err.statusCode, error: configs.ENV === &quot;prod&quot; ? null : err }); });};export default appLoader; @/configs, @/routers에서 @/* 는 src/*와 동일하다. 상대경로가 길어지는 것이 싫어서 alias 설정을 해주는 편인데, 이 부분은 tsconfig, webpack 설정을 마치면 사용할 수 있게 된다. ORM은 TypeORM을 주로 사용하는 편이다. 각자가 편한 ORM을 dbLoader에서 설정해주면 되지만, 아무튼 필자는 typeorm에서 필요한 모듈을 추가로 설치한 다음 아래와 같이 연결을 해준다. (typeorm은 프로젝트 최상단에 ormconfig.json을 두면 별 다른 옵션을 만들어줄 필요가 없다.) 1234// loaders/dbLoaders.tsimport { createConnection } from &quot;typeorm&quot;;const dbLoader = () =&gt; createConnection(); models모델에는 ORM 로직들을 담는다. models/entities 디렉토리에는 ORM으로 데이터베이스를 설계하는 로직(데이터베이스 테이블을 정의하는 부분들)이 들어간다. 123456789101112131415161718192021222324252627// src/models/entities/User.entity.tsimport { Entity, BaseEntity, PrimaryGeneratedColumn, Column, CreateDateColumn, UpdateDateColumn} from &quot;typeorm&quot;;@Entity()class User extends BaseEntity { @PrimaryGeneratedColumn() id: number; @Column({ type: &quot;varchar&quot;, length: 20 }) username: string; @CreateDateColumn({ type: &quot;timestamp with time zone&quot; }) createdAt: Date; @UpdateDateColumn({ type: &quot;timestamp with time zone&quot; }) updatedAt: Date;}export default User; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// src/models/userModel.tsimport User from &quot;./entities/User.entity&quot;;import { DeepPartial, FindOneOptions } from &quot;typeorm&quot;;import { QueryDeepPartialEntity } from &quot;typeorm/query-builder/QueryPartialEntity&quot;;class UserModel { getAll(): Promise&lt;User[]&gt; { return User.find(); } getOneById(id: number): Promise&lt;User | undefined&gt; { return User.findOne(id); } getOne(findOneOptions: FindOneOptions): Promise&lt;User | undefined&gt; { return User.findOne(findOneOptions); } async createOne(user: DeepPartial&lt;User&gt;): Promise&lt;Mutation&lt;User&gt;&gt; { try { const result = await User.create(user).save(); return { success: true, result }; } catch (e) { return { success: false, error: e }; } } async updateOneById( id: number, partialData: QueryDeepPartialEntity&lt;User&gt; ): Promise&lt;Mutation&gt; { try { await User.update(id, partialData); return { success: true }; } catch (e) { return { success: false, error: e }; } }}export { User };export default UserModel; 위 예시처럼, 작은 앱일 수록 src/models/userModel.ts 부분이 불필요한 로직만 추가하게 되는 경우가 많았는데, 복잡한 ORM 로직을 작성해야 하는 경우가 많아질 수록 이 부분이 참 쓸모가 있었다. 예를 들어서 User.update() 함수로 많은 업데이트가 해결 되지만, 디테일하게 ORM을 사용해야 하거나 Raw Query를 작성해야 하는 경우, services에 담고 싶지 않아지는 경우가 많이 생긴다. 따라서 작은 앱을 계획 중이라면, entities 부분을 models에 두고, ORM 로직을 services에 같이 두는게 훨씬 좋을 수 있다. 아래 User 부분을 다시 export한 이유는 딱히 없는데, 그냥 models/entities에 접근하는 파일이 models에만 있길 바라서 그랬다. 취향따라 빼면 된다. 보통 저렇게 export 하면, 타입 선언할 때 자주 사용된다. routers컨트롤러 로직이 담기게 된다. v1 디렉토리를 안에 두는 편인데, 버전이 크게 변하면 v2로 업데이트 한다. 123456789101112131415// src/routers/index.tsimport express from &quot;express&quot;;// http-status-codes는 상수를 담고 있는 패키지일 뿐이니, 취향대로 사용import { OK } from &quot;http-status-codes&quot;;import v1 from &quot;./v1&quot;;const router = express.Router();router.get(&quot;/health&quot;, (req, res) =&gt; { res.status(OK).json({ server: &quot;on&quot; });});router.use(&quot;&quot;, v1);export default router; 1234567891011// src/routers/v1/index.tsimport express from &quot;express&quot;;import userRouter from &quot;./userRouter&quot;;import { isAuthenticated, isNotAuthenticated } from &quot;@/routers/middlewares&quot;;const router = express.Router();router.use(&quot;/users&quot;, isNotAuthenticated, userRouter);router.use(&quot;/auth&quot;, isAuthenticated, authRouter);export default router; routers에 있는 middlewares 부분은 로그인이 되어있는지 아닌지 판단해주는 미들웨어를 달았다 (실제로 만든 건 아니고, middlewares는 여기에 둔다는 정도). 디테일한 부분은 글에서 다루지는 않는다. 1234567891011121314151617181920212223242526272829303132import express, { Request, NextFunction, Response } from &quot;express&quot;;import { Container } from &quot;typedi&quot;;import { OK } from &quot;http-status-codes&quot;;import UserService from &quot;@/services/userService&quot;;import { BadRequest } from &quot;@/errors/errRequest&quot;;const router = express.Router();router.get(&quot;&quot;, async (req: Request, res: Response, next: NextFunction) =&gt; { const userService = Container.get(UserService); const userList = await userService.getAllUser(); res.status(OK).json(userList);});router.post(&quot;&quot;, async (req: Request, res: Response, next: NextFunction) =&gt; { const { username } = req.body; try { if (!username) { throw new BadRequest(&quot;필수 필드가 필요합니다.&quot;); } const userService = Container.get(UserService); const ret = await userService.createOneUser(username); if (!ret.success) throw ret.error; res.status(OK).json(ret.result); } catch (e) { next(e); // error는 모아져서 loaders에 정의해둔 에러처리 미들웨어로 들어가게 된다. }});export default router; typedi에 대해서는 레포지토리 링크나, 위에서 언급한 이 프로젝트의 기반이 되는 링크를 참조하길 바란다. 사실 없어도 되고 있어도 되는데, 테스트 함수를 작성하기 훨씬 쉬워진다고 한다 (체감은 뭐… 그냥 그럼. 어짜피 테스트 코드는 힘드렁). UserService를 작성한 부분은 아직 작성되지 않았고 바로 다음에 이 부분을 확인할 수 있다. services123456789101112131415161718// src/services/userService.tsimport { Service } from &quot;typedi&quot;;import UserModel, { User } from &quot;@/models/userModel&quot;;@Service()class UserService { constructor(private userModel: UserModel) {} getAllUser() { return this.userModel.getAll(); } createOneUser(username: string): Promise&lt;ServiceData&lt;User&gt;&gt; { return this.userModel.createOne({ username }); }}export default UserService; models는 데이터베이스 정의된 로직과 관계가 있다. 하나의 테이블만 사용하는 로직을 짜야 하고, services는 여러 models의 로직을 조합해 controller에서 사용될 로직을 만드는 것이다. 따라서 네이밍도 UserServices가 되었지만 데이터베이스 테이블과 연관되어있기 보단, 라우터에 /users와 연관 되어 있다고 볼 수 있다. 즉, authModel은 없지만, 라우터에 /auth가 있으므로 AuthServices는 생성될 수 있는 것이다. (ServiceData는 필자가 정의한 서비스 로직에서의 리턴값이다. models.d.ts에서 Mutation과 값이 같다.) 함수 네이밍도 이와 관련 있다. UserService에서는 항상 User와 관련된 처리만 하는 경우가 아닐 수도 있다. /users 라우팅에서 처리하는 로직과 관련 되어있을 뿐이다. 따라서 모델 함수처럼 getAll에서 끝나는 것이 아니라, getAllUser라고 이름 붙이는 편이다. app.ts앱을 시작하는 로직을 담았다. 실제로 시작되는 곳은 index.ts이긴 한데, 여기서 시작해도 별 상관은 없다. 1234567891011121314151617181920// src/app.tsimport express, { Express } from &quot;express&quot;;import loaders from &quot;@/loaders&quot;;class App { private app: Express; constructor() { this.app = express(); } async bootstrap() { await loaders(this.app); this.app.listen(this.app.get(&quot;port&quot;), () =&gt; { console.log(&quot;Server is On&quot;); }); }}export default App; index.ts이 부분이 시작점이 된다. 1234567// src/index.tsimport &quot;reflect-metadata&quot;;import App from &quot;./app&quot;;const app = new App();app.bootstrap(); 최종적으로는 아래와 같은 모습이다. 대충 디렉토리 구조는 이렇게 생겼다. 이후로 redis라든지, socketIO 등을 붙일 때도 어디에 맞는 것인지 판단하면서 추가하면 된다. ESLint &amp;&amp; Prettier혼자 할 때는 사실 프리티어만 사용하는 편이다. 이유가 있긴 한데, ESLint가 솔직히 TS에 맞추려면 너무 귀찮게 할 게 많다. 그리고, 이 섹션은 그 귀찮은 것들을 모두 해결하지 않고 그냥 규칙을 off 한다. 근데 이제 팀 프로젝트가 되다 보면, 코드를 강제할 필요가 있기 때문에 진행하기는 하는데, 이거 말고, gts를 사용해보길 추천하는 바이다. ESLint 설정은 eslint --init으로 만들 수 있다. 1234567891011121314? How would you like to use ESLint? To check syntax, find problems, and enforce code style? What type of modules does your project use? JavaScript modules (import/export)? Which framework does your project use? None of these? Does your project use TypeScript? Yes? Where does your code run? Node? How would you like to define a style for your project? Use a popular style guide? Which style guide do you want to follow? Airbnb: https://github.com/airbnb/javascript? What format do you want your config file to be in? JSONChecking peerDependencies of eslint-config-airbnb-base@latestLocal ESLint installation not found.The config that you've selected requires the following dependencies:@typescript-eslint/eslint-plugin@latest eslint-config-airbnb-base@latest eslint@^5.16.0 || ^6.1.0 eslint-plugin-import@^2.18.2 @typescript-eslint/parser@latest? Would you like to install them now with npm? Yes 린트에서 typescript를 설치하라고 하기 때문에, 개발 의존성에 추가한다. prettier, eslint-plugin-prettier와 eslint-config-prettier도 함께 설치한다. eslint-plugin-prettier: prettier와 함께 쓸 수 있도록 해준다. eslint-config-prettier: eslint에서 prettier config를 찾게 함 1yarn add typescript prettier eslint-plugin-prettier eslint-config-prettier -D .prettierrc 파일을 프로젝트 최상단에 추가한다. 옵션들은 본인 취향에 맞게 설정하면 된다. 1234567// .prettierrc{ &quot;semi&quot;: true, &quot;singleQuote&quot;: false, &quot;tabWidth&quot;: 2, &quot;useTabs&quot;: false} package.json에 린트 명령어를 추가한다. 12345678// package.json{ ... &quot;scripts&quot;: { &quot;lint&quot;: &quot;eslint \\&quot;./src/**/*.ts\\&quot;&quot; }, ...} 프로젝트는 VSCode를 종료했다 새로 열면 빨간 줄을 열심히 보여준다. rules 중에 본인이 편한 방식과 안 맞는다면, rules에 해당 이름을 추가해서 바꿔주거나 끌 수 있다. 그런데, 필자의 방식에서 해결하지 못 한게 있는데, import/no-unresolved, no-unresolved와 Type을 사용할 때 no-unused-vars가 에러를 나타낸다는 점을 해결하지 못했다. 그리고 코드 스타일에 맞게 여기 저기 rules를 껐는데, 결과적으로 rules 부분은 아래와 같다. 1234567891011121314151617{ ... &quot;rules&quot;: { &quot;quotes&quot;: [2, &quot;double&quot;], &quot;class-methods-use-this&quot;: &quot;off&quot;, &quot;import/no-named-as-default-member&quot;: &quot;off&quot;, &quot;import/no-unused-vars&quot;: &quot;off&quot;, &quot;import/no-unresolved&quot;: &quot;off&quot;, &quot;no-unused-vars&quot;: &quot;off&quot;, &quot;max-classes-per-file&quot;: &quot;off&quot;, &quot;import/no-extraneous-dependencies&quot;: &quot;off&quot;, &quot;import/extensions&quot;: &quot;off&quot;, &quot;no-useless-constructor&quot;: &quot;off&quot;, &quot;no-empty-function&quot;: &quot;off&quot; } ...} 코드 스타일에 맞게도 그렇지만, TS를 제대로 인식 못 해서 useless하지 않은 것들을 useless라고 판단하는 이상한 것들이 많았다. TSLint는 ESLint와 통합된다고 한 걸로 알고 있는데, 이렇게 구지면 대안이 뭔가 싶다. (gts를 사용해볼 것을 추천한다… 필자는 써본 적은 없는데, 갓글이니까 훨씬 낫겠지) 참고로 alias 설정 때문에, 위 과정을 쭉 따라오고 있는 사람은 바로 돌려 볼 수도 없다. 데이터베이스도 연결 안 되어 있기 때문에. 데이터베이스는 마지막에 연결할 예정이다. 지금까지 과정을 확인하려면, import할 때 alias 사용하지 말고, dbLoader를 실행하는 부분 주석처리 하고, 디렉토리가 아니라 파일 .env를 최상단에 만든 다음, router 부분에서 v1으로 라우팅 되는 부분을 주석 해주면 된다. 서버가 돌아가긴 한다 (물론 ts-loader를 사용해서 index.ts를 실행 시켜야 한다. 또는 간단하게 빌드 해서 빌드된 파일을 돌려보면 된다.). 다음 글에서는 .env가 최상단에 있는 상황을 가정하고 진행한다. (.envs 디렉토리도 있음) .env는 이후 local.env로 사용하시면 됩니다. 지금까지 진행된 내용은 이 링크에서 확인할 수 있다. 후기글이 생각보다 너무 길어져서 끊어서 작성하려 하지만, 이미 너무 긴 감이 있다. 그리고 글쓰기 너무 힘들었다. 귀찮아서 대충 한 부분이 있나 싶기도 하다. Reference https://dev.to/santypk4/bulletproof-node-js-project-architecture-4epf https://jamong-icetea.tistory.com/349","link":"/posts/backend/serverside-typescript-setting-01/"},{"title":"Monolithic 서버사이드 타입스크립트 세팅 02","text":"지난 글에서 두 가지를 완료 했었다. 물론 잘 작동하는지 확인하기는 어려웠다. tsconfig, webpack까지만 잘 설정하고 나면, 확인은 가능할 것 같다. 맨 처음 목차와는 조금 다르긴 한데, tsconfig와 webpack을 먼저 설정한 다음 스웨거를 설정하도록 하자. Express 기본 아키텍처 구성 ESLint &amp; Prettier tsconfig, webpack 설정 Swagger 개발 환경, 운영 환경 분리 도커라이징 tsconfigtsconfig는 그냥 자바스크립트 이용자는 alias를 VSCode가 인식하게 한다는 면에서, jsconfig로 대체할 수 있다. 그 방법을 이 글에서 다루지는 않지만, 갓글에서 많은 분들이 제공해주고 있다. 일단 결론적으로 말하자면 현재 tsconfig의 설정은 아래와 같다. 12345678910111213141516171819202122232425262728293031323334353637{ &quot;compilerOptions&quot;: { /* Basic Options */ &quot;target&quot;: &quot;ES2018&quot; /* Specify ECMAScript target version: 'ES3' (default), 'ES5', 'ES2015', 'ES2016', 'ES2017', 'ES2018', 'ES2019', 'ES2020', or 'ESNEXT'. */, &quot;module&quot;: &quot;commonjs&quot; /* Specify module code generation: 'none', 'commonjs', 'amd', 'system', 'umd', 'es2015', 'es2020', or 'ESNext'. */, &quot;allowJs&quot;: false /* Allow javascript files to be compiled. */, &quot;sourceMap&quot;: true /* Generates corresponding '.map' file. */, &quot;outDir&quot;: &quot;./dist/&quot; /* Redirect output structure to the directory. */, &quot;downlevelIteration&quot;: true /* Provide full support for iterables in 'for-of', spread, and destructuring when targeting 'ES5' or 'ES3'. */, /* Strict Type-Checking Options */ &quot;strict&quot;: true /* Enable all strict type-checking options. */, &quot;strictPropertyInitialization&quot;: false /* Enable strict checking of property initialization in classes. */, /* Additional Checks */ &quot;noUnusedLocals&quot;: true /* Report errors on unused locals. */, &quot;baseUrl&quot;: &quot;./&quot; /* Base directory to resolve non-absolute module names. */, &quot;paths&quot;: { &quot;@/*&quot;: [&quot;src/*&quot;] } /* A series of entries which re-map imports to lookup locations relative to the 'baseUrl'. */, &quot;typeRoots&quot;: [ &quot;node_modules/@types&quot;, &quot;src/@types&quot; ] /* List of folders to include type definitions from. */, &quot;esModuleInterop&quot;: true /* Enables emit interoperability between CommonJS and ES Modules via creation of namespace objects for all imports. Implies 'allowSyntheticDefaultImports'. */, /* Source Map Options */ &quot;sourceRoot&quot;: &quot;./src&quot; /* Specify the location where debugger should locate TypeScript files instead of source locations. */, &quot;mapRoot&quot;: &quot;./src&quot; /* Specify the location where debugger should locate map files instead of generated locations. */, &quot;experimentalDecorators&quot;: true /* Enables experimental support for ES7 decorators. */, &quot;emitDecoratorMetadata&quot;: true /* Enables experimental support for emitting type metadata for decorators. */, /* Advanced Options */ &quot;forceConsistentCasingInFileNames&quot;: true /* Disallow inconsistently-cased references to the same file. */ }, &quot;include&quot;: [&quot;src/**/*.ts&quot;]} 위는 사용 중인 옵션을 제외하고는 모두 삭제한 것이다. 글을 쓰면서 평소에 아리송 했던 것들도 정리해봤다. 딱 보면 감이 오는 몇 가지를 제외하고, 먼저 baseUrl과 paths는 VSCode상에서 부분에서 alias를 설정해주는 것에 의미가 있다. 실제로 컴파일할 때 이 부분을 고려해서 컴파일 해주지는 않는다. (?.. 왜지, 진짜 버그같음) jsconfig에서도 해당 설정이 존재한다. 마찬가지로 VSCode에서 인식할 수 있게 바꿔주는 역할 정도만 해준다. ts-node를 사용해서 실행할 때 조차도 인식을 못 해주는데, tsconfig-paths 모듈을 설치하고 -r tsconfig-paths/register 옵션을 넣어줘야 한다. 자세한 내용은 개발 환경 설정 하면서 작성하려고 한다. emitDecoratorMetadata와 experimentalDecorators는 데코레이터 사용을 가능하게 해주는 것과 관련된 설정인데, typedi와 typeorm 사용시 필요하다. 메타데이터와 관련된 내용은 확인해보지 못 했다. strictPropertyInitialization 이 부분도 typeorm 테이블 모델링을 할 때 문제가 있어서 추가한 부분이다. 클래스 내부의 프로퍼티가 컨스트럭터에서 초기화 되지 않을 때 에러를 뿜는 것인데 이 부분을 꺼줬다. esModuleInterop 옵션은 commonjs로 작성된 모듈을 가져올 때, 원래는 import * as Module from &quot;module&quot;과 같은 형식으로 가져와야 하는데, import Module from &quot;module&quot;로 가져올 수 있도록, 컴파일 할 때 특수한 중간자를 둔다. namespace object를 만들어주는 함수를 사용하는 것 같다. 자세한 내용은 스택오버플로우에 있다. 모든 내용은 당연히 알지 못하고, (그거 하나하나 공부하면 시작을 못 한다고 생각하기 때문에…) 필요할 때마다 어떤 옵션을 써야 하는지 찾아보는 편인데, 타입스크립트는 tsc --init 명령어를 사용하면 모든 옵션이 있고, 설명도 주석으로 적당히 친절하게 달아둔 편이다. 지우지말고, 필요할 때 찾아가면서 옵션을 설정해주는 편이 좋다. Webpack우선, tsconfig를 통해 VSCode 상에서 alias로 코드를 가져온 것을 인식하게 만들었다. 그렇지만 tsc 명령어로 트랜스파일링을 시도한다고 해도, 이 부분이 실제로 반영되지 않는다. 그래서 webpack을 사용해서 해당 alias에 맞게 트랜스파일을 할 수 있게 만든다. 또, 번들링을 함으로써 코드를 최적화 하고 도커 이미지 크기를 줄이는 것에도 목적이 있다. 우선 웹팩으로 빌드하는 환경을 만들기 위해서 아래 의존 패키지를 설치한다. 1yarn add webpack webpack-cli webpack-node-externals rimraf @babel/{core,preset-env,preset-typescript,plugin-proposal-decorators,plugin-proposal-class-properties} babel-loader -D webpack, webpack-cli: 웹팩 설정 파일을 읽고 파일들을 번들링 해주는 핵심 역할을 한다. webpack-node-externals: 필자는 node_modules 내용을 함께 번들링 하고 싶어하는 편이다. 여러번 시도를 해봤지만, 성공할 수가 없었다. 특히 serverless-webpack에서 이 부분을 함께 번들링 하지 않는 것을 보고, 그냥 포기 했다. node_modules를 번들링 대상에서 제외해주는 역할을 하는 패키지이다. rimraf: OS에 관계없이 삭제하는 명령어를 수행할 수 있다. 빌드된 디렉토리를 삭제해준다. @babel/{core,preset-env,preset-typescript}: 필자는 ts-loader를 사용하지 않고, 바벨을 통해 해결하는 편이다. 설정이 깔끔해지고, 더 빠르다고 한다. 그 이유와 자세한 내용은 대단하기로 소문난 Toast팀의 블로그를 확인해보자. @babel/plugin-proposal-decorators: 바벨에서 데코레이터를 트랜스파일링 할 수 있게 해준다. @babel/plugin-proposal-class-properties: 바벨에서 클래스의 프로퍼티(메서드 말고)를 트랜스파일링 할 수 있게 해준다. 위 두 플러그인은 설정값과 순서도 지켜줘야 한다. babel-loader: webpack 모듈의 rules에서 로더로 작동하게 될 babel-loader이다. 플러그인들은 코드를 쓰다 보면 추가될 수 있다. 빌드 에러가 나면 해당 부분을 찾아서 필요한 플러그인을 추가해주면 된다. 설치 후 프로젝트 최상단에 webpack.config.js 파일을 만들어주면 된다. 내용은 결과적으로 아래와 같다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344const path = require(&quot;path&quot;);const nodeExternals = require(&quot;webpack-node-externals&quot;);module.exports = env =&gt; { return { // ReferenceError: regeneratorRuntime is not defined =&gt; polyfill entry: { app: [&quot;@babel/polyfill&quot;, &quot;./src/index.ts&quot;] }, devtool: &quot;source-map&quot;, output: { filename: &quot;[name].js&quot;, path: path.resolve(__dirname, &quot;dist&quot;) }, mode: env.NODE_ENV, target: &quot;node&quot;, node: { __dirname: false, __filename: false }, externals: [nodeExternals()], module: { rules: [ { test: /\\.(ts|js)?$/, use: { loader: &quot;babel-loader&quot;, options: { presets: [&quot;@babel/preset-env&quot;, &quot;@babel/preset-typescript&quot;], plugins: [ [&quot;@babel/plugin-proposal-decorators&quot;, { legacy: true }], [&quot;@babel/plugin-proposal-class-properties&quot;, { loose: true }] ] } } } ] }, resolve: { extensions: [&quot;.ts&quot;, &quot;.js&quot;, &quot;.json&quot;], alias: { &quot;@&quot;: path.join(__dirname, &quot;src&quot;) } } };}; nodeExternals() 덕분에 과거에 node 옵션으로 추가 했던 많은 부분을 사용 안해도 되는 것 같다 (확실하지는 않다. 간단하게 실험해봤을 때 unresolve되는 경우는 없었다.). 다만, __dirname과 filename은 그냥 사용하면 /, index.js가 나온다. 제대로 반영하게 하기 위해서 node 옵션에 __dirname, __filename을 false 값으로 둔다. (아마 이것도 추측이지만, webpack은 기본적으로 웹 환경에서 돌아가는 걸 상정해서 __dirname, __filename을 덮어 씌우는 것 같다. 그 기능을 꺼주는 걸로 알고 있다.) @babel/polyfill은 async await 문법을 컴파일 할 때 문제가 생기지 않게 해준다. 그냥 사용하게 되면 regeneratorRuntime is not defined라는 에러가 발생한다. module 부분에 타입스크립트를 어떻게 컴파일할 것인지 나와있다. ts-loader 역할을 대신하게 된다. resolve 부분에서 @/*를 src/*로 바꿔준다. 이제 컴파일을 하게 되면 온전하게 파일 경로를 설정해준다. 컴파일을 위한 webpack 설정은 얼추 마무리 됐지만, Swagger를 설정하면서, 데이터베이스를 붙이면서, 도커라이징을 하면서 조금씩 추가되거나 수정되는 부분이 생기게 된다. 기타 자세한 옵션은 webpack 공식 문서나 간단하게는 이 링크를 확인해봐도 좋을 것 같다. 마지막으로 웹팩 빌드를 위한 스크립트를 붙여주자. 1234567891011// package.json{ //... &quot;scripts&quot;: { &quot;build:dev&quot;: &quot;rimraf ./dist &amp;&amp; webpack --env.NODE_ENV=development&quot;, &quot;build:prod&quot;: &quot;rimraf ./dist &amp;&amp; webpack --env.NODE_ENV=production&quot;, &quot;start&quot;: &quot;node ./dist/app.js&quot;, &quot;lint&quot;: &quot;eslint \\&quot;./src/**/*.ts\\&quot;&quot; } //...} Swagger스웨거는 API 문서 만들어주는 툴이다. 사용 방법에 대해서 자세히 다루지는 못할 것 같고, 붙이는 것만 확인해보자. 우선 목표는, 주석으로 문서화를 할 수 있게 되어야 하고, 개발 서버에 배포될 때, 웹팩이 해당 주석을 지우지 않아야 한다. 우선 express에서 개발 서버와 함께 띄우기 위해 swagger-ui-express와 swagger-jsdoc을 사용한다. 1yarn add swagger-ui-express swagger-jsdoc swagger-ui-express: Express 서버에 Swagger 서버가 뜰 수 있게 미들웨어를 제공해준다. swagger-jsdoc: JSDoc 코멘트 형식으로 API Doc을 구성하게 되면 인식하고 swagger.json을 만들어준다. 이 내용을 swagger-ui-express에 넣어주면 된다. 문제는 코멘트 형식이라 빌드할 때 웹팩을 사용하면 지워진다. 두 가지 방식으로 해당 문제를 해결할 수 있다(있을 것으로 추정된다.). 한 가지 방법은 NODE_ENV에 따라서 @swagger가 붙은 코멘트를 지우지 않는 옵션을 넣어주는 것이고, 두 번째는 스타 2개 붙은 webpack 플러그인을 사용하는 것인데, 필자는 그래도 이 템플릿을 프로덕트에도 사용할 예정이라 그냥 첫 번째 방법으로 하기로 했다. 우선 swagger 디렉토리를 src 아래 만들었다. 안에는 index.ts를 만든다 1234567891011121314151617181920212223242526272829303132// src/swagger/index.tsimport path from &quot;path&quot;;import swaggerJSDoc from &quot;swagger-jsdoc&quot;;import configs from &quot;@/configs&quot;;import components from &quot;./components&quot;;const options = () =&gt; { const isLocal = configs.ENV === &quot;local&quot;; const apiPath = isLocal ? path.join(__dirname, &quot;..&quot;, &quot;routers&quot;, &quot;v1&quot;, &quot;*.*&quot;) : path.join(__dirname, &quot;app.js&quot;); return { swaggerDefinition: { info: { title: &quot;프로젝트 타이틀&quot;, version: &quot;프로젝트 버전&quot; }, host: configs.APP.HOST, // localhost 등 basePath: &quot;/api&quot;, schemas: [&quot;http&quot;, &quot;https&quot;], components }, apis: [apiPath] };};// Initialize swagger-jsdoc -&gt; returns validated swagger spec in json formatconst swaggerSpec = swaggerJSDoc(options());export default swaggerSpec; 빌드를 하면, 하나의 파일인 app.js에 모든 주석이 들어가게 되기 때문에 ts-node를 사용해 동작시키는 로컬 환경이 아니라면, app.js에서 주석을 찾아야 한다. 반면 타입스크립트 파일을 그대로 돌리는 ts-node를 사용하면, router 아래 라우터들마다 존재하게 된다. 따라서 apiPath는 컴파일 여부에 따라서 바뀐다. 이제, 일반적으로 사용되는 부분들을 컴포넌트로 만들기 위해서 swagger 폴더에 components.ts롤 만든다. 123456789101112131415161718192021222324252627282930313233343536373839import { MESSAGE } from &quot;@/errors/errRequest&quot;;const components = { response: { BadRequest: { description: MESSAGE.BAD_REQUEST, schema: { $ref: &quot;#/components/errorResult/Error&quot; } }, NotFound: { description: MESSAGE.NOT_FOUND, schema: { $ref: &quot;#/components/errorResult/Error&quot; } } }, errorResult: { Error: { type: &quot;object&quot;, properties: { message: { type: &quot;string&quot;, description: &quot;error message&quot; }, error: { type: &quot;string&quot;, description: &quot;error stack&quot; }, statusCode: { type: &quot;number&quot;, description: &quot;error code&quot; } } } }};export default components; 에러 메시지를 실제로 만들어내는 Default한 메시지 값과 동일하게 하기 위해서 src/errors/errRequest 아래 메시지를 컨스트로 관리한다. 그리고 errorResult의 형태는 express에서 마지막에 에러를 모아서 보내주는 부분과 동일하게 맞춰준다. 에러 컴포넌트 말고도, 일반적으로 사용되는 Response를 위와 같이 만들어두고 사용하면 편하다. 아래는 errors에서 constants를 관리하는 코드이다. 1234567891011121314151617181920// src/errors/errReqeust.tsimport { BAD_REQUEST, NOT_FOUND } from &quot;http-status-codes&quot;;export const MESSAGE = { BAD_REQUEST: &quot;유효하지 않은 요청입니다.&quot; as const, NOT_FOUND: &quot;리소스가 존재하지 않습니다.&quot; as const // ...};// ...export class BadRequest extends RequestError { constructor(message: string = MESSAGE.BAD_REQUEST) { super(message); this.statusCode = BAD_REQUEST; }}// ... 이제 appLoader에 /api-docs로 이어지는 부분을 추가해주자. 1234567891011121314151617181920212223242526// src/app.ts// ...const appLoader = async (app: Express) =&gt; { // ... if (configs.NODE_ENV === &quot;development&quot;) { const { default: swaggerUi } = await import(&quot;swagger-ui-express&quot;); const { default: swaggerSpec } = await import(&quot;@/swagger&quot;); app.use(&quot;/api-docs&quot;, swaggerUi.serve, swaggerUi.setup(swaggerSpec)); } app.use(&quot;/api&quot;, routers); app.use((err: any, req: Request, res: Response, next: NextFunction) =&gt; { console.log(err); res.status(err.statusCode || 500).json({ message: err.message, statusCode: err.statusCode, error: configs.ENV === &quot;prod&quot; ? null : err }); });};export default appLoader; src/loaders/index.ts에서 appLoader도 await을 붙여줘야 한다. 이제 그럼 webpack에서 prod 모드로 빌드 하면 /api-docs에는 아무 것도 없어야 하고, dev 모드로 빌드하면 어떤 UI가 나와야 한다. 테스트를 위해서 console.log를 붙이고 확인해봤다. production 모드 develoment 모드 Swagger의 UI 이제 스웨거가 붙긴 했다. 그렇지만 여전히 주석 형태로 API Doc을 달았을 때 웹팩이 지워버리는 문제가 있다. 이 부분은 웹팩의 optimization 옵션과 관련이 있다. minimizer를 terser-webpack-plugin을 이용해서, NODE_ENV에 따라서, 다르게 번들링 하도록 설정했다. 123456789101112131415161718192021222324252627282930const path = require(&quot;path&quot;);const TerserPlugin = require(&quot;terser-webpack-plugin&quot;);const nodeExternals = require(&quot;webpack-node-externals&quot;);module.exports = env =&gt; { const isDev = env.NODE_ENV === &quot;development&quot;; const terserPlugin = isDev ? new TerserPlugin({ terserOptions: { output: { comments: /@swagger/i } } }) : new TerserPlugin(); return { // ... output: { filename: &quot;[name].js&quot;, path: path.resolve(__dirname, &quot;dist&quot;) }, optimization: { minimize: true, minimizer: [terserPlugin] }, //...}; 테스트를 위해 health를 체크해주는 앤드포인트에 해당 주석을 달았다. 그리고, 실행 환경인 ENV는 원래는 local인데, 개발 서버에 올라갔을 경우의 문제를 해결하는 중이니까 ENV도 dev로 바꿔준다. 12345678910111213/** * @swagger * /health: * get: * summary: 서버 헬스 체크 * responses: * 200: * description: 서버 헬스 체크용도, Swagger 체크 용도 */router.get(&quot;/health&quot;, (req, res) =&gt; { res.status(OK).json({ server: &quot;on&quot; });}); 이제 다시 빌드를 해보고나서 같은 앤드포인트에 가보면 우리가 만든 스웨거가 등장한다. 로컬 환경에서는 어짜피 웹팩이 동작하지 않아서 무조건 살아있게 된다. 따라서 테스트 해볼 필요는 없겠지만, 다음 글에서 로컬 개발환경을 설정하고 나면 확인할 수 있다. 환경변수에 대해서 헷갈릴 수 있을 것이라 생각이 되어 주석을 달자면, NODE_ENV는 Node에서 정해진 환경변수이다. NODE_ENV는 development, production 두 가지 중에 하나이다. ENV는 필자가 정한 환경 변수이다. .env에 따라서 수정할 수 있다. NODE_ENV에 대해서 검색해보면 더 많은 정보를 확인해볼 수 있다. 이번 글은 이 정도로 마무리 지으려고 한다. 자세한 Swagger 사용법에 대해서는 다루지 않는다. (필자도 잘 모름… 쓰던 것만 쓰는 편임) 그래도 찾아보면 잘 나오니까 우리는 찾아가면서 쓰도록 하자. 다음 글에서 도커라이징한 다음, 환경 별로 .env를 설정해두는 방법, 로컬에서 돌리는 방법 등을 설정해두자. 테스트를 해보면서 이 글에 나오지 않은 부분들이 레포지토리에 추가 되긴 했다. 중요한 부분은 아니라서 글에 작성하지 않았다. 지금까지 구성된 내용은 이 링크에서 확인할 수 있다. 후기개발 서버에서 SwaggerUI를 보내는 방식이 깔끔한 방식인지 모르겠다. 일단 이상한 버그가 있는데, 필자는 함수 안에 주석을 넣고 싶다. 에디터에서 함수를 fold 했을 때, 함께 접히면 좋을 것 같아서. 그런데 어떤 이상한 확률로 그런 식으로 작성된 주석을 웹팩이 지워버렸다. 이유를 모르겠다. 그래서 밖에다 빼고 사용한지 좀 됐는데 그 이후로는 그런 문제가 생기지는 않았다. 1, 2, 3편으로 이어지는 이 글이 사실은 한 번에 설정하는 내용인데 끊어 정리 하느라, 매끄럽지 않은 느낌도 있다. 최대한 독립적인 부분을 독립적으로 수행하게 구성하려 했는데 그런 면에서는 성공적이지 못 했다. Reference https://stackoverflow.com/questions/56238356/understanding-esmoduleinterop-in-tsconfig-file https://trustyoo86.github.io/webpack/2018/01/10/webpack-configuration.html https://webpack.js.org/guides/ https://www.npmjs.com/package/swagger-ui-express https://github.com/webpack-contrib/terser-webpack-plugin","link":"/posts/backend/serverside-typescript-setting-02/"},{"title":"Monolithic 서버사이드 타입스크립트 세팅 03","text":"이놈의 Hexo 블로그가 future라는 옵션을 켜면 미래 글까지 보여준다는 걸 처음 알았다. 원래 2편이 12일에 올라가길 바랐던 글인데, 3시간 후라는 이름으로 11일에 올라갔다, 귀찮기 때문에 내리진 않았다. 이 주제로 마지막 글이다. 이번 글에서는 도커라이징을 해서 로컬 개발 환경을 구성하고, 개발 서버, 운영 서버로 나눠서 배포할 수 있는 환경 구성을 해보려고 한다. 도커라이징과 관련된 내용은 과거에 개발 환경과 관련된 글을 썼는데, 그 글에서 조금 더 자세하게 확인할 수 있긴 하다. 지금까지 진행된 내용은 아래와 같다. Express 기본 아키텍처 구성 ESLint &amp; Prettier tsconfig, webpack 설정 Swagger 개발 환경, 운영 환경 분리 도커라이징 위에서 언급한 것처럼 도커와 관련된 자세한 세부 사항들은 그전 글에서 확인하는 편이 낫다. 이번 글에서는 자세한 설명은 생략한다. 도커라이징과 개발, 운영 환경 분리도커라이징, 개발 운영 환경 분리라는 주제는 하나의 주제로 진행된다. (배포까지 하는 글이 아니기 때문에 환경별로 도커라이징까지 하고 끝난다. 도커 배포 글은 Fargate로 배포하기를 참고해보면 좋을 것 같다.) 로컬 개발 환경 구성우선 Dockerfiles라는 폴더를 만든다. 로컬에서 작업할 도커 파일과, 개발 서버에 배포 하는 것과, 운영 환경에서 배포할 도커 파일 이렇게 세 개를 보통은 구성한다. 네이밍에는 항상 고민이 있긴 한데, 서버만 관리하는 입장에서 여러 도커 파일을 만들 때, 메인 애플리케이션 하나만 도커라이징 되는 경우가 제일 많았기 때문에 이름은 별 다른 거 없이 local.Dockerfile, dev.Dockerfile, prod.Dockerfile로 만들어준다. 로컬 환경에서는 도커 내부와, 현재 작업하는 src 디렉토리가 공유되어야 한다. 별로 어려울 게 없으니 일단 구성해보자. 123456# Dockerfiles/local.DockerfileFROM node:12.16.1-alpineWORKDIR /usr/src/appCMD [&quot;yarn&quot;, &quot;docker:dev&quot;] 볼륨 공유와 포트포워딩은 docker-compose.yml에서 설정할 것이다. 다만 위 설정에서 아쉬운 점은, 로컬 개발 환경이 완벽하게 운영, 개발 서버와 같지 않다는 점이다. 완전히 같아지려면, node_modules를 도커 내에서 설치해야 하고, docker-compose.yml에서 node_modules를 공유하지 않는 방식으로 가야 한다. 방법이 있긴 한데 문제는 로컬 개발을 진행하면서 패키지 인스톨이 필요할 때마다 도커 컨테이너 내부에서 yarn add를 해줘야 한다. 적당히 타협해서, 다음과 같이 진행했다. 데이터베이스와 기타 필요한 것들을 편하게 사용하기 위해서 docker-compose를 로컬 개발 환경에서는 사용한다. 프로젝트 최상단에 docker-compose.yml 파일을 만든다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445# docker-compose.ymlversion: &quot;3.7&quot;services: app: container_name: app build: context: . dockerfile: Dockerfiles/local.Dockerfile ports: - &quot;3000:80&quot; volumes: - ./:/app env_file: - .envs/local.env depends_on: - database - pgadmin database: container_name: database image: postgres:10 ports: - &quot;5432:5432&quot; volumes: - db-volume:/var/lib/postgresql/data pgadmin: container_name: pg_admin image: dpage/pgadmin4 logging: driver: none ports: - &quot;5555:80&quot; volumes: - pg-volume:/var/lib/pgadmin depends_on: - database env_file: - .envs/local.envvolumes: db-volume: {} pg-volume: {} 내부 내용들에서 구체적인 부분들은 위에서 언급한 시리즈 2탄에서 자세하게 다룬다. pgadmin을 위해서 필요한 환경 변수는 두 가지이다. 123456789# envs/local.envPGADMIN_DEFAULT_EMAIL=your@email.comPGADMIN_DEFAULT_PASSWORD=1234NODE_ENV=developmentENV=localSECRET=mysecretAPP_PORT=80APP_LOGSTAGE=dev 위는 로컬 환경변수로 필요한 것들이다. 데이터베이스를 붙일 때 추가될 예정이지만, pgadmin을 도커로 돌리게 되면 로그인을 하게 되는데, 로그인할 이메일과 비밀번호를 위 이름으로 설정해줘야 한다. pgadmin을 위한 볼륨을 사용하고 있어서 만약 한 번 저 내용이 설정되고 나면 해당하는 volume 위치를 찾아서 설정을 변경해줘야 한다. 바뀔 일이 없어서 상관 없는데 만약 재설정 하고 싶다면, 그냥 해당 볼륨을 지우고 다시 빌드하는게 맘 편하다. 위 두가지 설정을 해주지 않으면 pgadmin이 켜지지 않는다. app이라는 이름으로 서버를 띄울텐데, context가 최상단에 위치해서, 이미지를 만드는 데 불필요한 부분들까지 모두 context에 복사된다. .dockerignore 파일을 만들어서 해결해주도록 하자. 최상단에 .dockerignore를 만들어준다. (로컬 개발 환경에서는 볼륨이 공유되어서 아래 내용들도 모두 존재하긴 한다. 다만 prod.Dockerfile, dev.Dockerfile에서 빌드될 때 불필요한 부분들을 넣었다.) 123456789# .dockerignore.gitDockefilesnode_modules.eslintrc.json.gitignore.prettierrcdocker-compose.ymlormconfig.json 스크립트도 docker-compose로 시작할 수 있게 바꿔주도록 하자. 파일이 변경되면 재시작되는 nodemon을 설치하고, 타입스크립트를 바로 실행할 수 있는 ts-node와, alias 설정을 반영해주는 tsconfig-paths 패키지를 설치한다. 1yarn add nodemon ts-node tsconfig-paths -D 1234567891011121314// package.json{ //... &quot;scripts&quot;: { &quot;docker:dev&quot;: &quot;nodemon --exec ts-node -r tsconfig-paths/register src/index.ts&quot;, &quot;build:dev&quot;: &quot;rimraf ./dist &amp;&amp; webpack --env.NODE_ENV=development&quot;, &quot;build:prod&quot;: &quot;rimraf ./dist &amp;&amp; webpack --env.NODE_ENV=production&quot;, &quot;dev&quot;: &quot;docker-compose up&quot;, &quot;start&quot;: &quot;node ./dist/app.js&quot;, &quot;lint&quot;: &quot;eslint \\&quot;./src/**/*.ts\\&quot;&quot; } //...} yarn dev 명령어로 실행해볼 수 있다. 로컬 개발 환경이 잘 작동하고, 앱에서는 현재 만들어진 데이터베이스가 없기 때문에 데이터베이스가 없다는 에러가 뜰 수 있다. ORM 별로 다르겠지만, 이 글에서 사용한 TypeORM을 사용하고 있다. pg_admin을 통해 원하는 데이터베이스 이름을 만들어주고 (또는 database 도커 컨테이너 내부로 가서 직접 만들어줘도 된다.) 실행하면 원활하게 실행이 된다. TypeORM은 프로젝트 최상단에 ormconfig.json을 두든, 앱 내부에서 설정 값을 넘겨줘야 한다. ormconfig.json을 사용하면 데이터베이스 설정값을 여러개로 관리해야 하지만, 내부에 넘겨주게 되면, .env에서 해결할 수 있다. 또한 내부에 뒀을 때 생기는 문제점들을 해결해 줄 수 있지도 않다. 따라서 createConnection에 옵션을 넘기는 방식을 사용하겠지만, 내부에서 옵션을 직접 지정하면, entities 옵션을 설정할 때 빌드된 다음 과정을 고민해봐야 한다. 빌드 되면 해당 경로가 무의미 해진다. 따라서 경로로 설정해주는 것이 아니라 직접 모델을 넣어줘야 하는데, 그렇게 되면 minify되는 이름들 때문에 테이블 이름을 온전하게 찾지 못하는 문제가 발생한다. 경로 지정을 해서 생기는 문제와, 직접 모델을 import 해줄 때 생기는 문제 중에 해결하기 쉬운 건 이름이 바뀌는 문제이다. 이 부분도 웹팩의 optimization과 관련이 있다. 클래스 이름이 변경되는 것을 막기 위해서 다음 옵션을 TerserPlugin에 추가한다. 123456789101112131415161718192021222324// ...module.exports = env =&gt; { // ... const terserPlugin = isDev ? new TerserPlugin({ terserOptions: { output: { comments: /@swagger/i }, keep_classnames: true } }) : new TerserPlugin({ terserOptions: { keep_classnames: true } }); return { // ... };}; src/configs/index.ts와 src/loaders/dbLoader.ts에 다음과 같이 설정 값을 추가한다. 1234567891011121314// src/configs/index.ts// ...export default { // ... DB: { HOST: String(process.env.DB_HOST), DATABASE: String(process.env.DB_DATABASE), USERNAME: String(process.env.DB_USERNAME), PASSWORD: String(process.env.DB_PASSWORD) }}; 123456789101112131415161718192021222324// src/loaders/dbLoader.tsimport { createConnection, ConnectionOptions, getConnection } from &quot;typeorm&quot;;import configs from &quot;@/configs&quot;;import { User } from &quot;@/models/userModel&quot;;const defaultOrmConfig: ConnectionOptions = { type: &quot;postgres&quot;, logging: [&quot;warn&quot;, &quot;error&quot;, &quot;migration&quot;], port: 5432, host: configs.DB.HOST, database: configs.DB.DATABASE, username: configs.DB.USERNAME, password: configs.DB.PASSWORD, entities: [User], synchronize: configs.ENV === &quot;local&quot;};const dbLoader = (ormConfig: ConnectionOptions = defaultOrmConfig) =&gt; createConnection(ormConfig);export const clearDatabase = () =&gt; getConnection().close();export default dbLoader; ormConfig를 인자로 받는 함수를 만든 이유는 테스트 함수에서 컨넥트 할 때 같은 함수를 사용하고 싶어서이다. 지금까지 로컬 개발 환경에서 필요한 local.env는 다음과 같다. 123456789101112131415PGADMIN_DEFAULT_EMAIL=your@email.comPGADMIN_DEFAULT_PASSWORD=1234NODE_ENV=developmentENV=localSECRET=mysecretAPP_PORT=80APP_LOGSTAGE=devAPP_HOST=localhost:3000DB_HOST=databaseDB_USERNAME=postgresDB_PASSWORD=postgresDB_DATABASE=database 이제 서버가 잘 돌고 완전히 로컬에서 개발할 준비가 마쳐졌다. 현재 프로젝트는 아래와 같이 생겼다. 이 방식으로 하면, 최상단의 ormconfig.json은 사용하지 않아도 된다. ormconfig.json은 로컬 환경에서만 개발할 때는 사용하기 편하지만, 설정 값이 많아지는 건 귀찮은 일이다. 다만 typeorm cli를 사용하는 사람은 이 옵션이 필요하다. 주로 migration을 하는 일인데, 이 부분은 ormconfig.json의 migrations폴더를 지정하고 타입스크립트로 코드를 돌리도록 하면 해결 되긴 한다. 필자는 운영, 개발 서버는 pgAdmin을 사용해서 migration을 해주는 편이기 때문에 사용하지 않는다. 로컬 환경은 sync를 맞춰주는 걸로 한다. 다만 이 프로젝트에서는 지우지 않고 migrations를 사용하시는 분들을 위해 기본적인 세팅을 해뒀다. 이제 공백으로 남겨뒀던 dev.Dockerfile과 prod.Dockerfile을 작성해보자. 지난 번에 도커 베스트 프랙티스를 번역 했으니, 그걸 좀 활용해보자. 123456789101112131415161718# Dockerfiles/dev.DockerfileFROM node:12.16.1-alpine AS builderWORKDIR /usr/src/appCOPY src .COPY package.json .COPY yarn.lock .RUN yarn installRUN yarn build:devFROM node:12.16.1-alpineCOPY --from=builder /usr/src/app/dist ./distCOPY --from=builder /usr/src/app/node_modules .COPY .envs/dev.env ./.envEXPOSE 80CMD [&quot;yarn&quot;, &quot;start&quot;] 실행 할 때 node_modules를 새로 설치하는게 더 빠를까 COPY 하는 게 더 빠를까 고민을 했는데, 정확하지는 않지만, 이게 더 나을 것 같아서 위와 같이 작성했다. 아래는 prod.Dockerfile이다. 123456789101112131415161718192021222324252627# Dockerfiles/prod.DockerfileFROM node:12.16.1-alpine AS builderWORKDIR /usr/src/appCOPY .envs/prod.env ./.envCOPY webpack.config.js .COPY tsconfig.json .COPY package.json .COPY yarn.lock .COPY src ./srcRUN yarn installRUN yarn build:prodFROM node:12.16.1-alpineCOPY --from=builder /usr/src/app/dist ./distCOPY package.json .COPY yarn.lock .RUN yarn install --productionEXPOSE 80CMD [&quot;yarn&quot;, &quot;start&quot;] 둘 모두 빌드가 잘 된다. 추가: 테스트 코드Jest를 사용한 테스트 코드 사용도 추가해보려고 한다. 아래 필요한 패키지를 추가로 설치한다. 1yarn add jest ts-jest @types/jest -D 그 다음 package.json에 타입스크립트 코드를 테스트할 수 있게 Jest 설정과 스크립트를 추가한다. 123456789101112131415161718192021222324252627282930// package.json{ // ... &quot;scripts&quot;: { // ... &quot;test&quot;: &quot;jest --detectOpenHandles --forceExit&quot; }, // ... &quot;jest&quot;: { &quot;transform&quot;: { &quot;^.+\\\\.ts$&quot;: &quot;ts-jest&quot; }, &quot;testRegex&quot;: &quot;\\\\.test\\\\.ts$&quot;, &quot;collectCoverage&quot;: true, &quot;moduleFileExtensions&quot;: [&quot;ts&quot;, &quot;js&quot;, &quot;json&quot;], &quot;moduleNameMapper&quot;: { &quot;@/(.*)$&quot;: &quot;&lt;rootDir&gt;/src/$1&quot; }, &quot;globals&quot;: { &quot;ts-jest&quot;: { &quot;diagnostics&quot;: true } } }} collectCoverage는 테스트 결과와 함게 커버리지를 커멘드라인으로 띄워 보여주는 옵션이다. moduleNameMapper는 테스트 코드 내에서 @/* 형태로 모듈을 임포트 할 수 있게 지정해주는 옵션이다. --forceExit 옵션은 Jest를 종료 해야 하는 경우, 종료되지 않는 프로세스 때문에 꺼지지 않는 것을 막아주는데 이 옵션을 사용하려면 --detectOpenHandles 옵션이 필요하다. 이 옵션은 Jest가 깔끔하게 종료되지 않게 방해하는 것을 모아서 프린트해준다. 프로젝트 최상단에 tests 디렉토리를 만들고, src프로젝트에서 테스트 해야 할 디렉토리와 동일한 이름으로 만든다. 필자는 보통 models, services를 테스트 하는 편이다. 테스트 되어야 하는 파일에 .test.ts 확장자로 테스트 파일을 만든다. tsconfig의 include 부분에도 tests/**/*.ts를 추가한다. 테스트를 할 때 데이터베이스가 필요한 경우가 있다보니, 어떻게 해야 할지 고민을 하게 됐는데, 마찬가지로 docker-compose.test.yml을 만들어서 전체 테스트를 실행하게 할려 했다. 실제로 해봤는데 생각보다 너무 느려서, localhost:5432에 붙는 데이터베이스(yarn dev 했을 때 뜨는 데이터베이스도 localhost:5432에 포트포워딩 되어있다.)에다가 testdb라는 데이터베이스를 만들어서 진행했다. 이게 훨씬 빨라서 로컬 테스트는 그렇게 진행하기로 했다. 아래는 userService.test.ts와 userModel.test.ts이다. 1234567891011121314151617181920212223242526272829303132// tests/services/userService.test.tsimport UserModel, { User } from &quot;@/models/userModel&quot;;import { DeepPartial } from &quot;typeorm&quot;;import UserService from &quot;@/services/userService&quot;;const userModel: Partial&lt;UserModel&gt; = { createOne: ({ username }: DeepPartial&lt;User&gt;): Promise&lt;Mutation&lt;User&gt;&gt; =&gt; { const date = new Date(); const userMock = { id: 1, username, createdAt: date, updatedAt: date }; return new Promise(resolve =&gt; resolve({ result: userMock as User, success: true }) ); }};describe(&quot;UserService Test&quot;, () =&gt; { describe(&quot;Signup&quot;, () =&gt; { test(&quot;username을 넣으면 회원가입이 된다.&quot;, async () =&gt; { const userService = new UserService(userModel as UserModel); const username = &quot;testUser&quot;; const ret = await userService.signup(username); expect(ret.success).toBe(true); expect(ret.result?.username).toBe(username); }); });}); 1234567891011121314151617181920212223242526// tests/models/userModels.test.tsimport UserModel from &quot;@/models/userModel&quot;;import dbLoader, { clearDatabase } from &quot;@/loaders/dbLoader&quot;;import ormConfig from &quot;../ormConfig&quot;;beforeAll(async () =&gt; { await dbLoader(ormConfig);});afterAll(async () =&gt; { await clearDatabase();});describe(&quot;UserModel Test&quot;, () =&gt; { describe(&quot;createOne&quot;, () =&gt; { test(&quot;username이 있으면 데이터베이스에 저장된다.&quot;, async () =&gt; { const userModel = new UserModel(); const username = &quot;testUser&quot;; const ret = await userModel.createOne({ username }); expect(ret.success).toBe(true); expect(ret.result?.username).toBe(username); }); });}); 아래는 테스트에서 사용되는 ormConfig이다. 123456789101112131415161718// tests/ormConfig.ts;import { ConnectionOptions } from &quot;typeorm&quot;;import { User } from &quot;@/models/userModel&quot;;const ormConfig: ConnectionOptions = { type: &quot;postgres&quot;, logging: [&quot;warn&quot;, &quot;error&quot;, &quot;migration&quot;], port: 5432, host: &quot;localhost&quot;, database: &quot;testdb&quot;, username: &quot;postgres&quot;, password: &quot;postgres&quot;, entities: [User], synchronize: true};export default ormConfig; 테스트를 돌려보면 다음과 같이 리포트를 뽑아준다. 프로젝트 최상단에 coverage라는 리포트도 웹 형태로 빌드해준다. TADA~ 자동으로 만들어주던 보고서 후기정말 긴 시리즈였다. 중간 중간 추가해가면서 진행해서 글을 다듬는 것도 많았다. 잘 읽히게 쓰여진 건지 잘 모르겠지만, 아무튼 템플릿 프로젝트를 하나 완성했다. 이제 타입스크립트 보일러 플레이트는 이걸 계속 업데이트해가면서 쓸 계획이다. Reference https://jestjs.io/docs/en/cli.html https://yonghyunlee.gitlab.io/temp_post/jest/ https://hub.docker.com/_/postgres https://github.com/Radu-Raicea/Dockerized-Flask/wiki/%5BDocker%5D-Access-the-PostgreSQL-command-line-terminal-through-Docker https://www.freecodecamp.org/news/get-your-npm-package-covered-with-jest-and-codecov-9a4cb22b93f4/","link":"/posts/backend/serverside-typescript-setting-03/"},{"title":"도커 베스트 프랙티스 (번역)","text":"깃헙에는 현재 백만이 넘는 도커파일들이 있다. 하지만 모든 도커파일들이 동일하지는 않다. 효율성은 중요한 것이고, 이번 블로그 시리즈는 도커파일의 다섯 가지 영역: 빌드 시간의 증가, 이미지의 사이즈, 유지성, 보안, 반복성에 있어서 베스트 프랙티스를 다룰 것이다. 만약 독자가 도커에 막 시작하는 사람이라면, 이 글은 독자를 위한 글이다! 이후 시리즈는 좀 더 Advanced한 주제가 될 것이다. Important note: 아래의 이 팁들은 메이븐을 기반으로한 자바 프로젝트의 도커파일을 향상시키는 일련의 과정이다. 마지막 도커파일이 결국에 추천하는 도커파일이 될 것이고, 모든 중간 과정들은, 구체적인 베스트 프랙티스를 설명하기 위한 과정이다. Incremental build time개발 사이클에서, 코드를 변경시키고, 다시 빌드 하면서 도커 이미지를 빌드할 때, 캐시를 활용하는 것은 중요하다. 캐싱은 불필요한 빌드 과정을 동작하는 것을 피하게 만든다. Tip #1: 캐싱에는 순서가 중요하다. 빌드 단계 (도커파일의 Instructions)의 순서는 중요한데, 왜냐하면 하나의 스탭의 캐시가, 파일이 바뀌거나, 도커파일의 라인을 수정함으로써 유효하지 않게 되었을 때, 이어지는 단계들의 캐시가 깨질 것이기 때문이다. 너의 Instruction 순서를 최소로 수정되는 것에서 자주 수정되는 순서대로 정렬해야 캐싱을 최적화 할 수 있다. Tip #2: 캐시가 깨지는 것을 제한하기 위해서 더 구체적인 COPY를 사용해라 꼭 필요한 것만 카피해라. 되도록 COPY .와 같은 인스트럭션은 피해라. 너의 이미지에 파일들을 카피해 넣을 때, 너가 아주 구체적으로 너가 원하는 카피 대상을 지정하도록 해라. 카피한 어떠한 파일의 변경점은 캐시를 무효화 한다. 위 예시에서, 오직 미리 빌드된 jar 앱만 이미지 내에서 필요하다. 그러므로 딱 그것만 카피해라. 이 방법으로 연관 없는 파일의 변경점은 캐시에 영향을 주지 않게 된다. Tip #3: apt-get update &amp; install과 같은 캐싱 가능한 단위를 일원화 해라 각 RUN 인스트럭션은 캐싱 가능한 실행의 단위라고 볼 수 있다. 너무 많은 체이닝으로 명령을 하는 것은 캐시를 무효화(breaking)할 수 있기 때문에 조심해야 하지만, 너무 많이 있는 것은 불필요하다. 패키지를 패키지 매니저에서 설치할 때는, 너는 항상 같은 RUN 인스트럭션에서 인덱스를 업데이트하고 그리고 패키지를 설치하고 싶어한다: 그것들은 함께 하나의 캐싱 가능한 유닛을 형성한다. 이 방법을 사용하지 않으면, 너는 outdated된 패키지를 설치할 위험이 있다. Reduce image size작은 이미지들은 빠른 배포와 더 작은 공격 표면(attack surface?)과 이어지기 때문에, 이미지 사이즈는 중요하다. Tip #4: 불필요한 의존성들을 제거해라 불필요한 의존성을 제거하고 필요한 디버깅 툴이 나중에 설치될 수도 있는 것이면, 디버깅 툴을 설치하지 말아라. 특별한 패키지 매니저(apt와 같은)는 자동적으로 user-specified 패키지들을 불필요한 footprint를 남기면서 설치한다. Apt는 실제로는 불필요한 의존성을 설치하지 않는 --no-install-recommends 플래그가 있다. 그것(해당 옵션으로 인해 설치되지 않은)들이 필요한 경우 명시적으로 그것(필요한 의존성)을 더해라 Tip #5: 패키지 매니저의 캐시를 삭제해라 패키지 매니저들은 그들 스스로의 캐시를 보관한다. 그것들은 결국엔 이미지가 된다. 이것을 해결하는 방법은 패키지를 설치한 같은 RUN 인스트럭션에서 캐시를 삭제하는 것이다. 이것을 다른 RUN 인스트럭션에서 지우게 되면, 이미지 사이즈를 줄이지 못한다. 멀티 스테이지 빌드에서 이미지 사이즈를 줄이는 몇 가지 방법들을 마지막에 다룰 것이다. 다음 베스트 프랙티스는 유지성(maintainability), 보안성, 반복 가능성(repeatability)를 어떻게 최적화 할지에 대해서 다룬다. MaintainabilityTip #6: 가능한 공식 이미지들을 사용해라 공식 이미지들은 유지에 많은 시간 소비를 하는 것을 막아준다. 모든 인스톨 과정이 완료 되어있고, 베스트 프랙티스가 적용되어 있기 때문이다. 다양한 프로젝트들이 있다면, 그들이 정확히 같은 베이스 이미지를 사용하기 때문에 그들은 그들의 레이어들을 공유할 수 있다. Tip #7: 더 구체적인 태그들을 사용해라 latest 태그를 사용하지 말아라. 이것은 도커 허브에 있는 공식 이미지들에게 항상 사용가능한 편리한 방법이지만, 가끔은 큰 변화들이 있을 수 있다. 얼마나 오랜 시간을 해당 베이스 이미지를 가지고 개발 했는지에 따라서 도커파일을 캐시 없이 다시 빌드 시켰을 때, 빌드 되지 않을 수 있다. 대신 더 구체적인 태그들을 사용해라. 위 경우는 openjdk를 사용하고 있다. 더 많은 태그들을 사용할 수 있으니 도커 허브 문서를 확인해라 Tip #8: 최소한의 flavors(지원기능)를 찾아라 이 태그들 중 어떤 것들은 최소한의 flavors를 가지고 있는데, 그럴수록 이미지는 더 작다. jre-slim 버전은 최소한의 데비안 이미지를 베이스로 하는데, 더 작은 jre-alpine 버전은 알파인 리눅스 배포판을 기반으로 만들어져 있다. 눈에 띄는 차이점은 데비안은 GNU libc를 사용하는 반면, 알파인은 더 작지만, 몇 가지 성능 이슈들을 야기할 수 있는 mucl libc를 사용한다. 이번 케이스인 openjdk의 경우, (아마 알파인이) sdk가 아니라 java 런타임만 담고 있기 때문에 이것은 극적으로 이미지를 줄일 수 있게 만들어준다. Reproducibility지금까지 도커파일들은 너의 jar artifact가 호스트에서 빌드된다고 가정하고 있었다. 이건 너가 컨테이너가 제공해주는 일관적인 환경의 혜택을 잃는다는 점에서 이상적이지는 않다. 예를 들어서 만약 너의 자바 앱이 특정 라이브러리들에 의존하고 있다면, 그것은 아마 어떤 컴퓨터에서 앱이 빌드되는지에 따른 일관적이지 않게 되는 것들을 환영하지 않을 것이다. Tip #9: 일관적인 환경에서 소스를 빌드해라소스 코드는 너가 빌드하고자 하는 도커 이미지의 source of truth이다. 그 도커파일은 간단히 말해 청사진이다. 너는 너의 앱을 빌드하기 위해 필요한 모든 것들을 일원화 하는 것으로서 시작할 필요가 있다. 우리의 간단한 자바 앱은 Maven과 JDK를 필요로 한다. 그러므로 우리의 도커파일을 구체적이고 최소화된 공식 maven 이미지 기반으로 하도록 하자. 만약 너가 더 많은 의존성을 설치해야 한다면, RUN 인스트럭션을 통해 추가할 수 있다. pom.xml과 소스 폴더들은 마지막 mvn package와 함께 app.jar을 생성하는 RUN 단계에서 필요하기 때문에 복사된다. (-e 플래그는 에러를 보여주기 위한 것이고, -B는 batch 모드로 동작시키기 위한 것이다.) 우리는 비일관적인 환경 문제를 해결했다. 하지만 다른 한 가지 방법이 있다: 코드가 바뀔 때마다, 모든 pom.xml에 정의된 모든 의존성들이 받아와진다. 다음 팁에서 고쳐보자. Tip #10: 의존성을 각 분리된 단계에서 가져오자 위에서 언급했던 ‘캐시 가능한 실행의 단위’들이라는 것을 다시 생각해봄으로써, 우리는 의존성을 받아오는 것이 소스 코드에 의존적인 것이 아니라 pom.xml이 변경되는 것에만 의존하는 분리된 캐시 가능한 유닛이라는 것을 확신할 수가 있다. 두 COPY 단계 사이에 RUN 단계에서 Maven에게 의존성을 가져오라고 지시한다. 일관된 환경속에서 빌딩하는 것에 대해 소개된 내용 중에 한 가지 문제가 더 있다: 이미지가 런타임에는 필요하지 않은 빌드 타임의 모든 의존성을 포함하고 있기 때문에, 우리 이미지가 이전보다 더 커진다. Tip #11: 빌드 의존성을 제거하기 위해 멀티 스테이지 빌드를 사용해라 (최종 버전) 멀티 스테이지 빌드는 여러 개의 FROM 인스트럭션에서 확인할 수 있다. 각 FROM은 새로운 스테이지를 시작한다. 그들은 AS라는 키워드(위 이미지에서 우리 첫 FROM 스테이지를 나중의 참조를 위해 builder라고 이름 붙인)로 이름 붙여질 수 있다. 두 번째 스테이지는 최종 이미지를 내보낼 우리의 최종적 스테이지이다. 이것은 제한적으로 런타임에 필요로 하는 것을 담을 것이다. 이 경우에는 최소한의 알파인 베이스의 JRE를 담고 있다. 중간적인 빌드 스테이지는 캐시될 것이지만, 최종 이미지에 제공되지는 않는다. artifacts를 최종 이미지에 넣기 위해서 COPY --from=STAGE_NAME을 사용한다. 위 이미지에서는 builder가 되겠다. 멀티 스테이지 빌드는 빌드 타임의 의존성을 제거하는 해결책이 된다. 우리는 빵빵하고, 일관적이지 않게 빌드 하는 것에서 최소한의 이미지를 일관적인 환경에서, 캐시 친화적으로 빌드하는 것으로 왔다. 다음 블로그 포스트에서 우리는 멀티 스테이지 빌드에 대해서 더 자세하게 다룰 예정이다. 후기정말 좋은 꿀팁들을 많이 얻었다. 초보를 위한 글이라고 도입에서 말한 것처럼 초보에게 정말 많은 도움이 되었다. 글이 순서대로, 쉽게 잘 읽혀서 타입스크립트에서도 바로 적용할 수 있을 것 같다. 도커 블로그 자주 애용해야겠다. 읽고 이해하기가 어렵지 않았는데 한글로 이해하기 쉽게 글을 고치는 것이 너무 익숙하지 않아 글이 훼손된 느낌이다. 여러번 해보다 보면 나아지겠거나 하고 있다. 원본https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/?fbclid=IwAR2IvfL95fHkckX1Hq9F0ARfOCbSjjpMgu7nPwNRsf3S8kz1QAvZfrXW3D8","link":"/posts/docker/Docker-best-practices/"},{"title":"복잡한 애플리케이션 설정을 위한 복수의 도커 파일 사용하기(번역)","text":"대부분의 웹 기반의 앱은 실제 서비스에서 개발자가 높은 트래픽에서 증가하는 워크로드를 처리할 능력을 더할 수 있게 하는 여러 컨테이너들을 사용하게 된다. 여러 도커파일을 사용하는 것은 개발자에게 어떻게 하나의 애플리케이션을 분할할 것인지를 결정하는 것을 돕고, 그러므로서 기능적인(함수적인) 집계가 동작할 수 있게 하지만, 개발자는 결정을 내리는 과정 중에 반드시 먼저 스스로에게 몇가지 질문들을 해야 한다. 개발자는 아마도 복수의 도커파일을 로드밸런서 앞단에서 동작하는 프론트앤드에서 작동하는 노드를 유저의 요청에 맞추기 위해 사용할 것이다. 하지만 개발자는 어떤 앱이 도커파일을 통해 이득을 얻을 수 있을지, 얼마나 많은 도커파일들이 각 애플리케이션에 필요한지, 그리고 개발자가 도커파일을 추적하는 과정을 취할 수 있는지와 같은 반드시 몇 요인들을 결정해야 한다. 애플리케이션을 도커파일들로 설정하기 위한 고려사항들분산 애플리케이션을 만들고 관리하는 것은 복잡한 과정이 될 수 있다. 하지만 도커파일과 Github과 같은 툴을 사용하면 최대한 간단한 설정을 유지할 수 있다. 도커파일을 사용하기 앞서, 아래 질문들을 확인해보자. 어떤 타입의 애플리케이션들이 복수의 도커파일이 이득일까?어떤 유저와 마주하고 있는 프론트, 논리적 처리와 어떠한 종류의 스토리지를 가지고 있는 백앤드가 있는 어떤 종류의 앱이라면, 복수의 도커파일로 이익을 볼 수 있다. Github에서 이용가능한 도커 투표앱 샘플은 이러한 아키텍처의 한 예시이다. 이 앱은 파이썬, 노드, Redis, PostgreSQL, .NET 애플리케이션을 사용하고 있다. 만약 개발자가 이 앱을 로컬 환경에서 테스트하기로 결정했다면, 개발자는 반드시 도커로 세팅된 애플리케이션으로부터 드라이브릉 공유할 수 있게 해야 한다. (역주: 이 글을 읽으면서 직접 샘플을 테스트 하려면, drive sharing이 필요하다고 하는 것 같습니다.) 이 앱에서 각 노드들은 각각의 커스텀 설정 도커 파일로부터 온다. 어떤 경우에서는, 개발자는 특정한 태스크들을 처리할 수 있는 실행 선택지, 예를 들어 프론트앤드 웹 애플리케이션을 위한 파이썬 또는 ASP.NET Core과 같은 선택지를 갖게 될 것이다. 개발자는 샘플 애플리케이션을 실행할 다섯 가지 다양한 기능적인 노드들 전체와, 각 노드들이 동작 가능하게 하는 도커파일들을 갖는다. (역주: 아마 저 샘플 앱을 까봤을 때 어떤 구성인지 설명하는 건데, 해석을 하다보니 좀 딱딱합니다.) 개발자는 어떻게 한 서비스가 그 서비스만의 도커파일이 필요한지 알 수 있을까?도커 투표앱 예시로부터, 개발자는 상글 노드가 될 수 잇는 두 프론트 앱과, 하나의 캐시, 하나의 데이터베이스 노드, 그리고 하나의 워커 노드를 갖게 된다. 애플리케이션을 기능적 영역까지 쪼개는 것은 각 노드에게 다양한 도커파일 옵션들을 제공하는 것을 쉽게 만든다. (역주: 즉, 기능적 단위까지 애플리케이션을 쪼개고, 기능적 단위마다 도커파일을 주는 것이 좋다고 말하는 듯) 하나의 애플리케이션당 얼마나 많은 도커파일들이 필요하게 될까?개발자는 하나의 독립적인 서비스 또는 함수당 하나의 도커파일을 가지고 있어야 한다. 예를 들어서 도커 투표 앱에서 개발자는 각 5개의 기능적인 노드들에 대한 하나의 도커파일을 찾을 수 있다. 각 서비스들은 다양한 실행 방법을 위한 복수의 도커파일들을 가지고 있다. 워커 노드는 ASP.NET 버전, 그리고 자바 실행을 위한 구체적인 명령들과 함께 두 개의 도커파일이 있다. (역주: 노드가 ASP.NET에서 실행될 경우, JAVA에서 실행될 경우 두 가지로 구분된 명령어를 담고 있는 도커파일 두 개가 있다는 뜻) 두 개 모두 구체적인 서비스들에 집중된 최소의 명령어들만 담고 있다. 어떻게 개발자가 하나의 앱을 복수의 도커파일들로 컨테이너화 할 수 있을까?도커 컴포즈는 다양한 도커파일들을 사용해 애플리케이션을 빌드하는 가장 일반적인 방법이다. 이것은 컨테이너를 만들기 위해 몇가지 명령어들에 기반한 YAML 파일을 요구한다. 도커 도큐맨트 사이트는 도커 투표 앱을 위한 컴포즈 파일의 리스트를 가지고 있다. 각 기능적 서비스들은 구분된 애플리케이션을 빌드하기 위해 반드시 필요한 모든 것을 갖추고 있는 섹션을 갖고있다. 도커 컴포즈에서 dockerfile 키워드는 개발자에게 다양한 파일 이름을 지명 하는 것을 가능하게 한다. 어떻게 과도한 리소스 소비를 피할 수 있을까?리소스 제한 또는 할당량은 컨테이너 아키텍처에서 가장 최상위의 사용 리소스를 제한하는 방법이다. 도커와 함께, 개발자는 --memory-swap 그리고 kernel-memory와 같은 커멘드라인 옵션으로 물리적 메모리 한계를 강제할 수 있다. CPU 사용도 비슷한 방법으로 제한 가능하다. 쿠버네티스는 또한 몇몇 정의들과 limits 키워드를 가지고 있는 YAML 파일과 함께 리소스 소비의 제한을 이름공간 기반으로 서포트 하고 있다. 쿠버네티스에서 름공간(namespace)은 개발자가 개별 리소스들을 적절한 관리와 보안 통제들과 함께 프로비전 또는 인스턴스화 할 수 있는 프로그래밍 언어에서의 특징과 유사하다. 그리고 이것은 파이썬에서 가상 워크스페이스와도 어쩌면 유사할 수 있다. 번역 후기주제가 굉장히 자극적인 주제라 언젠가는 읽어봐야지 하고 있던 글인데, 막상 읽어보니 구체적이지 않고 디테일한 부분이 없어서 많이 아쉬웠다. 마지막 문단과 마무리 글은 형식적이고, 무의미 한 내용이라고 생각 되어서 아예 빼버렸다. 그래도 내용 중에 한 두 문장으로 정리된 위에 내용들 중에서는 당연스럽게도 항상 고려해봐야 하는 내용들이 있었다고 생각한다. 그렇지만 결과적으로 이번 번역 주제는 잘 못 골랐다. 다음은 Best Practice, Bad Practice에 대한 글을 번역할 생각인데, 도커 블로그에 올라온 글이라 기대가 되는 바이다. Reference https://searchservervirtualization.techtarget.com/tip/Use-multiple-Dockerfiles-for-complex-application-configuration?fbclid=IwAR0Q-0BPSwv0LysLt2Bk_Le6GJ4ewQtzCDs7hMbj-Rcn4EN7s02Zn14H0YU","link":"/posts/docker/Use-multiple-Dockerfiles-for-complex-application-configuration/"},{"title":"Docker로 개발 환경 구축하기 (1)","text":"현재 개발 동아리 팀에 다양한 OS로 인해 생기는 여러 문제들을 Docker로 힙하게 해결 하고자 Docker를 이용해 개발 환경을 구축 했다. 이후 동아리에서 Docker hands on 세션을 진행 하기 위해, 내용을 정리해 보려고 한다. 목표는 docker compose를 활용한 일반적인 백엔드 개발 환경을 구성해보는 것이고, 이번 편은 docker image까지 사용하는 방법을 확인해볼 예정이다. Docker는 무엇일까?도커는 2013년 솔로몬 하익스라는 사람이 최초로 오픈 소스로 공개했다고 한다. 도커의 역사나 기타 배경적 이야기는 제쳐두고, 도커의 기본 개념을 살펴보자. 도커는 컨테이너 기반 오픈 소스 가상화 플랫폼이다. 컨테이너 기반이라는 것은 컨테이너형 가상화 기술을 사용 한다는 뜻이다. 기존의 호스트 안에 게스트 OS를 가상화 하는 것은 운영 체제 가상화라고 하는데, 운영 체제 가상화를 위해서는 가상화 소프트웨어를 사용 해야만 하고 OS 전체를 가상화한다. 컨테이너 가상화 기술은 이와 비교했을 때 오버헤드가 작아 운영 환경에서도 사용 가능하고, 개발 환경에서도 사용 가능하다. 가벼움 뿐 아니라 도커는 실행 환경을 컨테이너로 추상화하고 동일한 인터페이스를 제공해 프로그램의 배포 및 관리를 단순하게 한다. 도커는 컨테이너형 가상화를 위한 상주 앱(도커 엔진), 이를 관리하기 위한 명령형 도구로 이루어져 있다. 아래 이미지에 나타난 것처럼 컨테이너를 올릴 도커 엔진과, 이들을 관리하기 위한 명령어들로 구성되어 있다는 것이다. Docker는 왜 사용할까? 실행 환경이 변하지 않는다. 코드를 통해 실행 환경을 만들 수 있다. 실행 환경이 곧 앱이 되기 때문에 배포가 간단해진다. (컨테이너 자체를 바꿔서 버전을 업데이트 할 수 있다.) 도커를 왜 사용해야 하는가에 대한 글은 이 링크에서 확인하면 더 좋을 것 같다. Docker는 어떻게 쓰는 걸까?나는 이미 도커를 쓰겠다고 마음을 먹었으니 앞선 주제는 이 정도 선에서 마무리 짓고, 도커를 어떻게 쓸지를 정리하도록 하겠다. 도커는 기본적으로 이미지를 다루는 섹션과 컨테이너를 다루는 섹션으로 구분 지을 수 있다. 쉽게 말하자면 이미지는 컨테이너를 정의하는 것이라고 볼 수 있고, 이에 따라 컨테이너가 생성된다. 이제 본격적으로 이미지와 컨테이너를 다룰 건데, 이번 글에서는 먼저 이미지에 대한 내용을 공부하고, 다음 컨테이너를 다뤄 보도록 하겠다. Docker Help와 Search먼저 docker의 help 명령어와 search 명령어를 보고 넘어가자. docker help도커 명령어들은 하위 명령을 가지고 있는 박스처럼 구성되어 있어서, help 명령어로 내려가면서 명령들을 찾아볼 수 있다. 하위 명령을 찾는 방법은 다음처럼 어떤 명령어 뒤에 --help를 붙여서 확인할 수 있다. docker image --help docker search도커 허브에 등록된 이미지들을 찾아볼 수 있다. --limit 명령어로 검색 건수를 제한할 수도 있다. docker search --limit 5 ubunut (--limit을 뒤에 붙여도 됨) Docker 이미지Docker 이미지는 컨테이너 실행에 필요한 파일과 설정 값 등을 포함하고 있는 것이다. 이미지는 Dockerfile을 통해서 빌드 할 수 있고, 빌드 된 이미지는 기본적으로 불변이다. 이미지는 Dockerfile이 변경되면 새롭게 빌드할 수 있고, 개인적으로는 이때 이미지는 변경된다기 보다는 대체된다는 느낌이다. 1Dockerfile -&gt; Docker Image -&gt; Docker Container Dockerfile은 우리가 필요한 이미지가 어떤 형태, 상태인지를 정의한 파일이다. 이 파일을 기반으로 이미지를 빌드할 수 있다. 여러 Instruction이 있는데 이후 정리해보도록 하고, 우선 가장 기본적인 Instruction을 정리해보자 Instructuon 설명 FROM 도커 이미지의 바탕이 될 베이스 이미지를 지정한다. 이 이미지는 Docker hub라는 레지스트리에 공개된 것이어야 한다. RUN 이미지를 실행할 때 컨테이너 안에서 실행할 명령어를 정의한다. COPY 호스트의 파일 또는 디렉토리를 도커 컨테이너 안으로 복사한다. CMD 도커를 실행할 때 컨테이너 안에서 실행할 명령을 정의한다. RUN과 다르게 RUN은 이미지를 빌드할 때 실행되고, CMD는 컨테이너를 실행하면 한 번만 실행된다. 이제 지금 배운 걸로 docker image를 만들어보자. 아래와 같은 코드가 있고, Dependencies도 모두 설치 된 상태라고 가정하자. 123456789101112// app.jsconst app = require(&quot;express&quot;)();app.use(logger(&quot;dev&quot;));app.get(&quot;&quot;, (req, res) =&gt; { res.send(&quot;&lt;h1&gt;Hello world docker!&lt;/h1&gt;&quot;);});app.listen(3000, () =&gt; { console.log(&quot;Server is on 3000&quot;);}); 이런 자바스크립트 서버를 도커 위에서 동작하게 하려면 다음과 같은 Dockerfile이 필요하다. 123456789# DockerfileFROM node:latestCOPY . .RUN npm iCMD [&quot;npm&quot;, &quot;start&quot;] 위 Instruction을 확인해보면 위 Dockerfile은 node 최신 버전의 이미지를 베이스로, 현재 폴더를 도커 컨테이너의 현재 폴더에 복사 해 넣고, npm i를 수행한 상태의 컨테이너 이미지를 빌드하게 해준다. 이제 그럼 정말로 이미지를 만들어보자. docker image builddocker image build를 사용하면 도커 이미지가 빌드된다. 정확한 사용 방법은 아래와 같다. docker image build &lt;Dockerfile 위치&gt; docker image build . 명령어로 이미지를 빌드 한 다음 docker image ls를 사용해서 빌드된 이미지를 확인할 수 있다. 이 명령어는 빌드된 이미지의 목록과 Dockerfile의 FROM Instruction에 의해 pull 받아온 이미지들까지 보여주는 명령어이다. 123REPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 14c7411cfdf3 4 seconds ago 935MBnode latest 1a77bcb355eb 5 days ago 933MB node의 이미지와 다르게 방금 우리가 만든 이미지의 REPOSITORY와 TAG는 &lt;none&gt;이다. 이미지의 이름과 태그라고 하는데 이를 지정해 주어야 나타난다. 빌드할 때 이미지에 이름과 태그를 붙이려면 -t 옵션을 붙여서 사용해야 한다. docker image build -t 이름[:태그] &lt;Dockerfile 위치&gt; 태그는 없어도 되는데, 없는 경우 latest로 자동 설정 된다. 아래 명령어를 다시 실행한 다음 목록을 확인해 보면, none 이미지의 태그와 이름이 붙었다. docker image build -t changhoi/hands-on:latest . 123REPOSITORY TAG IMAGE ID CREATED SIZEchanghoi/hands-on latest 14c7411cfdf3 45 seconds ago 935MBnode latest 1a77bcb355eb 5 days ago 933MB IMAGE ID를 통해서 &lt;none&gt;과 changhoi/hands-on이 같은 이미지라는 것을 알 수 있는데, Dockerfile의 변경점이 없어서 대체된 것으로 보인다. Dockerfile을 기반으로 빌드를 할 때 도커는 캐시를 사용한다. 예를 들어서 아무 변경 사항 없이 다시 docker image build . 명령어를 사용하면 아래처럼 콘솔에 찍히는 것을 볼 수 있다. 12345678910111213Sending build context to Docker daemon 2.168MBStep 1/4 : FROM node:latest ---&gt; 1a77bcb355ebStep 2/4 : COPY . . ---&gt; Using cache ---&gt; 8fd65e34f927Step 3/4 : RUN npm i ---&gt; Using cache ---&gt; 1001c43e9d1cStep 4/4 : CMD [&quot;npm&quot;, &quot;start&quot;] ---&gt; Using cache ---&gt; 071dd8f024c6Successfully built 071dd8f024c6 캐시를 사용하는 것으로 인해서 원래는 &lt;none&gt;이 생성되어야 하는데, 기존에 이름과 태그가 그대로 붙어 있는 것을 확인할 수 있다. 첫 번째 step은 FROM Instruction을 실행하는데, 캐시를 사용한다는 말은 없지만, 지정한 이미지가 호스트에 남아있다면 그 이미지를 그대로 가져다 쓴다. 위 두 가지 상황(FROM을 별도 pull없이 가져옴, 명령어를 캐시해서 사용함)을 캐시 없이 빌드 하도록 강제할 수 있는데 아래 옵션을 추가하면 된다. docker image build --pull=true: FROM에 있는 베이스 이미지를 새로 pull 받도록 한다. docker image build --no-cache: 다른 Instruction을 캐시 없이 진행한다. 아래 명령어를 다시 실행해보고 docker image ls를 해 봤다. docker image build -t changhoi/hands-on --no-cache --pull=true . 1234REPOSITORY TAG IMAGE ID CREATED SIZEchanghoi/hands-on latest d320649c3efe 10 seconds ago 935MB&lt;none&gt; &lt;none&gt; 14c7411cfdf3 16 minutes ago 935MBnode latest 1a77bcb355eb 5 days ago 933MB 아까 이미지는 &lt;none&gt;으로 바뀌고 새로운 latest가 생긴 것을 확인할 수 있다. 이것은 이름:태그가 고유하기 때문이다. 이름은 중복 가능하지만, 같은 이름의 같은 태그는 존재하지 않는다. docker image tag&lt;none&gt;이라고 붙은 이미지에 새롭게 태그를 붙여주자. 태그 명령어는 아래와 같이 사용할 수 있다. docker image tag 원래이름:태그 새로운이름:태그 docker image tag 구분가능한IMAGEID 새로운이름:태그 구분 가능한 IMAGEID라는 것은 ID를 모두 치지 않더라도 유일해질 정도만 쳐도 괜찮다는 뜻이다. 아래 명령어로 태그를 붙인 다음, 목록을 확인해 보면 구분 가능한 ID에 새롭게 이름과 태그가 붙는 것을 확인할 수 있다. docker image tag 14c changhoi/hands-on:old 1234REPOSITORY TAG IMAGE ID CREATED SIZEchanghoi/hands-on latest d320649c3efe 3 minutes ago 935MBchanghoi/hands-on old 14c7411cfdf3 5 minutes ago 935MBnode latest 1a77bcb355eb 5 days ago 933MB docker image rm만든 이미지는 당연히 지울 수도 있는데, 아래 명령어로 이미지를 삭제할 수 있다. 다만, 지우려는 이미지가 다른 이미지의 베이스 이미지면 안된다. docker image rm changhoi/hands-on:old 위 명령어를 실행한 뒤 목록을 확인해 보면 삭제된 것을 알 수 있다. 123REPOSITORY TAG IMAGE ID CREATED SIZEchanghoi/hands-on latest d320649c3efe 45 minutes ago 935MBnode latest 1a77bcb355eb 5 days ago 933MB ETC docker image pull: Docker hub에 있는 특정한 이미지를 Pull 받을 수 있다. AppendixDockerfile Instruction Instruction 설명 FROM 도커 이미지의 바탕이 될 베이스 이미지를 지정한다. 이 이미지는 Docker hub라는 레지스트리에 공개된 것이어야 한다. RUN 이미지를 실행할 때 컨테이너 안에서 실행할 명령어를 정의한다. COPY 호스트의 파일 또는 디렉토리를 도커 컨테이너 안으로 복사한다. CMD 도커를 실행할 때 컨테이너 안에서 실행할 명령을 정의한다. RUN과 다르게 RUN은 이미지를 빌드할 때 실행되고, CMD는 컨테이너를 실행하면 한 번만 실행된다. ADD COPY + 압축 파일 해제, URL로부터 컨테이너 파일 및 디렉토리 추가 ARG docker image build 실행할 때 사용하는 변수 ENTRYPOINT 컨테이너를 실행 가능 파일로 사용할 때 정의하는 명령 ENV 컨테이너 안에서 사용하는 환경 변수 EXPOSE 컨테이너가 노출하는 포르 설정 HEALTHCHECK 명령을 실행한 다음 결과를 헬스 체크에 사용한다 LABEL 이미지에 추가되는 메타 데이터 ONBUILD 컨테이너 안에서 실행되는 명령을 정의함. 이미지에서는 실행되지 않음 STOPSIGNAL 컨테이너에 전달되면 컨테이너를 종료 USER 컨테이너 사용자. USER Instruction 이후에 나오는 RUN Instruction도 해당 사용자 권한으로 실행된다. VOLUME 호스트나 다른 컨테이너에서 볼륨을 마운트 WORKDIR 컨테이너의 작업 디렉토리 Reference Book: 도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문 https://github.com/voyagerwoo/docker-hands-on https://www.44bits.io/ko/post/why-should-i-use-docker-container https://www.44bits.io/ko/post/almost-perfect-development-environment-with-docker-and-docker-compose?fbclid=IwAR3GPrNSySCj4k-4qu3pL-fRq29uBQ8RR6MSfviibHZnmYj-BQiV_G9e34U","link":"/posts/docker/docker-development-env-(1)/"},{"title":"Docker로 개발 환경 구축하기 (2)","text":"그 전 글은 여기에서 볼 수 있다. 이제 우리는 Docker 이미지를 다룰 수 있다. 이미지를 다루는 방법을 잘 모르는 상태라면 이전 글을 읽고 오는 편이 좋지만, 알고 있다면 더 준비할 내용 없이 바로 이 글을 읽어도 좋을 것 같다. 컨테이너를 다루는 방법은 이미지를 다루는 방법에 비해서 짧고 간단하다. Docker 컨테이너짧게 컨테이너 명령어를 위한 기본 지식을 살펴본 다음 명령어들을 확인해보자. 컨테이너를 사용해서 실행되는 앱을 객체화 한 것처럼 실행할 수 있다. 컨테이너는 다음과 같은 상태로 나뉜다. 실행 중 정지 파기 위 주기는 용어로서 사용 된 건 아니고 그냥 말 그대로 이해하면 된다. 실행 중은 도커 컨테이너 안에서 프로세스가 동작하고 있는 상태이고, 정지는 컨테이너는 남아있지만 프로세스가 종료된 상태, 파기는 컨테이너가 Terminate 되는 것을 의미한다. docker container ls기본적으로는 현재 실행 중인 컨테이너의 목록을 보여준다. -a옵션을 붙이는 것을 통해서 현재 정지 상태에 있는 컨테이너까지 모두 볼 수 있다. docker container rundocker container run으로 이미지에서 컨테이너를 만들고 CMD 명령어를 실행시킬 수 있다. 명령어는 다음과 같이 쓴다. 12docker container run [옵션] 이미지명:태그 [명령] [명령 파라미터] ordocker container run [옵션] 이미지ID [명령] [명령 파라미터] 옵션과 명령, 명령 파라미터 부분은 모두 생략이 가능하다. 명령, 명령 파라미터는 Dockerfile에서 정의했던 CMD 인스트럭션 부분을 오버라이드 할 수 있다. 123456$ docker container run changhoi/hello-world-docker&gt; hello-world-docker@1.0.0 start /&gt; node indexHello world Docker! 주로 사용하는 옵션들은 아래와 같다. -p: 포트포워딩을 정의한다. 컨테이너의 포트와 호스트의 포트를 연결하는 옵션이다. 이에 대한 설명은 아래 잠깐 더 나와있다. -d: 백그라운드에서 실행하도록 하는 옵션이다. --name 컨테이너명: 실행되는 컨테이너에 원하는 이름을 붙일 수 있다. 이후 도커 명령을 사용할 일이 있을 때 컨테이너를 특정하기 쉬워진다. -i: 컨테이너의 표준 입력을 호스트와 연결한다. -t: 터미널 기능을 활성화 한다. --rm: 컨테이너가 종료될 때 컨테이너가 삭제되도록 한다. -v: 호스트와 컨테이너간 디렉토리, 파일을 공유하기 위해 사용하는 옵션이다. (갓도우는 무슨 설정을 해줘야 사용 가능하다.) 포트포워딩을 설명하자면, 일단 도커 컨테이너는 마치 하나의 게스트 OS처럼 자체 포트를 가지고 있다. 따라서 도커 안에 올라간 앱이 만약 3000번 포트를 listen하는 서버라고 치면 localhost:3000에서는 접근할 수 없다는 뜻이다. 그래서 도커 컨테이너의 3000번 포트를 로컬 호스트의 3000번 포트와 연결하겠다고 옵션을 만들어줘야 위 주소로 접근이 가능한 것이다. -p옵션은 다음과 같이 사용할 수 있다. -p 로컬의포트:도커포트 123456$ docker container run -p 8000:3000 changhoi/hands-on&gt; hello-world-docker@1.0.0 start /&gt; node indexServer is on 3000 위에 작동 중인 Docker는 3000번 포트를 사용하지만, 로컬 호스트의 8000번 포트와 붙어있는 것이기 때문에 localhost:8000에서 확인할 수 있다. 12curl localhost:8000&lt;h1&gt;Hello world docker!!!&lt;/h1&gt;% 여러 글을 확인 해본 결과 -i 옵션과 -t 옵션은 보통 같이 쓰이는 경우가 많다. 다음 파이썬 프로그램을 도커에 올렸다고 치자. 그 아래는 Dockerfile을 정의한 모습이다. 12345678# cal.pydef add_ten(num): return num + 10val = int(input(&quot;enter number: &quot;))output = add_ten(val);print(output); 12345FROM python:latestCOPY . .CMD [&quot;python3&quot;, &quot;cal.py&quot;] 이제 -it 옵션인 경우, -i옵션, -t옵션을 각각 쓴 경우, 안 쓴 경우를 비교 해보자. 순서는 안 쓴 경우, -i, -t, -it 순서이다. 12345678$ docker container run changhoi/py-calenter number: Traceback (most recent call last): File &quot;cal.py&quot;, line 4, in &lt;module&gt; val = int(input(&quot;enter number: &quot;))EOFError: EOF when reading a line# EOFError 에러가 난 모습 123456$ docker container run -i changhoi/py-calenter number: 1020# 정상 작동 12345$ docker container run -t changhoi/py-calenter number: 10# 컨테이너쪽 터미널에 입력이 전달되지 않는다. 123456$ docker container run -it changhoi/py-calenter number: 1020# 정상 작동 실험을 통해서, -t 옵션을 써야 터미널을 활용 할 수 있는 상태가 되는 것 같고, 터미널에서 사용자 I/O가 필요한 경우에 -i 옵션을 줘야 한다는 것을 알았다. 터미널에서 사용자 입력을 하고 싶으면 -it를 사용하면 된다. 위 실험을 해보는 동안 정지 상태의 컨테이너가 많이 생겼다. 이런 일회성 컨테이너는 일일히 삭제를 해줘야 하고, 만약 --name 옵션을 준 명령어를 사용한 다음, 컨테이너를 삭제하지 않고 또 다시 실행하는 경우에는 도커는 에러를 발생시킨다. (이미 존재하는 컨테이너라고) 이런 경우 --rm 명령어를 추가해서 간단하게 바로 삭제가 가능하다. 아래 예시에서도 --rm을 붙이면 docker container ls -a에서 나타나지 않는 것을 알 수 있다. 1234567891011$ docker container run --name unuse-rm node$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb2ed686f960e node &quot;docker-entrypoint.s…&quot; 9 seconds ago Exited (0) 8 seconds ago unuse-rm$ docker container run --name use-rm --rm node$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb2ed686f960e node &quot;docker-entrypoint.s…&quot; 2 minutes ago Exited (0) 2 minutes ago unuse-rm -v 옵션은 로컬의 파일시스템을 컨테이너와 공유할 수 있게 해주는 옵션이다. -v 로컬디렉토리:컨테이너디렉토리 형태로 작성한다. 아래 명령어로 현재 위치의 파일들을 컨테이너와 공유할 수 있다. 1$ docker container run -v `{PWD}`:/app docker container stop실행 중인 상태의 도커를 정지 상태로 만든다. docker container stop 컨테이너명 or 컨테이너ID를 사용해서 정지할 수 있다. docker container restart실행 중인 상태의 도커를 재시작한다. docker container restart 컨테이너명 or 컨테이너ID를 사용해서 재시작할 수 있다. docker container rm정지 상태의 컨테이너를 파기할 수 있다. docker container rm 컨테이너명 or 컨테이너ID를 사용해 파기할 수 있다. 일반적으로는 컨테이너가 정지 상태여야 파기가 가능하지만, -f옵션을 주면 실행 중인 컨테이너를 정지하고 파기할 수 있다. docker container exec실행 중인 컨테이너에 명령을 실행할 수 있다. docker container exec [옵션] [컨테이너명 or 컨테이너ID] [실행할명령] 형태로 사용한다. 기타 알아보면 좋을 것들 docker container cp: 실행 중인 컨테이너에 파일을 복사해 넣을 때 사용한다. docker container log: 표준 입출력으로 나타나는 로그들을 볼 수 있다. 파일에 저장되는 것은 볼 수 없다. docker container prune: 정지 상태인 도커 컨테이너를 모두 삭제해준다. 마치며컨테이너 명령어는 이미지보다 간단하게 정리했다. 이제 컨테이너들을 사용해서 개발 환경을 구축하다가 삽질 한 내용을 올려볼까 했는데 사실 그 삽질기는 여기 REF에서 더 자세하게 확인할 수 있다. 결과적으로 팀원들과 컨테이너 개발 환경을 구축했는데 개발 환경을 구축하는 것은 Docker compose를 사용해 여러 컨테이너를 활용한 로컬 개발 환경을 구축한 것이었다. 해당 결과는 이 링크에서 볼 수 있다. 해당 글은 이 시리즈처럼 자세한 이론은 설명하지 않았다. 결과적으로 어떻게 만들어내는 건지 궁금하다면 바로 링크를 확인하면 될 것 같다. 컴포즈에 대한 글은 다음 글에서 다뤄볼 생각인데, 꼼꼼한 설명보다는 필요한 내용과 어디서 어떻게 찾아볼 수 있는지에 대한 내용이 될 것 같다. Reference Book: 도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문 https://github.com/voyagerwoo/docker-hands-on https://docs.docker.com/v17.12/edge/engine/reference/commandline/container/ https://www.44bits.io/ko/post/almost-perfect-development-environment-with-docker-and-docker-compose?fbclid=IwAR3GPrNSySCj4k-4qu3pL-fRq29uBQ8RR6MSfviibHZnmYj-BQiV_G9e34U","link":"/posts/docker/docker-development-env-(2)/"},{"title":"컨테이너 서버리스 Fargate 배포하기","text":"최근 개발 중인 앱이, 이제 서비스 준비를 앞 두고 있는데, 개발자가 부족하므로, 운영 환경에 최대한 얽메이지 않고 싶다는 욕구에 의해 컨테이너 서버리스인 Fargate 배포에 눈을 돌리게 되었다. 이 글에서는 AWS ECS를 동작하기 위한 가장 기본적인 개념과 Fargate가 무엇인지 간단하게 설명한 다음 사이드 프로젝트로 진행 중이던 프로젝트를 Fargate 위에 띄우는 것까지 해볼 계획이다. 우선 Fargate가 뭔지, AWS에서 뭐라고 소개하는지 살펴보자. Fargate는 ECS의 서버리스 서비스이다. 공식적인 소개로는, 아래와 같이 작성되어있다. AWS Fargate는 컨테이너에 적합한 서버리스 컴퓨팅 엔진으로, Amazon Elastic Container Service(ECS) 및 Amazon Elastic Kubernetes Service(EKS)에서 모두 작동합니다.Fargate는 애플리케이션을 빌드하는 데 보다 쉽게 초점을 맞출 수 있도록 해 줍니다.Fargate에서는 서버를 프로비저닝하고 관리할 필요가 없어 애플리케이션별로 리소스를 지정하고 관련 비용을 지불할 수 있으며, 계획적으로 애플리케이션을 격리함으로써 보안 성능을 향상시킬 수 있습니다. 운영 환경을 직접 관리해야 하는 경우 보다는 당연히 애플리케이션에 집중할 수 있는 장점이 있다. ECS를 기반으로 하고 있기 때문에 ECS에 대한 개념 부분을 먼저 확인해보자. ECS에 대한 내용은 44bits의 글에서 가져온 내용들이 많다. 나에게 필요한 부분들만 선별해 요약했지만, 자세한 내용을 읽고 싶은 사람에게 꼭 추천한다. (아주 잘 정리 되어있음) 클러스터와 컨테이너 인스턴스ECS의 가장 기본적인 단위는 클러스터이다. 클러스터는 도커 컨테이너를 실행할 수 있는 가상의 공간이다. 클러스터는 프로젝트나 컨테이너의 성격에 따라 나눠질 수 있다. 클러스터는 컴퓨팅 자원을 포함하고 있지 않은 논리적, 가상적인(즉, 실제 컴퓨팅 자원이 필요하지 않다.) 단위이다. 이후에 EC2에 ecs-client라는 컨테이너 인스턴스의 자원을 모니터링하고, 관리하고, 클러스터로 요청된 컨테이너들을 적절하게 실행하는 역할을 하는 서비스를 실행해서 특정 클러스터에 연결할 수 있다. 이렇게 클러스터에 연결 된 EC2 인스턴스를 컨테이너 인스턴스라고 한다. (Fargate에서는 이처럼 관리를 담당하는 컨테이너가 필요하지 않게 된다.) Task &amp; Task definitionECS에서 컨테이너를 실행하는 최소 단위는 Task(태스크)이다. Task는 하나 이상의 컨테이너로 구성된다. 하나의 Task로 실행되는 컨테이너들은 모두 같은 컨테이너 인스턴스에서 실행되는 것이 보장된다. Task를 실행하려면 Task definition(태스크 디피니션)이 필요하다. Task를 통해 컨테이너를 실행하려면 여러 설정이 필요한데, 이러한 설정들을 미리 하나의 집합 단위로 정의해 놓고 사용하는 것이다. 이러한 집합 단위를 Task definition 이라고 한다. 한 번 Task definition을 설정해 주면, 그것을 기반으로 특정 설정을 변경할 수 있다. 이런식으로 변경된 내용들은 모두 리비전으로 저장된다. (리비전이 뭔지 아래 간단하게 언급하게 된다.) Service클러스터에는 두 가지 방식으로 태스크를 실행할 수 있다. 먼저 첫 번째 방식은 Task deifinition을 직접 사용해 태스크를 실행하는 것이다. 이런 식으로 실행된 태스크는 실행 이후 관리되지 않는다. 만약 일회성 프로그램의 경우는 명령 이후 바로 실행된 다음 종료되고, 데몬 프로세스는 프로세스를 종료할 때까지 컨테이너를 남겨둔다. 다른 방식으로는 Service를 정의하는 것이다. 간단하게 Service는 클러스터 내의 태스크를 스케줄링 하는 용도로 사용된다. 만약 EC2를 사용해서 프로세스를 배치하는 경우, 직접 어떤 인스턴스에 어떤 프로세스를 언제 배치할지 결정해야 한다. ECS에서는 Service가 이러한 스케줄링을 담당하고 있는 것이다. Service는 인스턴스에 설치된 ecs-client에서 수집된 정보를 기반으로 어디에 어떤 태스크를 언제 실행할지 결정하게 된다. Service의 타입으로 두 가지 타입이 있는데, 하나는 리플리카타입, 하나는 데몬 타입이다. 데몬 타입은 컨테이너 인스턴스(위에서 설명한 것처럼, 자원 모니터링, 관리, 컨테이너 실행을 담당하는 인스턴스)에 해당하는 태스크가 하나씩만 실행된다. 이 타입은 인스턴스 관리를 위한 용도로 사용된다. 리플리카 타입을 사용하려면 태스크 개수가 지정되어 있어야 한다. Service는 클러스터에서 이 개수만큼만 태스크가 실행되도록 자동적으로 관리해준다. 리플리카 타입은 실제 애플리케이션용으로 주로 사용된다. 리플리카 타입인 경우, 다수의 컨테이너 인스턴스가 올라갈 때 우리는 언제 어디에서 태스크가 실행될지를 알 수는 없다. Fargate 에서는 앞서 언급한 데몬 타입의 컨테이너 인스턴스가 필요하지 않다. 따라서 Fargate 설정을 선택하면 자동적으로 리플리카만 선택할 수 있다. 이제 직접 Fargate를 구성해보자. 먼저 이 글은 주관적인 경험을 작성 하고 있고, 데모를 위한 앱을 준비하지 않았다. 또한 디테일한 앱의 모습이나, 환경 변수 등도 따로 보여주지 않는다. 이미 돌아가는 앱을 Fargate에 올리는 것에만 집중한 글이다. 우선 현재 개발 중인 프로젝트는 다음과 같이 생긴 흔한 Node 프로젝트이다. .env는 프로젝트의 환경 변수들을 모아둔 곳이고, Dockerfiles는 배포, 개발 환경 별 도커파일을 담아두는 곳이다.현재는 개발 환경만 dev.Dockerfile과 docker-compose.yml로 도커라이징 되어 있다. 배포를 어떤 걸로 해두지 하고 있었는데, 이제 데모도 만들 겸 Fargate로 처음부터 올려보도록 하겠다. 앱 도커라이징사실 도커를 사용하게 되면서 고민은 환경 변수, SecretKey 등을 어떻게 전달할 것이냐를 자주 고민했고 케이스도 많이 찾아봤다. 몇 가지 방법 중에서 자주 본 방법은 .env 파일을 S3에 저장하고 환경에 맞게 Dockerfile에 다운로드 하고 export가 돌게 해라. AWS Systems Manager를 사용해라 (Fargate, ECS에서 해당 서비스에서 환경 변수를 불러올 수 있다.) 이 정도가 가장 기억이 난다. 위 두 가지 방법 중에 두 번째 방법을 트라이 해보고 싶은 생각은 들었지만 이번 프로젝트에서는 .env를 이미지 안에 붙여 넣고 dotenv를 사용해서 넣어줄 생각이다. 협업 하는 분들이 .env를 올바른 경로에 가지고 있어야 한다는 불완전성이 있지만, 개발 인원도 작고 프로젝트도 작으니 Fargate 데모에 집중하기 위해서 간단하게 시도해봤다. 우선 prod.Dockerfile을 만들자. 1234567891011121314151617FROM node:12.15WORKDIR /usr/src/appENV TZ Asia/SeoulCOPY package.json ./COPY yarn.lock ./COPY .env/prod.env ./.envRUN yarn installCOPY dist ./distEXPOSE 80CMD ['yarn', 'start:docker'] 도커 이미지를 빌드 할 때 이미지를 가볍게 만들고 싶어서 정말 필요한 것만 넣었다. 컨텍스트도 줄이기 위해서 .dockerignore를 만들어서 복사 제외 대상을 모두 뺐다. 정상적으로 원하는 결과를 얻기 위해서는 도커 이미지를 빌드할 때 애플리케이션이 먼저 빌드 되어야 한다. 이 정도로 간단하게만 작성한 다음 AWS의 도커 이미지 레지스트리인 ECR을 사용해서 빌드한 이미지를 올려보자. 이 이미지는 이후 Fargate에서 돌아갈 컨테이너가 될 것이다. ECR(Elastic Container Registery)을 이용해 도커 이미지 버전 관리하기AWS ECR은 프라이빗 도커 레지스트리 서비스이다. 프라이빗 레지스트리를 관리하기 위해서 AWS의 서비스를 사용하려고 한다. 아래 명령어와는 조금 다르지만, AWS에서도 Push 명령어를 확인할 수 있다. 저장소 만들고 도커 로그인일단 기본적으로 아래 명령어들은 aws cli가 설치되어 있어야 한다. 하지만 설치 과정을 설명하지는 않겠다. (얼마 전 v2가 정식 릴리즈 된 것으로 알고 있는데, 이건 v1을 기준으로 작성 했다.) 123456789# ECR 저장소 만들기aws ecr create-repository --repository-name [네임스페이스]/[레포지토리이름]# 이름 앞에 demo는 namespace 역할을 하고 뒷 부분이 저장소 이름이 된다.# ...결과가 나옴# ECR 저장소에 로그인 하기, 파이프라인 뒤에 zsh는 각자 환경의 쉘을 사용하자aws ecr get-login --no-include-email | zsh# 12시간동안 특정 레지스터에 유효한 토큰을 받는다. 그리고 docker login 명령어에 사용하면 도커를 업로드할 준비가 된 것. 토큰이 만료될 때까지 도커 이미지를 푸시하고 풀 할 수 있음# ... 결과가 나옴 이미지를 도커 파일 위치에 맞춰주고 컨텍스트는 현재 폴더로 지정한다. Dockerfile 내부에 package.json, yarn.lock, .env 폴더가 있는 곳을 컨텍스트로 둔 것을 가정하고 작성했기 때문에 컨텍스트는 프로젝트의 최상위가 되어야 한다. 태그는 만들었던 ECR 레포지토리로 맞춰줬는데 푸시 하기 위한 설정인 것 같다. 아래 명령어에서 xxxxxxx 부분은 아이디 값 처럼 생긴 고유 숫자가 들어간다. 아래 명령어를 입력하게 되면 좀 전에 만든 ECR 레포지토리로 빌드된 이미지가 올라가게 된다. 123docker image build -tag xxxxxxx.dkr.ecr.ap-northeast-2.amazonaws.com/demo/fargate-demo:latest -f ./Dockerfiles/prod.Dockerfile . # 도커 이미지의 태그를 ECR 태그로 맞춘다.docker image push xxxxxxx.dkr.ecr.ap-northeast-2.amazonaws.com/demo/fargate-demo:latest # 도커 이미지 푸시 위 명령어들은 그 전의 태그들이 untagged 되는 것을 고려 하지 않았다. 기존 latest 태그를 다른 태그로 변경해준 다음 푸시하는 작업을 진행하는 것이 좋을 것 같다. ECS 시작하기이제 ECS를 본격적으로 배포 해볼 계획인데 아래부터 나오는 이미지의 구체적인 명명과 설명 내용을 최대한 동일하게 글을 작성했지만, 조금씩 다를 수 있다. 설명은 실제 서비스를 하면서 작성한 것이고, 캡쳐는 데모를 준비하면서 뜬 거라… 그런 것이므로 헷갈려 하지 않았으면 좋겠다. AWS 콘솔에서 ECS 클러스터를 생성해야 한다. 템플릿이 총 세 가지가 있다. 여기서는 Fargate를 제공하는 네트워킹 전용 템플릿을 선택한다. 그 다음으로 클러스터를 구성할 때는 기존의 VPC를 사용할 예정이므로 VPC 생성은 해제 상태로 두고, 컨테이너 인사이트는 뭔지 궁금하니까 눌러둔다. 예시 이미지의 컨테이너 이름은 demo-cluster로 하기로 했다. Task definition 정의하기이제 태스크 디피니션을 만들 것인데, 태스크 디피니션은 위에서 설명한 바와 같이 하나 이상의 컨테이너 디피니션들과 컨테이너 실행 환경에 대한 정보를 담고 있는 리소스이다. 클러스터에 종속 되어 있지 않아서 정의한 다음 다른 클러스터에서도 재사용이 가능하다. 컨테이너에서 사용하는 환경변수와 컨테이너 실행 옵션을 포함하고 있다. 개념적으로는 docker-compose.yml과 비슷한 역할을 한다. 태스크 디피니션은 내부적으로 버전 관리가 이루어지는데, 처음 task라는 태스크 디피니션을 만들면, 실제로는 task:1 이런 식으로 버전이 표기 된다. 이런 버전 하나를 리비전이라고 한다. 태스크 디피니션은 0개 이상의 리비전들로 구성된다. task:1인 상태에서 특정 환경 변수를 수정하고 싶을 때는 리비전을 새로 만들어야 한다. 새로 만들어진 리비전은 task:2가 될 것이다. 콘솔에서 태스크 디피니션 (한국어로 작업 정의라고 표현되어있다.)을 만들자. 파게이트로 호환 방식을 설정하고 그 다음 단계에서 이름(작업 정의 이름)과 롤(작업 역할)을 지정해야 한다. 예시에서의 이름은 demo-task-def로 만들고 롤을 따로 지정하지 않았다. 여기서 롤은 IAM 권한을 뜻한다. 그 다음 태스크 실행 롤(작업 실행 역할)과 태스크 사이즈(작업 크기)를 설정해 줘야 한다. 태스크 실행 롤은 태스크에서 컨테이너 이미지를 가져오고, 사용자를 대신해서 CloudWatch에 컨테이너 로그를 게시할 때 이 역할이 필요하다고 한다. (태스크 실행에 IAM을 붙여주는 것 같음) 태스크 사이즈는 세세한 부분까지는 명확하게 이해하지는 못 했는데 태스크가 실행될 때 컴퓨팅 스팩을 정의하는 것 같았다. 우선 둘 다 가장 작은 사이즈로 골랐다. 사이즈 별로 파게이트의 요금 페이지에서 자세한 내용을 확인할 수 있다고 한다. 마지막으로는 컨테이너 정의를 추가 해야 한다. 컨테이너 추가 버튼을 눌러서 컨테이너를 추가해준다.컨테이너 이름으로 demo-app, 이미지 부분에는 xxxxxxx.dkr.ecr.ap-northeast-2.amazonaws.com/demo/fargate-demo를 넣어준다. 아래 많은 옵션이 있긴 한데, 작업 디렉토리를 실제 Dockerfile에서 작성한 곳으로 설정 해주는 것 외는 모두 디폴트로 두었다. 이후 실제 도입을 준비하면서 알아보도록 하고, 우선 기본 WORKINGDIR을 맞춰주는 옵션과, 80 포트와 매핑해주는 것으로 마무리 한다. 이후 아래 옵션들도 디폴트 값으로 두고 작업 정의를 생성한다. 컨테이너 외부에 공개Fargate의 특성상 태스크가 어떤 인스턴스에서 실행될지 알 수 없기 때문에, 외부로 노출하기 위해서는 실행된 서비스의 위치를 파악할 필요가 있다. 이런 것을 디스커버리라고 하는데, AWS에서는 이러한 기능을 LB를 사용해 해결할 수 있다. 일반적으로 로드밸런서를 만들 듯, 이름은 fargate-demo-lb, ALB로 구성하고, http를 열어주었다. 4번째 단계인 라우팅 구성에서 대상 그룹을 새로 만들고, 이름을 fargate-demo-group으로 만들었다. 하지만 ECS 서비스 생성할 때 대상 그룹을 새로 생성해서 지금 만들어지는 대상 그룹을 사용하지 않는다 (넘어가기 위해 만든 거라고 보면 된다). 5단계에서도 바로 넘어가도록 한다. 만들어진 LB에 가서 이전에 http를 연결해둔 대상그룹을 삭제한다. (리스너를 삭제한다.) 마찬가지로 방금 만들었던 fargate-demo-group을 삭제한다. ECS 서비스 만들기이제 서비스를 만들어보자. demo-cluster를 누르고 들어간 곳에서 서비스 탭에서 서비스를 생성할 수 있다. 서비스는 서비스 설정, 네트워크 설정, 오토 스케일링 설정, 리뷰 총 네 단계로 구성된다. 먼저 서비스를 구성할텐데, 시작 유형은 Fargate로 설정하고, 태스크 디피니션 (작업 정의)에는 아까 만들어둔 것을 올리고, 클러스터도 demo-cluster로 설정한다. 플랫폼 버전은 시작 유형을 Fargate로 설정해야 나오는데, LATEST를 그대로 사용한다 (아마 Fargate의 버전을 말하는 게 아닐까 싶다). 작업 개수는 2개로 설정한다. 작업 개수는 태스크가 최대 몇 개나 동작할 것인가를 정하는 것이다. 최소 정상 상태 백분율은 (정상 상태인 태스크의 비율이 최소 얼마여야 하는가) 50으로 두고 최대 백분율을 (태스크는 최대 몇 퍼센트까지 뜰 수 있게 할 것인지) 200으로 둔다. 위에 작업 개수를 2개로 했으니 방금 설정에 따르면 Fargate에서 관리하는 태스크는 최소 1개, 최대 4개까지 뜨는 것이다. 그 다음으로 네트워크를 설정해줘야 한다. 이 글에서는 외부 접근이 제한된 RDS와 연결을 해야 하는 상황을 작성했다. 단지 Fargate 배포 데모 정도 구경하는 과정은 여기까지만 읽고, 네트워크 설정을 기본 디폴트로 한 다음, 자동 할당 퍼블릭 IP를 열어두는 정도로 마무리 하면 배포가 완료될 것이다. 프라이빗한 RDS와 연결해주려면 같은 VPC를 선택 해야 한다. 그러고나서 서브넷을 퍼블릭으로(인터넷 게이트웨이가 붙은 라우터 테이블과 연결된 서브넷) 해줬다. (설명에 프라이빗만 가능하다고 되어 있는데, 이유를 잘 모르겠다. 읽어보지 않고 했는데 잘 됐다.) 그 다음 보안 그룹을 RDS에서 접근을 허용하는 보안 그룹을 포함하고 있어야 한다. 또한 HTTP 통신만 할 예정이므로, HTTP 역시 포함 되어 있어야 한다. (HTTPS는 LB에 붙일 생각이다. HTTPS 포트로 LB에 요청이 들어오더라도, LB는 요청을 컨테이너의 80번 포트로 전달해준다.) 마지막으로(서비스 검색은 체크 해제한다) LB를 붙여주면 되는데, 전에 만들었던 로드밸런서를 붙이고 80번 포트를 리슨하는 것으로 해준다. 그 다음 단계인 오토스케일링 단계 역시 이번 데모에서 사용하지 않으므로 건너 뛰자. 배포가 잘 이루어 지는 것을 확인했고, LB의 DNS 주소로 접근 할 수 있게 된다. 후기아쉽게도 이번 데모는 성공한 여러 버전들이 프라이빗인 관계로 레포를 준비할 수 없었다. (삽질한 것들은 그냥 지웠다.) 아마 다음 오토스케일링을 볼 때에도, 프라이빗일 것 같다. 설명서를 이번에는 뒤늦게 자주 보게 되었는데, 좀 더 천천히 살펴볼 필요가 있을 것 같다. (ECS 설명서는 목차가 나름 깔끔하고 내용도 잘 읽힌다. 아래 레퍼런스 링크에서 한 번쯤은 읽어봐도 좋을 것 같다.) ECS Fargate의 가격적인 측면을 사실 간과할 수 없다고들 하는데, 경험하지 못해서 그런지, 설명만 봐서는 그렇게 무자비한 가격은 아니라고 생각한다. Lambda와 비교하자면 더 비쌀 것 같긴 한데, (작은 서비스일수록, 프리티어가 해결해주는 범위 내면 싸다고 볼 수 있다.) 일반적으로 만들어지는 모놀리식한 방식의 앱을 배포할 수 있는 좋은 방법이 될 것 같다. 사실 도커라이징을 한 다음, EC2에 배포하기에 뭔가 아쉬운 점이 있어서 지금까지 그렇게 해본 적은 없는데, 항상 바라던 서비스가 아닌가 싶은 느낌이 든다 (쿠버네티스는 사용한 적 없어서 비교할 수 없다.). 다만 다시 한 번 확인해야 하는 점은 오토 스케일링인데, 서버리스라는 이름과 맞지 않게 오토 스케일링을 적용한다? 라는 말이 있어서 갸웃 했다. 맥시멈, 미니멈만 정해주면 알아서 오토 스케일링이 적용 되는 게 아닌가? 싶었다. 고민 해보면 기본적으로 원래 ECS가 EC2컨테이너를 사용한 도커 서비스라는 점 때문에(결국에 EC2를 어떻게 돌릴 것인가에 대한 설정이 필요한 건지?.. 내공이 부족하여 디테일 함 없이 둥실둥실 떠다니는 정도의 생각밖에 못 한다.) 람다와 같은 완전 서버리스같은 느낌이 덜 나는 것이 아닌가 싶다. ECS Fargate의 오토스케일링에 대해서 다음 번에는 공부해보고 도입해봐야겠다. Reference https://www.44bits.io/ko/post/container-orchestration-101-with-docker-and-aws-elastic-container-service#%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0cluster%EC%99%80-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4cluster-instance https://www.44bits.io/ko/post/getting-started-with-ecs-fargate#ecselastic-container-service-%EA%B8%B0%EC%B4%88 https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/developerguide/clusters.html","link":"/posts/docker/fargate-demo/"},{"title":"리눅스에서 Docker 명령어 sudo 없이 사용하기","text":"리눅스에서는 docker 명령어 앞에 항상 sudo를 붙여야 하는 불편함이 있었는데, 현재 계정을 docker 그룹에 추가하면 권한 문제가 해결된다. 방법은 아래와 같다. 1sudo usermod -aG docker $USER","link":"/posts/linux/add-user-to-docker-group/"},{"title":"우분투 zip, unzip 하기","text":"zip, unzip압축 하고 푸는 것을 도와주는 프로그램인데, 설치되어있지 않은 우분투에서는 아래 명령어로 설치할 수 있다. 1$ sudo apt-get install zip unzip zip 하는 방법1234567$ zip [압축파일 이름].zip [압축할 파일 or 디렉토리] [압축할 파일 or 디렉토리] ...$ zip -r [압축파일 이름].zip [압축할 파일 or 디렉토리] ... # 디렉토리 압축$ zip -j [압축파일 이름].zip [압축할 파일 or 디렉토리] ... # 디렉토리 이름 제외하고 압축$ zip -u [압축파일 이름].zip [압축할 파일 or 디렉토리] ... # 변경되었거나 새로운 파일만 압축 ex. 1$ zip Lambda-Deployment.zip ./* # 현재 폴더의 모든 파일을 Lambda-Deployment.zip으로 압축 unzip 하는 방법123$ unzip [압축파일 이름].zip$ unzip [압축파일 이름].zip -d [압축 푸는 위치] # 압축이 풀리는 위치를 특정할 수 있다. Reference https://araikuma.tistory.com/120 http://www.dreamy.pe.kr/zbxe/CodeClip/143985","link":"/posts/linux/zip-command/"},{"title":"리눅스 파일 정보 확인하기","text":"리눅스에서 ls -al 명령어로 확인할 수 있는 현재 위치의 전체 파일들을 자세히 보여주는 명령어 화면에서 확인할 수 있는 정보들을 정리 해보자 1drwxr-xr-x 2 user group 4096 Nov 9 20:23 filename d : 파일 타입을 의미한다. d: 디렉토리 / l: 링크 파일 / -: 일반 파일 등등 … rwxr-x-r-x : 파일 권한을 뜻한다 r: 읽기, w: 쓰기, x: 실행하기 [소유자 권한][그룹 권한][그 밖 사용자 권한] 위는 소유자는 읽고 쓰고 실행, 그룹은 읽고 실행, 그 밖은 읽고 실행이 가능하다. 읽는 방법은 2진수로 3자리로 끊어 읽는다. 위의 경우 755이다. 2 : 링크된 수를 뜻한다. in [대상 파일] [링크 파일] 명령으로 링크 파일을 만들 수 있다. user : 해당 파일을 소유한 유저 group : 해당 파일을 소유한 그룹. 특별한 변경이 없는 경우 소유자가 속한 그룹이 소유 그룹으로 지정된다. 4096 : 파일의 용량 Nov 9 20:23 : 파일 생성 날짜, 시각 filename : 파일 이름 chmod, chown 명령어를 알아봐야 겠다","link":"/posts/linux/detail-of-ls/"},{"title":"DHCP에 대하여","text":"DHCP란DHCP(Dynamic Host Configuration Protocol)은 호스트 IP에 주소를 동적으로 자동 할당하는 방식이다. 호스트에게 IP를 할당하는 작업은 수동으로도 할 수 있지만, 일반적으로 DHCP을 사용한다. 관리자는 수동으로 IP를 설정하지 않고, 특정 호스트가 들어왔을 때 이 호스트가 특정 IP를 받도록 하거나, 임시 IP 주소를 받도록 DHCP를 설정해두면 된다. 아래 이미지는 공유기 설정에 있는 DHCP 설정 화면이다. 호스트에게 동적으로 IP를 할당해주는 것 말고도, 서브넷 마스크 첫 번째 홉 라우터 (Default Gateway)의 IP 주소나, 로컬 DNS 서버 주소를 얻게 해준다. 이 기능은 호스트가 빈번하게 들어왔다 나갔다 하는 환경에 아주 유용하다. 예를 들어 도서관과 같은 곳에서 사람들이 IP를 얻기 위해 매번 네트워크 관리자를 찾아갈 필요가 없다. DHCP는 클라이언트/서버 구조의 프로토콜이다. 여기서 클라이언트는 네트워크 설정을 위한 정보를 얻기 위해 도착한 새로운 호스트이다. 서브넷에는 일반적으로 하나의 DHCP 서버를 갖는다. 만약 없다면, 해당 네트워크에서 사용할 DHCP 서버 주소를 알려줄 DHCP 연결 에이전트(일반적으로 라우터)가 필요하다. 223.1.2.0/24 서브넷에 연결된 DHCP 서버가 있고 나머지 두 서브넷에는 없기 때문에, 라우터가 DHCP 연결 에이전트 역할을 한다. DHCP 동작새로 들어온 클라이언트는 4단계 과정을 거쳐 IP를 할당받는다. 1. DHCP 서버 발견 (DHCP server discovery)새롭게 도착한 호스트는 상호 동작될 DHCP를 찾아야 한다. 이 과정은 DHCP 발견 메시지(DHCP discover message)를 통해 수행된다. 클라이언트는 서버의 67번 포트로 UDP 패킷을 보낸다. 현재 호스트는 자신이 접속될 네트워크 주소도 모르고 DHCP 서버의 주소도 모른다. 이런 상황이기 때문에 DHCP 발견 메시지를 포함하는 IP 데이터그램으로 UDP 패킷을 캡슐화 하는데, 이 메시지 내의 목적지 IP 주소는 브로드캐스팅 IP 주소인 255.255.255.255를 사용한다. 출발지 주소는 0.0.0.0으로 설정한다. 이 데이터그램은 링크 계층에 의해 서브넷 안에서 연결된 모든 노드로 브로드캐스팅 된다. 2. DHCP 서버 제공 (DHCP server offer)DHCP 발견 메시지를 받은 서버는 DHCP 제공 메시지를 보내 응답한다. 이 때에도 다시 IP 브로드캐스팅 주소 255.255.255.255를 사용해 서브넷의 모든 노드로 보낸다. 제공 메시지에는 다음 정보들이 포함되어있다. 수신된 메시지의 트랜잭션 ID 클라이언트에게 제공된 IP 주소 네트워크 마스크 IP 주소 임대 기간 (IP 주소가 유효한 시간) 3. DHCP 요청 (DHCP request)서브넷에는 도달할 수 있는 DHCP 서버가 복수일 수 있으므로, 클라이언트는여러 응답 중 하나를 선택해 파라미터 설정을 통해 DHCP 요청 메시지를 만들어 응답한다. 4. DHCP ACK서버는 DHCP 요청 메시지에 대해 요청된 파라미터를 확인하는 DHCP ACK 메시지로 응답한다. 클라이언트가 ACK를 받으면 상호 동작은 종료되고 IP 주소를 임대 기간 동안 사용할 수 있다.","link":"/posts/network/about-dhcp/"},{"title":"HTTP&#x2F;3에 대하여","text":"HTTP/2가 나온 지 얼마나 됐다고 벌써 HTTP/3 도입 사례가 들리기 시작할까? 네이버는 검색 서비스에, 토스는 페이먼츠에 HTTP/3을 도입했다고 한다. HTTP/2는 2015년 5월에 릴리즈됐다고 한다. 약 7년 전이고 생각보다는 오래됐지만 HTTP/1.1이 버텨온 기간보다는 짧다. HTTP/3은 무엇을 해결하려고 했고, 어떻게 해결했을지 정리했다. HTTP/2의 문제이전에 gRPC를 설명하면서 간단하게 HTTP/2에 관해 설명한 적이 있다. HTTP/2는 과거보다 나아진 현대 네트워크 환경에 맞는 프로토콜이라고 볼 수 있다. 하나의 TCP 물리적인 컨넥션을 사용해 논리적인 스트림을 사용하는 구조이다. 이를 통해 시스템의 물리적인 메모리 사용이라든지 리소스를 훨씬 덜 사용하면서도 HTTP/1의 결점을 수정할 수 있었다. 오늘날 HTTP/2는 인터넷 통신의 약 40%를 차지한다.TCP는 신뢰성 있는 데이터 전송을 위해 Handshake 과정이 포함되어있다. 일반적으로 TLS/TCP 스택을 생각해보면 애플리케이션 레이어가 데이터를 주고받기까지 최소 두 번(TCP 연결 &amp; TLS 연결)의 라운드 트립 딜레이가 발생한다. CPU, 네트워크는 점점 빨라지지만 빛의 속도는 변하지 않기 때문에 절대적으로 걸리는 시간을 줄이는 데 한계가 있다. 이러한 TCP의 구조적 문제 말고, TCP 위에서 스트림을 이용해 멀티플렉싱하고 있는 HTTP/2의 문제가 있는데, TCP의 선형적인 데이터 전송 특성으로 인해, 임의의 스트림이 느려지는 것이 다른 스트림에게 영향을 주게 된다는 점이다. 즉, HOL(Head Of Line) Blocking 문제가 발생한다. 만약 패킷 손실률이 2%라고 했을 때 HTTP/2를 사용하는 것 보다 여러 TCP 컨넥션을 사용하는 HTTP/1.1이 더 나은 성능을 보여준다고 한다. 패킷을 100개 보냈을 때 2개가 손실되는 것은 정말 열악한 환경이다. 서버 사이의 통신에서는 이런 상황이 자주 나오지 않을 수 있다. 하지만 클라이언트와 통신하는 환경은 워낙 다양하고 네트워크 품질이 안 좋은 상황이 자주 있다. HTTP/3HTTP/3을 단순하게 말하자면 HTTP Over QUIC라고 볼 수 있다. QUIC는 UDP 위에서 구현된 전송 레이어이다. 그래서 이제부터는 HTTP/3의 본체인 QUIC에 대해 얘기할 예정이다. 위는 논문의 공식적인 그림이다. QUIC는 일종의 전송 프로토콜이다. 첫 번째로 HTTP 메시지를 전달하는 역할을 하는 것이다. 그래서 초기 HTTP/3은 HTTP Over QUIC라고 불렸다. 다른 프로토콜을 QUIC 위에 전송하도록 하는 작업은 공식 버전 1.0 출시 이후 연기되었다고 하는데, 그로부터 3년이 지났으므로 어떤 발전이 있을 수도 있다. 왜 QUIC이어야 했을까?왜 QUIC라고 하는 전송 프로토콜을 UDP 위에 만들었어야 했을까? 왜 새로운 전송 레이어를 만들지 않았나왜 TCP, UDP, QUIC같이 전송 계층에 새로운 레이어를 만들지 않았을까? 이는 사실 간단한데, 배포가 굉장히 어렵다. 전송 레이어의 프로토콜이 추가되는 것은 커널의 업데이트가 요구되기도 하고, 모든 중간 미들박스에서 이를 통과시켜줘야 한다. 그냥 단순히 통과시킬 수도 있지만, 보안 측면으로 익숙하지 않은 패킷의 형태가 통과되지 않을 가능성도 있다. 왜 TCP 업데이트 하지 않았나TCP 동작 방식을 변경하는 것은 사실 위와 유사한 이유로 인해 문제가 발생한다. TCP는 커널에 의해 구현되어 있으므로 OS 업데이트가 요구된다. 따라서 QUIC가 배포되기 위해 네트워크 경로의 미들박스들이 모두 커널을 업데이트해야 하는 문제가 있다. 또는 미들 박스들이 기존 TCP 스택과 강하게 바인딩 된 어떤 소프트웨어로 동작하고 있는 경우 문제를 발생시킬 여지가 있다. 왜 UDP인가위에서 언급한 것처럼 TCP를 사용하면서, TCP의 문제점을 수정한 업데이트 버전을 사용하지 않는다면 비효율적인 Handshake라든지, HOL Blocking 등 근본적인 문제와 또다시 직면하게 된다. UDP는 네트워크 프로토콜이라고 불릴 만큼 네트워크 레이어로부터 받은 데이터에 단순히 포트와 IP 정보만 추가된 정말 얇은 전송 계층의 프로토콜이다. 이 위에 신뢰성을 보장해줄 수 있는 QUIC를 만들었다.이러한 이유로 QUIC를 바닥부터 설계하게 되었는데, QUIC의 설계는 다음과 같은 목표를 달성하기 위해 디자인되었다. Deployability: 기존 시스템이 온전히 인식할 수 있어야 한다. Security: 기존 TLS 암호화 과정처럼 데이터를 안전히 전달해야 한다. Reduction in Handshake: 기존 TLS/TCP 스택이 갖고 있던 Handshake 비효율 문제를 해결해야 한다. HOL Blocking: 멀티플렉싱 되는 데이터가 HOL Blocking 문제를 맞이하면 안 된다. 위 목적을 달성하면서도 TCP처럼 신뢰성 있는 전달, 흐름 제어 및 혼잡 제어 등을 처리해야 하는 목적도 있다. 이제 QUIC가 어떻게 동작하는지 하나씩 살펴보자. ConnectionQUIC 역시 신뢰성 있는 통신을 위한 Handshake 과정이 있지만, TCP와 다르게 암호화와 통신을 위한 Handshake 과정이 통합되어있다. 먼저 논문에 소개된 그림은 다음과 같다. 하나씩 천천히 확인해보자. Initial Handshake맨 처음 클라이언트는 서버에 대한 어떤 정보도 없기 때문에 서버의 정보를 얻어 오기 위한 가장 처음 요청이 필요하다. 이 Client Hello 단계를 Inchoate CHLO라고 한다. 논문을 봤을 때는 아마 이 메시지에는 QUIC 버전 협상과 관련된 정보가 있는 것으로 보인다. 버전 협상에 대한 내용은 이후 잠깐 후술한다. REJ 메시지는 “reject”라는 뜻으로, 클라이언트의 메시지가 응답을 보내기에 부적합한 경우 서버가 보내는 메시지이다. 이 메시지에는 다음과 같은 내용이 포함되어 있다. 서버의 설정 (서버의 장기(Long-term) Diffie-Hellman(이하 DH) 공개값 등) 서버 인증서와 시그니처 클라이언트의 공개 IP 주소와 서버의 타임스탬프가 포함된 인증 암호 블록 인증서와 시그니처는 HTTPS 연결 과정 처음처럼 해당 서버가 인증된 서버인지 확인할 수 있게 해주고, 인증 암호 블록은 이후 클라이언트가 IP 주소의 오너십을 증명하기 위해 다시 서버에게 보낸다. DH는 두 개의 키를 가지고 하나의 시크릿 값을 추출할 수 있는 알고리즘이다.Inchoate CHLO 과정 이후 REJ 메시지를 받은 클라이언트는 서버의 장기 DH 키값을 가지고 있게 되고 자신의 임시(Ephemeral) DH 키를 사용해 시크릿 값을 서버에게 전달할 수 있게 된다. 알고리즘에 대한 자세한 내용은 다음 링크에서 확인해보자. Final Handshake위에서 언급한 것처럼 REJ 메시지를 받은 클라이언트는 서버의 장기 키와 클라이언트의 임시 키를 사용해 DH 알고리즘으로 비밀 값을 만들어낼 수 있다. 이렇게 만들어진 비밀 값을 initial key라고 한다. 클라이언트는 서버에게 메시지를 initial key로 암호화하고, 사용된 클라이언트의 임시 DH 값을 같이 보낸다. 즉 두 번째 Handshake부터 서버에게 암호화된 메시지를 보내기 시작하므로, 이를 1-RTT(1 Round Trip Time) Handshake라고 한다. 이때 보내지는 클라이언트의 메시지는 Complete CHLO라고 불린다. 이 메시지를 받은 서버가 내용을 올바르게 복호화하고 Handshake가 성공적으로 되면 SHLO(Server Hello)라는 메시지를 보낸다. SHLO에서는 서버의 장기 DH 키가 아니라, 임시 키로 만든 비밀 값을 사용해 응답을 암호화 한다. 즉, 클라이언트의 임시 키와 서버의 임시 키로 새로운 비밀 값을 만들어내는 것이다. 이때 만들어지는 비밀 값을 forward-secure key라고 한다. 마찬가지로 SHLO에 서버의 임시 키를 포함함으로써 클라이언트 역시 forward-secure key를 만들 수 있게 한다. 이후 메시지는 양측 모두 forward-secure key를 통해 암호화하고 복호화하게 된다. 클라이언트는 Handshake에 성공하게 되면 서버 설정 및 소스 주소 토큰을 캐시하고 있다가 같은 곳으로 반복되는 컨넥션이 발생할 때 Inchoate CHLO를 건너뛰고 바로 Complete CHLO를 보낼 수 있게 된다. 만약 이 Handshake가 성공하게 되면 바로 응답을 받게 되므로 0-RTT 컨넥션이 성공하는 것이다. 항상 Complete CHLO가 성공적인 것은 아니다. 위 논문의 이미지에서처럼 만약 클라이언트가 캐시를 사용해 Complete CHLO를 서버에 보냈을 때, 모종의 이유로 서버의 설정이 변경되거나 장기 DH 값이 바뀌는 경우가 있다. 이 경우도 서버는 REJ를 보내게 되고 클라이언트는 다시 Complete CHLO를 보내야한다. Version NegotiationQUIC 클라이언트와 서버는 컨넥션이 일어나는 동안 버전 협상을 한다. QUIC 클라이언트는 첫 번째 패킷에 사용할 버전을 명시해서 보내는데 만약 서버가 사용할 수 없는 버전을 가지고 있다면 서버는 서버가 사용할 수 있는 모든 버전을 담아 협상 패킷을 보내야한다. 이 과정은 RTT 딜레이를 만드는 원인이 된다. 컨넥션을 만들 때 복잡한 Handshake 과정을 생략하고 바로 0-RTT로 컨넥션이 성공하거나 1-RTT만으로 암호화된 데이터를 전달할 수 있기 때문에 지표상 컨넥션 퍼포먼스가 TCP보다 항상 높은 편이다. Stream MultiplexingQUIC는 HTTP/2가 프레임 같은 데이터 유닛을 TCP의 추상화된 바이트 스트림을 통해 멀티플렉싱하는 것처럼 UDP에서 이렇게 동작하도록 해두었다. QUIC는 TCP의 순차 전달로 인해 생기는 HOL Blocking을 막기 위해 UDP 위에서 동작하고 특정 논리적인 스트림에서 데이터 손실이 발생해도 다른 스트림의 흐름을 막지는 않는다. QUIC의 스트림은 신뢰성 있는 양방향으로 바이트 스트림을 전달하는 가벼운 논리적인 스트림이다. 스트림은 최대 2^64 바이트의 임의 크기 메시지를 애플리케이션에 전달할 수 있으며 아주 가벼워 여러 스트림이 동시에 동작할 수 있다. QUIC의 패킷은 다음 그림처럼 하나 이상의 프레임이 뒤따르는 공통 헤더로 구성되어 있다.스트림은 스트림 ID를 통해 구분되는데, 이 값은 서버에서 시작되는 경우 짝수 아이디를 갖게 되고 클라이언트로부터 시작되는 경우 홀수의 아이디를 갖게 되어 충돌을 막는다. 스트림은 첫 번째 바이트가 보내질 때 생성되고 양측이 마지막 스트림 프레임에 FIN 플래그 비트를 찍음으로써 스트림을 닫게 된다. 만약 클라이언트나 서버 중 한쪽이라도 스트림 위의 데이터가 필요 없다고 판단되는 경우 다른 스트림이나 QUIC 컨넥션 자체를 파괴하지 않으면서 스트림을 취소할 수 있다. QUIC 패킷 전송 속도는 흐름 제어 및 혼잡 제어 등으로 인해 제한된다. 따라서 여러 스트림이 사용할 수 있는 대역폭을 나누는 방법을 결정해야 한다. QUIC의 구현에서는 특별한 방법은 없고 HTTP/2의 스트림 우선순위 기능에 의존한다. Loss RecoveryTCP에서는 패킷에 시퀀스 번호를 부여하고 순서대로 패킷이 전송될 수 있도록 신뢰성 있는 통신을 한다. 손실 복구 작업 역시 이 패킷의 시퀀스 번호를 사용하는데, 서버가 ACK 응답을 보낼 때 받았던 패킷의 번호를 그대로 사용하므로 클라이언트는 재전송 패킷에 대한 ACK 응답인지 오리지날 패킷에 대한 ACK 응답인지 알지 못한다. 이를 “재전송 모호 문제“(Retransmission Ambiguity Problem)이라고 한다. 또한 일반적으로 재전송 세그먼트의 손실 같은 경우 타임아웃에 의해 탐지되는데 이는 아주 비싼 동작 방식이다. Flow Control &amp; Congestion ControlQUIC는 두 가지의 흐름 제어 전략을 쓰고 있다. 하나는 컨넥션 자체의 모든 스트림들이 가지고 있는 버퍼의 총량에 대한 흐름 제어, 다른 하나는 스트림마다 소비하는 버퍼의 흐름제어이다. 만약 애플리케이션에서 특정 스트림을 소비하는 속도가 느리다면 해당 스트림이 전체에 자치하는 버퍼를 제한한다. 그렇지 않으면 스트림 버퍼에 데이터가 늘어나면서 다른 스트림이 사용할 수 있는 버퍼 공간을 더 소비하게 된다. 이는 전체적인 관점에서 HOL Blocking과 같은 효과를 가져올 수 있다. 기본적으로 윈도우 사이즈는 패킷이 주고받을 때마다 커지며, 컨넥션 레벨의 흐름제어와 스트림 레벨의 흐름 제어가 모두 동일하게 동작한다. 다만 컨넥션 레벨의 크기가 훨씬 커 여러 스트림들이 내부적으로 동시 동작해도 문제가 없도록 설계되어 있다. QUIC에서 혼잡 제어 알고리즘은 특정하고 있는 바가 없다. 인터페이스를 제공하고 해당 인터페이스를 구현한 혼잡 제어 알고리즘을 사용할 수 있다. 논문에서 말하길 구글에 배포된 상태에서는 TCP와 QUIC 모두 Cubic을 혼잡 제어 컨트롤러로 사용하고 있다고 한다. Connection MigrationQUIC 연결은 64비트 Connection ID를 통해 식별된다. Connection ID는 클라이언트 IP 또는 포트가 달라지더라도 컨넥션을 유지할 수 있게 해준다. 예를 들어 NAT 타임아웃이 발생하면서 Rebinding이 발생한다든지, 클라이언트가 네트워크 연결을 변경한다든지 하는 상황이 있을 수 있다. Discovery for HTTPS맨 처음 요청을 보낼 때 클라이언트는 사실 서버가 QUIC를 제공하고 있는 서버인지 알 수는 없다. 처음에는 일반적인 TLS/TCP 요청을 보내는데, 서버는 응답 헤더에 Alt-Svc 헤더를 포함하여 QUIC를 지원하고 있다는 것을 알린다. 이후 클라이언트는 서버에게 후속 요청을 QUIC와 함께 보내게 된다. 후속 요청의 경우 기존 연결이 시작된 TLS/TCP 스택과 QUIC 스택이 동시에 전달되며 경합 과정을 거친다. 하지만 300ms 정도의 차이가 있어도 (즉, TCP 위의 요청이 300ms 안쪽으로 빨랐다면) QUIC를 선호하여 선택하게 된다. 후속 요청에서 MTU 패킷 사이즈보다 QUIC Handshake 사이즈가 더 크거나, 중간에 UDP 차단으로 인해 Handshake 과정이 실패한다면 TLS/TCP 연결을 사용하게 된다. HTTP/3 성능과 문제HTTP/3은 컨넥션 과정을 간결하고 빠르게 설정되도록 줄였고 TCP에서 발생하는 HOL Blocking 문제를 완화했다. 아주 실험적인 것처럼 보일 수 있지만 구글에서는 과거부터 유튜브 같은 곳에서 HTTP/3을 도입했다고 한다. 그래서 생각보다는 HTTP/3가 인터넷 트래픽에서 차지하는 비율이 꽤 높다. QUIC의 Handshake는 그냥 보기에도 의미가 있어 보인다. 아래는 TCP와 QUIC의 Handshake를 비교한 모습이다. 하지만 HOL Blocking은 레이턴시가 적은, 네트워크 환경이 좋은 곳이라면 TCP와 큰 차이가 안 난다. 아래 표에서는 레이턴시가 적은 곳에서는 오히려 성능상 이점을 보이지 못했다. 그리고 계속 개선될 것으로 보이지만, UDP 스택이 TCP에 비해서는 그렇게 많이 최적화되지 않았다. 그래서 CPU 같은 컴퓨팅 자원을 더 소비하게 된다고 한다. Reference https://platum.kr/archives/196664 https://blog.toss.im/article/tosspayments-upgrades-web-protocol https://blog.cloudflare.com/ko-kr/http3-the-past-present-and-future-ko-kr/ https://http3-explained.haxx.se/ko https://medium.com/codavel-blog/quic-vs-tcp-tls-and-why-quic-is-not-the-next-big-thing-d4ef59143efd https://blog.acolyer.org/2017/10/26/the-quic-transport-protocol-design-and-internet-scale-deployment/ https://w3techs.com/technologies/details/ce-http2 https://w3techs.com/technologies/details/ce-http3 https://evan-moon.github.io/2019/10/08/what-is-http3 https://www.cse.wustl.edu/~jain/cse570-21/ftp/quic/index.html https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46403.pdf","link":"/posts/network/about-http3/"},{"title":"NAT에 대하여","text":"NAT은 클라우드 환경에서 인프라를 구성하다 보면 쉽게 보이는 단어들이다. 사설망을 유지하면서 외부 인터넷과 단방향으로 연결하려면 어떻게 할지?와 같은 상황에서 NAT 인스턴스를 통해 외부와 연결하는 방법이 있다. NAT이 어떻게 동작하는 건지 간단하게 정리했다. 동작 방식일단 네트워크를 활용해야 하는 장비는 모두 IP가 필요하다. 그런데, 모든 장비에게 각각의 IP를 부여하면, 하나의 작은 오피스에도 넓은 범위의 IP 주소 범위를 할당해야 한다. 이런 문제를 해결하는 방법이 네트워크 주소 변환(NAT, Network Address Translation)이다. NAT 역할을 하는 라우터는 실제로 외부에서 보기엔 라우터처럼 보이지 않고, 하나의 IP 주소를 갖는 장비로서 동작한다. 위 이미지를 보면, NAT 안쪽으로는 192.168.1.0/24 에 해당하는 서브넷을 사용하고 있다. 이 주소는 사설망이라고 하고, 전세계적으로 사용되는 사설망 IP 대역이 있다. 10.0.0.0/8 (10.0.0.0 ~ 10.255.255.255) 172.16.0.0/12 (172.16.0.0 ~ 172.31.255.255) 192.168.0.0/16 (192.168.0.0 ~ 192.168.255.255) 위 주소는 수많은 네트워크들이 동일하게 부여받기 때문에, NAT을 넘어서 외부에서는 사용할 수가 없다. NAT 안쪽을 LAN 밖을 WAN 방향이라고 표현할 수 있다. 192.168.1.2 PC에서 173.194.67.102 호스트로 요청을 보내기 위해서는 192.168.1.1을 할당받은 NAT 인터페이스로 들어가서 NAT이 ISP의 DHCP를 통해 부여받은 IP를 사용해 요청을 보내게 된다. 각 PC가 사설망의 IP 주소를 얻는 과정도 NAT의 [[DHCP]] 서버를 통해 얻게 된다. 구체적인 예시를 생각해보자. 192.168.1.2 PC는 173.194.67.102:80 으로 요청을 보내고 있다. 출발지 PC는 요청에 임의의 포트 번호를 달아 보내게 된다. 여기서는 3345를 할당했다고 가정해보자. 192.168.1.2:3345 형태로 LAN에 데이터그램을 보내면, NAT은 이 데이터그램을 받아, NAT 출발 IP 주소를 NAT의 WAN쪽에 있는 인터페이스의 IP로 바꾸고, 포트번호도 NAT에서 할당 할 수 있는 새로운 출발지 번호로 다시 변경한다. 즉, 192.168.1.2:3345를 67.210.97.1:5505로 바꾼다는 뜻이다. 이 때, 변환된 정보를 저장하기 위해 NAT 변환 테이블(NAT Translation Table)을 사용한다. 이런 TCP 요청을 보내면, 다음과 같은 엔트리가 추가된다. Protocol LAN WAN TCP 192.168.1.2:3345 67.210.97.1:5505 서버는 다시 67.210.971:5505에게 응답을 보내게 되고, NAT는 NAT 변환 테이블을 사용해서 다시 원래 IP와 포트번호로 바꿔 LAN 네트워크로 보낸다. NAT의 포트 번호와 LAN의 장비를 연결해 테이블 엔트리를 작성하기 때문에, 결국 포트 숫자 만큼 동시접속을 허용한다. 2^16, 약 6만개의 동시접속을 허용하는 것이다. 위 방식은 NAT 종류 중 PAT(Port Address Translation) 방식에 해당한다. 사용 이유 IP 주소 절약: 처음 언급한 것처럼, 하나의 공인 IP를 가지고 여러 사설망을 운영함으로써 IP 소비를 줄일 수 있다. 보안: NAT의 특성상, 요청을 보낸 측의 IP를 숨기는 효과가 있다. 라우터 안쪽의 최종 목적지를 외부에서는 알 수 없다. 따라서, 내부에서 외부로의 요청은 가능하지만, 외부에서 내부로 요청은 불가능하다. 단, NAT에서 포트포워딩을 통해 특정 포트에 대해서 내부로 라우팅을 처리한다면 가능하다.","link":"/posts/network/about-nat/"},{"title":"Rust Ownership","text":"Rust는 GC가 없는 언어이다. 보통은 언어가 힙 메모리를 관리하기 위해 GC를 사용하거나 개발자가 직접 관리하는 두 가지 노선을 선택해 왔지만, Rust는 조금 독자적인 방법을 선택했다. 각 변수가 사용하는 메모리에 대한 소유권을 하나만 유지하면 GC가 필요 없다는 점을 이용한다. 만약 하나의 변수에 힙 영역 데이터가 묶여있다면 해당 변수가 더 이상 접근 불가능한 상태가 되었을 때 메모리를 곧바로 해제해버리면 된다. 실제로 러스트를 사용하다 보면 힙 메모리를 free 하지 않아서 간단한 프로그램을 쓸 때 꼭 GC가 있는 언어처럼 느껴진다. 이번 글에서는 소유권 및 그와 연관된 여러 러스트의 컨셉을 정리했다. Ownership (소유권)GC가 있는 언어 같다고 했지만, 개인적으로 사실 조금 이질적인 느낌이 있다. Ownership은 러스트를 잘 쓰기 위해 숙련도가 요구되는 주범이다. The Rust Programming Language 책에서는 소유권을 “규칙”이라고 설명한다. 소유권과 관련된 규칙들을 컴파일 타임에 모두 확인하고 하나라도 지켜지지 않는다면 컴파일되지 않는다. 컴파일 타임에 확인되므로 런타임에서는 퍼포먼스에 영향을 주지 않는다. 규칙은 다음과 같다. 각 값은 모두 주인(Owner)을 가지고 있다. 한 번에 하나의 Owner를 갖는다. Owner가 스코프를 벗어나면 값은 정리된다. 값은 하나의 식별자를 주인으로 갖게 된다고 이해하면 좋을 것 같다. 식별자는 러스트의 변수 스코프에 의해 접근 불가능해지는 순간이 오는데, 이때 값들을 모두 정리한다. Variable변수는 자신이 담고 있는 값이 어느 정도의 메모리를 사용해 할당되는지 알고 있어야 한다. 예를 들어서 다음과 같이 u8 타입이 있다면 컴파일러 입장에서 이 타입은 1바이트를 사용한다는 것을 알 수 있다. 1let v = 1_u8; 하지만 그렇지 않은 경우도 있다. 예를 들어 가변 길이의 벡터라든지, 문자열을 필드로 가지고 있는 구조체라든지 사용자에게 입력받는 경우 런타임에 데이터가 결정되므로 메모리 크기를 컴파일 타임에 알 수 없다. 일반적으로 언어에서 이러한 경우는 힙에 데이터를 넣는 방식으로 해결한다. 보통 동적 할당한다고 표현하는데, 런타임에 메모리를 필요한 만큼 힙 메모리에 할당해 사용한다. 메모리를 관리해야 하는 상황은 이렇게 힙에 할당하는 상황이다. GC 역시 힙 영역의 메모리 관리에 대해 얘기를 한다. GC에 대해서 작성한 글 Rust의 String 타입이 힙을 사용하면서, 아주 친근한 데이터 타입으로, Ownership을 설명하기 적합하다. 123{ let s = String::from(&quot;hello&quot;);} // scope 종료 이후 s와 해당 메모리는 정리된다. Rust는 이렇게 스코프가 종료되는 시점에 drop 함수를 호출하는데, 이 함수는 작성자가 메모리를 free 하기 위한 코드가 담겨있다. 1234567891011121314151617struct MyType { v: u8,}impl Drop for MyType { fn drop(&amp;mut self) { println!(&quot;drop my type&quot;) }}fn main() { let s = MyType { v: 1 }; println!(&quot;{}&quot;, s.v);}// 1// drop my type 이 drop 함수는 예시에서 보이는 것처럼 Drop 트레잇을 구현한 것이다. 러스트를 모르지만, 컨셉이 궁금해서 온 사람들은 Drop이 인터페이스라고 생각하면 될 것 같다. 이렇게 소유권은 하나만 갖게 하면서 소유권을 가진 식별자가 스코프를 벗어날 때 Drop 하므로 GC가 필요 없다. 마치 Reference Count를 하나로 유지하는 것과 비슷하다. C 언어를 잘 알지는 못하지만, 패키지를 사용할 때 잘 만들어진 패키지의 경우 사용된 데이터 타입을 어디서 free 하는 책임을 갖는지 주석으로 명시한다고 들었는데, 러스트는 마치 이 방향을 컴파일 타임에 규칙으로서 명시해 문제를 해결하는 느낌이다. 러스트처럼 변수 스코프가 종료되는 시점에 패턴을 C++에서는 RAII(Resource Acquisition Is Initialization)이라는 이름으로 부른다. 이제 String 타입을 통해 소유권에 대해 조금 더 자세히 설명한다. 소유권 이전본격적인 소유권 이전에 대한 설명 전에 스택 위에서 할당되는 값으로 보자. 다음과 같이 y = x처럼 다른 식별자에 값을 복사해 넣는 것을 비교해볼 예정이다. 12345let mut x = 1;let mut y = x;x += 1;y -= 1;println!(&quot;x: {} / y: {}&quot;, x, y); // 2, 0 x는 1을 할당 받고 y는 x를 할당 받았지만, 둘 다 값이 복사되어 서로 다른 메모리에 있는 값을 보게 된다. 이는 정수형처럼 정해진 사이즈를 가진 값인 경우 할당 연산을 수행할 때 새로운 값이 스택에 복사되어 메모리를 따로 받게 되기 때문이다. 구구절절 설명했지만, 일반적으로 프로그래밍 언어에서 우리는 이러한 상황을 아주 자연스럽게 받아들일 수 있다. 이제 힙을 사용하는 경우를 확인해 보자. 힙을 사용하는 데이터 중에 일반적이라고 생각되는 자바스크립트의 코드를 살펴보면 다음과 같이 동작한다. 1234let obj = { data: 10 };let copeid = obj;copied.data = 100;console.log(obj); // { data: 100 } 우리는 이전에 사용했던 obj 객체에 접근할 수도 있고, 같은 힙 메모리를 공유하며 각자의 수정 사항을 모두 동일하게 확인할 수 있다. 러스트는 아주 독특하게 동작하는데, 결과만 말하자면 힙에 있는 데이터를 가리키고 있는 식별자는 다른 식별자에 힙 포인터를 복사해 넣는 순간 해당 데이터에 대한 소유권을 이전한다. 그리고 소유권을 잃은 식별자는 더 이상 접근할 수 없다. 123456789101112let s1 = String::from(&quot;hello&quot;);let s2 = s1;println!(&quot;s1: {} / s2: {}&quot;, s1, s2);/*let s1 = String::from(&quot;hello&quot;); -- move occurs because `s1` has type `String`, which does not implement the `Copy` traitlet s2 = s1; -- value moved hereprintln!(&quot;s1: {} / s2: {}&quot;, s1, s2); ^^ value borrowed here after move*/ 이해가 어려운 말들이 나오는데, 동작을 설명하기에 앞서 먼저 String 타입을 간단히 설명하자면 아래와 같이 문자열 맨 앞을 가리키는 포인터, len, 그리고 capacity를 스택에 담는 구조이다. len은 문자열의 길이를 뜻하고 capacity는 바이트로 표시한 컨텐츠의 메모리 크기를 의미한다. 따라서 s2 식별자에 s1을 할당하는 동작은 앞서 정수로 설명한 경우와 동일하게 위 세 개 정보를 스택에 복사하는 것과 같다. 힙에 있는 데이터는 복제되지 않는다. 우리는 앞서 Rust의 동작 중에 식별자가 스코프에 벗어나면 사용되던 값들을 모두 정리한다고 배웠는데, 위와 같은 구조에서는 문자열이 두 번(s1의 Drop &amp;&amp; s2의 Drop) 정리되는 상황이 생긴다. 즉, 두 번 free를 하는 것과 같고 이는 메모리 충돌을 발생시킨다. 메모리 안전성을 위해서 러스트는 이런 상황에서 s1이 더 이상 유효하지 않다고 판단해버린다. 즉 말해서 러스트는s1이 스코프를 벗어나든 아니든 Drop과 관련된 로직을 수행할 필요가 없다. 다른 언어에서는 이렇게 값을 복사하는 과정에 포인터 내부의 값을 복사하지 않는 것을 얕은 복사라고 표현한다. Rust는 이 동작이 모든 데이터에 적용된다. 자동으로 깊은 복사를 수행하는 경우가 없다. 책에서 설명하는 방식으로는 Rust에서 이런 동작을 move라고 한다는데, 한국어로는 이전이라고 표현하면 될 것 같다. 조금 명시적으로 말하자면 힙 포인터를 얕은 복사하는 경우 발생하는 동작이 “이전”이라고 표현할 수 있을 것 같다. Copy &amp; Clone힙에 있는 데이터까지 복제하는 작업을 할 때는 일반적으로 Clone이라는 Trait을 구현한다. String 타입도 마찬가지로 Clone 타입을 구현하고 있다. 1234567891011#[cfg(not(no_global_oom_handling))]#[stable(feature = &quot;rust1&quot;, since = &quot;1.0.0&quot;)]impl Clone for String { fn clone(&amp;self) -&gt; Self { String { vec: self.vec.clone() } } fn clone_from(&amp;mut self, source: &amp;Self) { self.vec.clone_from(&amp;source.vec); }} 123456fn main() { let s1 = String::from(&quot;hello&quot;); let s2 = s1.clone(); println!(&quot;s1 = {}, s2 = {}&quot;, s1, s2);} 위 동작은 다음 그림처럼 힙 데이터 역시 복사해서 새로운 식별자인 s2에 담는다. s1의 소유권은 그대로 유지된다. 다시 스택에 할당되는 값만 가진 이 코드로 돌아와 보자. 12345let mut x = 1;let mut y = x;x += 1;y -= 1;println!(&quot;x: {} / y: {}&quot;, x, y); // 2, 0 스택에 할당되는 값의 경우 할당 연산을 수행할 때 소유권 이전이 발생하지 않고 값을 복사해버린다. 여기서는 얕은 복사니 깊은 복사니 하는 것이 의미가 없다. 이렇게 소유권 이전 없이 값을 간단히 스택에서 복사해버릴 수 있는 타입들은 모두 Copy Trait을 구현하고 있다. Copy 타입은 러스트 시스템의 특별한 어노테이션으로서, 만약 이 Trait을 구현하고 있는 타입이라면 할당 연산을 할 때 소유권을 이전하지 않는다. 12let s1 = String::from(&quot;hello&quot;); -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait 아까 에러 메시지를 다시 한번 보면, String은 Copy Trait을 구현하고 있지 않기 때문에 move가 발생한다고 설명한다. Copy 타입을 직접 구현하도록 할 수 있는데, 등호 연산을 오버로딩하는 느낌이 아니라 그냥 스택에서 값을 복사할 수 있는 타입의 경우 그 자격을 명시하는 정도이다. Copy를 구현하려면 대상 타입이 Clone을 구현하고, 그 타입 자체 혹은 타입을 구성하는 다른 필드들 모두가 Drop Trait을 구현하고 있지 않아야 한다. 즉, 타입을 구성하는 모든 필드 및 값이 스택에 할당될 수 있어야 한다는 것을 의미한다. 12345678910impl Copy for MyType {}impl Clone for MyType { fn clone(&amp;self) -&gt; Self { *self }}// or#[derive(Copy, Clone)]struct MyStruct; Copy Trait은 위 코드처럼, 구현해야 하는 메소드가 없다. 이는 러스트가 의도적으로 오버로딩을 구현하지 못하도록 막은 것이고, 이를 통해 임의 코드가 런타임에서 실행되지 않도록 막는다. Copy가 구현될 수 있는 규칙을 설명할 때 책에서는 위와 같이 Clone + Not Drop으로 설명하지만, 코드 주석에서는 필드가 모두 Copy를 구현해야 한다고 설명한다. 즉, 기본 타입들은 모두 Not Drop이라면 Copy를 구현하고 있는 것 같다. 위 코드에서 Copy를 구현하는 두 가지 방법은 미묘한 차이가 있는데, 이번 글의 범위를 벗어난다. 궁금하다면 Derivable Traits 문서와 Rust Copy 코드의 주석을 보자. Copy Trait을 직접 구현해야 하는 일은 거의 드물다. Copy가 구현되어 있다면 최적화가 되어있고 clone 메소드가 아니라 할당 연산자를 사용할 수 있음을 의미하므로 코드가 더 간결해질 수는 있다. Copy를 구현하고 있는 기본 타입들은 다음과 같다. 모든 정수형 타입들 (u32, u16, …) Boolean 타입 부동소수 타입 (f64, …) 모든 캐릭터 타입 (char) Copy 구현체들을 담고 있는 튜플 ((i32, i32)) 함수와 Ownership함수 파라미터로 값을 넘기는 것이나 리턴 값으로 넘기는 것 모두 할당 연산과 비슷한 동작을 한다. 할당과 마찬가지로 값을 복사하기 때문에 파라미터로 Copy를 구현하지 않은 값을 넣거나, 리턴하는 경우 소유권 이전이 발생한다. 1234567891011121314151617181920212223242526272829struct MyType { v: u8,}impl MyType { fn from(v: u8) -&gt; Self { MyType { v } }}impl Drop for MyType { fn drop(&amp;mut self) { println!(&quot;drop my type&quot;) }}fn main() { let s = MyType::from(1); take_ownership(s); println!(&quot;finish&quot;);}fn take_ownership(param: MyType) { println!(&quot;{}&quot;, param.v)}// 1// drop my type// finish s는 take_ownership 함수에 넘겨질 때 소유권 이전이 발생한다. 따라서 take_ownership 이후로는 접근이 불가능하다. take_ownership 함수가 끝날 때 해당 변수의 스코프가 종료되므로 drop 함수를 수행한다. 리턴하는 값이 Drop을 구현하고 있는 경우도 마찬가지로 리턴 값에 대한 소유권이 할당받는 식별자에게 넘어가게 된다. 따라서 같은 스코프에 변경된 값을 유지하고 싶으면 다시 함수에서 바깥으로 소유권을 전달해야 한다. 12345678fn main() { let s = String::from(&quot;hello&quot;); let (s, slen) = calculate_length(s);}fn calculate_length(s: String) -&gt; (String, usize) { (s, s.len())} 이러한 동작이 사실 매우 귀찮기 때문에 러스트에서는 값은 참조하지만, 소유권은 넘겨주지 않는 방법으로 함수를 사용할 수 있도록 했다. 참조와 소유권 대여위와 같은 상황에서 소유권을 넘기지 않으려면 참조(Reference)를 넘긴다. 원본 데이터에 접근하지 않고 스택의 데이터를 참조하는 자료형으로 파라미터에 전달된다. 참조형으로 전달된 값은 기본적으로 불변 자료형이고, 참조하는 식별자이기 때문에 식별자 스코프가 종료되어도 Drop을 수행하지 않는다. 123456789fn main() { let s = String::from(&quot;hello&quot;); let len = calculate_length(&amp;s); println!(&quot;{} length: {}&quot;, s, len); // hello length: 5}fn calculate_length(s: &amp;String) -&gt; usize { s.len()} 위 코드처럼, s 식별자를 다른 함수에 참조형으로 전달하게 되면 해당 함수 이후에도 s를 사용할 수 있고, 해당 값을 유지하기 위한 리턴도 필요 없게 된다. 다만 다음과 같이 값을 변경하려고 하는 경우는 컴파일 에러가 발생한다. 1234fn change(s: &amp;String) { s.push_str(&quot;, world&quot;); // compile error} 위에서 짧게 언급한 것처럼 기본적으로 참조형이 불변형(Immutable Reference)이기 때문이다. 원래 러스트는 기본적으로 값을 불변형으로 선언한다. let으로 선언된 모든 식별자는 불변 식별자이다. 만약 값을 바꾸려면 mut 키워드를 식별자 앞에 선언해서 해당 식별자가 가변적임을 컴파일러에 알려야 한다. 참조한 변수를 가변적(Mutable Reference)으로 사용하려면 다른 변수들처럼 mut 키워드를 사용한다. 1234567891011fn main() { // 여기도 mutable 한 변수가 되도록 `mut` 키워드를 사용한다. let mut s = String::from(&quot;hello&quot;); let len = calculate_length(&amp;s); change(&amp;mut s); println!(&quot;{} length: {}&quot;, s, len); // hello, world length: 5}fn change(s: &amp;mut String) { s.push_str(&quot;, world&quot;);} 러스트는 참조형을 사용할 때 두 가지 안전 장치가 있다. Data Race를 방지하기 위한 특징 쓰레깃값을 만들지 않기 위한 특징 Mutable Reference Data Race 방지일단 Data Race는 다음과 같은 상황을 얘기한다. 두 개 이상의 포인터가 같은 데이터에 접근 가능 최소 하나의 포인터가 데이터 쓰기가 가능 데이터 접근을 동기화하는 메커니즘 부재 위 세 개의 상황이 동시에 발생하고 있을 때 Data Race 상태라고 볼 수 있다. 동시성 문제의 Race Condition과 유사한데, 데이터 변경과 읽기 순서에 따라 결과가 달라질 수 있는 상황이다. 이 문제는 미묘한 상황과 복잡한 코드에 의해 런타임에 찾기가 어려운 경우가 많지만, 러스트는 뚱뚱한 컴파일러를 지향하는 언어답게 사전에 컴파일 단계에서 이를 방지해준다. 방지하는 방법은 Reference를 전달할 때 Shared Lock, Exclusive Lock을 거는 것처럼 동작을 제한한다. Mutable Reference의 스코프를 벗어나기 전까지 Exclusive Lock처럼 다른 Reference가 걸리는 것을 막는다. 읽기 전용 Reference는 Shared Lock처럼 다른 읽기 전용 Reference가 걸리는 것은 막지 않는다. 하지만 Mutable Reference는 동시에 사용될 수 없다. 123456fn main() { let mut s = String::from(&quot;hello&quot;); let ref1 = &amp;mut s; let ref2 = &amp;mut s; // compile error println!(&quot;{} {}&quot;, ref1, ref2)} 다음과 같이 Mutable Reference를 동시에 Borrow 해줄 수 없다는 에러가 나온다. Immutable Reference가 먼저 있어도 비슷한 컴파일 에러가 발생한다. 123456789error[E0499]: cannot borrow `s` as mutable more than once at a time --&gt; src/main.rs:27:20 |26 | let ref1 = &amp;mut s; | ------ first mutable borrow occurs here27 | let ref2 = &amp;mut s; | ^^^^^^ second mutable borrow occurs here28 | println!(&quot;{} {}&quot;, ref1, ref2) | ---- first borrow later used here 참조형의 스코프는 마지막으로 사용된 시점까지 유지되므로 다음과 같은 경우는 문제없이 코드가 동작한다. 12345678fn main() { let mut s = String::from(&quot;hello&quot;); let ref1 = &amp;s; println!(&quot;{}&quot;, ref1); // hello // 컴파일러는 여기서 `ref1` 스코프가 종료된다고 판단한다. let ref2 = &amp;mut s; change(ref2); println!(&quot;{}&quot;, ref2); // hello, world} Dangling ReferenceDangling Reference는 free 된 레퍼런스를 식별자로 가지고 있는 경우를 말한다. 러스트에서는 이를 컴파일러가 절대 Dangling Reference를 가지고 있지 않도록 보장해준다. 만약 어떤 데이터가 참조되고 있다면 해당 참조 식별자의 스코프가 종료되기 전까지 데이터의 스코프가 끝나지 않도록 해야 한다. 12345678fn main() { let reference_to_nothing = dangle();}fn dangle() -&gt; &amp;String { let s = String::from(&quot;hello&quot;); &amp;s} 12345678910111213141516$ cargo run Compiling ownership v0.1.0 (file:///projects/ownership)error[E0106]: missing lifetime specifier --&gt; src/main.rs:5:16 |5 | fn dangle() -&gt; &amp;String { | ^ expected named lifetime parameter | = help: this function's return type contains a borrowed value, but there is no value for it to be borrowed fromhelp: consider using the `'static` lifetime |5 | fn dangle() -&gt; &amp;'static String { | ~~~~~~~~For more information about this error, try `rustc --explain E0106`.error: could not compile `ownership` due to previous error 라이프타임 어노테이션에 대해서는 이 글에서 다루지 않는다. 자세한 내용은 이 링크에서 확인하면 좋을 것 같다. Reference https://doc.rust-lang.org/book/appendix-03-derivable-traits.html https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html","link":"/posts/rust/rust-ownership/"},{"title":"서버리스로 CronJob 만들기 (코로나 크롤러)","text":"현재 진행하고 있는 프로젝트에서는 크로나 현황을 간단하게 보여주는 섹션이 존재하는데, 이 부분을 누군가가 (API 형식으로다가?) 제공해주고 있는 것으로 알고 있었다. 그렇지만 아직 나온 건 없었고, 어쩔 수 없이 간단한 크롤러를 만들고 업데이트 해주기로 했다. 현재 프로젝트가 서버리스로 돌고 있기도 하고, Scheduled 된 작업을 돌리기 위해서는 람다가 적합하지 않을라나 싶어서 같은 서버리스 프로젝트에서 크론잡을 돌리는 함수를 만들어보기로 했다. 이 글에서 정리하고 싶은 건, DynamoDB 리소스를 정해주는 작업, 크론잡을 만드는 것 정도가 될 것 같다. 프로젝트가 팀의 큰 프로젝트의 일부라, 그 외 디테일한 구조라든지, 세세한 과정을 모두 생략했다. 자세한 튜토리얼을 기대하고 읽으시는 분들은 원하는 만큼의 정보를 얻지 못할 것 같고, 간단하게 플로우를 확인하고자 하시는 분들에게 적절한 도움을 드릴 수 있지 않을까 싶다. 아마 서버리스로 간단한 TODO리스트 만들기를 데모로 처음부터 끝까지 써보려고 하는데, 그 글에서 자세한 서버리스 튜토리얼을 볼 수 있을 것 같다. 크롤링하기이 부분은 네이버 검색에 나오는 부분을 크롤링 하기로 했다. 정확하게 우리가 원하는 데이터만 보여주는 화면이 있어서, 그 화면을 크롤링 해서 숫자만 가져가려고 한다. 크롤러는 cheerio를 사용했다. 크롤링 하는 부분은 이번 주제의 핵심이 아니니 간단하게 코드만 보고 가자. 123456789101112131415// src/services/utils/index.tsimport * as cheerio from &quot;cheerio&quot;;import Axios from &quot;axios&quot;;import { ScheduledEvent } from &quot;aws-lambda&quot;;import * as CoronaStatusModel from &quot;../../models/coronaStatusModel&quot;;import { getCoronaData } from &quot;../../utils&quot;;//...export const cronCrawler = async (event: ScheduledEvent) =&gt; { const data = await getCoronaData(event.time); const latest = await CoronaStatusModel.getLatest(); if (latest) await CoronaStatusModel.updateLatest(latest.date, event.time); await CoronaStatusModel.create(data as CoronaStatusTableFields);}; 우선, getCoronaData 함수에서는 테이블에 들어갈 데이터를 만들어온다. 이 부분에서 크롤링이 동작한다. 그 다음 현재 사용되고 있던 latest 버전을 찾아온 다음, usage 필드를 교체되는 시간으로 바꿔준다. 위와 같이 동작하게 한 이유는 다이나모디비 사용에 미숙한 이유가 있다. Date로 내림차순 한 다음, 맨 첫 번째 값만 가져오면 되는데, 이 부분을 아무리 찾아봐도 디테일한 레퍼런스를 찾지 못해서 (정말 흔한 케이스임에도 불구하고…), 특히 다이나모디비에서 오더링에 관한 내용이 LocalSecondaryIndexes를 설정해줘야 하는 걸로 나오게 되는데, 추가적인 비용이 나오기도 하고, 딱 이 기능 하나로 그 부분을 추가해야 하나 싶기도 하고 해서 … (변명이지만, 더 깊게 공부할 타이밍이 아니였다.) 아무튼 위와 같은 방식으로 latest를 항상 하나로 유지하게 하는 것으로 이후, 최근 값을 찾기 위해서는 usage = &quot;latest&quot;인 값만 찾게 했다. CoronaStatusModel은 나중에 살펴보도록 하자. cheerio가 동작하는 getCoronaData 함수는 다음과 같다. 1234567891011121314151617181920212223242526272829303132333435363738394041export const getCoronaData = async ( date: string): Promise&lt;CoronaStatusTableFields&gt; =&gt; { const { data: html } = await Axios.get( &quot;https://search.naver.com/search.naver?sm=top_hty&amp;fbm=1&amp;ie=utf8&quot;, { params: { query: &quot;코로나&quot; } } ); const translateKeys: { [P in keyof CoronaStatusRawData]: keyof CoronaStatusTableFields; } = { 검사진행: &quot;inspected&quot;, 격리해제: &quot;isolationReleased&quot;, 사망자: &quot;dead&quot;, 확진환자: &quot;confirmed&quot; }; const $ = cheerio.load(html); const $boxList = $(&quot;div.graph_view&quot;).children(&quot;.box&quot;); const data = { usage: &quot;latest&quot;, date }; $boxList.map((_index, box) =&gt; { const key = $(box) .find(&quot;p span.txt_sort&quot;) .text(); const value = $(box) .find(&quot;p strong.num&quot;) .text(); const numValue = Number(value.split(&quot;,&quot;).join(&quot;&quot;)); data[translateKeys[key]] = numValue; }); return data as CoronaStatusTableFields;}; Functions 설정하기이 부분에서 Schedule expression을 사용해서 특정 시간마다 함수가 돌 수 있게 만들어야 한다. Schedule expression의 문법은 크게 두 가지가 있다. 하나는 rate를 사용하는 것이고, 다른 하나는 cron을 사용하는 것이다. rate이 문법은 특정한 주기를 설정할 수가 있게 된다 (분, 시간, 일 등 단위마다 시작하는 형식으로). 기본적으로 형태는 rate(value unit) 형태를 띄고, rate를 사용하는 방법은 아래와 같다. 1234- events: - schedule: rate(15 minutes) - schedule: rate(1 hour) - schedule: rate(2 days) 위와 같이 값과 유닛단위를 적어줘야 하는데, 지원하고 있는 유닛들은 아래와 같다. minute or minutes hour or hours day or days 값에 따라 복수형 단수형을 지켜 써줘야 한다고 한다 (아마 안될 것 같지는 않다). cron이 문법은 조금 더 복잡한 방식의 리눅스의 crontab 문법을 차용하고 있다고 한다. cron(minute hour day-of-month month day-of-week year) 형식의 문법을 사용한다. 쉼표를 가지고 여러 값을 넣을 수도 있고, 와일드카드(?, * 등)를 사용할 수 있다. 각 필드에 대한 타입과 와일드 카드들은 아래와 같다. Field Values WildCards Minutes 0-59 , - * / Hours 0-23 , - * / Day-of-month 1-31 , - * ? / L W Month 1-12 or JAN-DEC , - * / Day-of-week 1-7 or SUN-SAT , - * ? L # Year 1970-2199 , - * / 와일드카드들의 지원과 제한 등에 대해서는 여기 아마존 링크를 확인해보자. 영어로밖에 지원을 안해주는 페이지다. 아래는 cron의 예시이다. 123456- event: - schedule: cron(15 3 ? * MON *) # 월요일 3:15AM 마다 도는 함수 - schedule: cron(1/10 * ? * W *) # 주말 제외 시작하고나서 10분 마다 도는 함수 # (run in 10-minute increments from the start of the hour on all working days) - schedule: cron(0 18 * * ? *) # 매일 18:00에 동작하는 함수 위 예시에서 계속 등장하는 몇 가지 와일드 카드들은 아래와 같은 의미가 있다. *: 해당 필드의 모든 것을 의미한다. 예를 들어서 Hours Field의 *는, ‘모든 시간에 대해서’ 라는 의미를 갖는다. 이 와일드 카드는 day-of-week 필드와 day-of-month 둘 다에 사용할 수는 없고, 둘 중 하나는 반드시 ?를 써야 한다. ?: 해석을 하자니 모호해서… 우선 구체적인 예시는, day-of-month에 7을 넣고, 그 7일이 무슨 요일이든 상관이 없으면 day-of-week에 ?를 쓸 수 있다. 일단 내가 작업하고 있는 일은 매일 오전 10시, 오후 5시에 작업이 돌아야 한다. cron 문법을 사용하는 것이 좋겠다. 다만 cron의 시간이 UTC만 지원하기 때문에 원하는 시간에서 9시간을 뺀 값으로 적어줘야 한다. 123456# functions.ymlCronCoronaCrawler: handler: ./src/services/utils/index.cronCrawler events: - schedule: cron(0 1,8 * * ? *) # 10시, 17시 이제 이 functions.yml을 serverless.yml의 functions 프로퍼티 쪽에 리스트 형식으로 ${file(path/to/functions.yml)} 이런 식으로 붙여주면 된다. Resource 설정하기데이터베이스를 붙여주고 CoronaStatusModel에서 사용된 함수들만 만들어주면 되겠다. (이미 프로젝트에서는 만들어둔 적이 있어서, 위에서 이미 사용된 체로 코드를 올렸다.) 다이나모디비를 붙여주자. tables를 아래와 같이 설정해줬다. 12345678910111213# tables.ymlCoronaStatus: Type: AWS::DynamoDB::Table DeletionPolicy: Retain Properties: TableName: ${self:custom.TABLE_PREFIX}corona-status AttributeDefinitions: - AttributeName: date AttributeType: S KeySchema: - AttributeName: date KeyType: RANGE AttributeDefinition은 KeySchema에 정의되어야 하는 필드만 넣어두었다. ProvisionedThroughput은 BillingMode가 PROVISIONED인 경우에는 반드시 정해줘야 하고 PAY_PER_REQUEST인 경우엔 정할 수 없다고 한다. 기본 값은 PROVISIONED이기 때문에 읽기는 초당 15, 쓰기는 초당 5로 지정했다. 마찬가지로, 이 yml 파일은 serverless.yml에 붙여준다. 12345#...resources: Resources: ${file(./migrations/tables.yml)}#... CoronaStatusModel을 짠 곳이다. 123456789101112131415161718192021222324252627282930313233343536373839404142import { dynamodb } from &quot;../common/aws&quot;;import { CORONA_STATUS_TABLE } from &quot;../databases/dynamodb&quot;;export const create = (data: CoronaStatusTableFields) =&gt; dynamodb .put({ TableName: CORONA_STATUS_TABLE, Item: data }) .promise();export const getLatest = async (): Promise&lt;CoronaStatusTableFields&gt; =&gt; { const { Items } = await dynamodb .scan({ TableName: CORONA_STATUS_TABLE, FilterExpression: &quot;#u = :usg&quot;, ExpressionAttributeNames: { &quot;#u&quot;: &quot;usage&quot; }, ExpressionAttributeValues: { &quot;:usg&quot;: &quot;latest&quot; } }) .promise(); return Items[0] as CoronaStatusTableFields;};export const updateLatest = (date: string, usage: string) =&gt; dynamodb .update({ TableName: CORONA_STATUS_TABLE, Key: { date: date }, UpdateExpression: &quot;SET #u = :usage&quot;, ExpressionAttributeValues: { &quot;:usage&quot;: usage }, ExpressionAttributeNames: { &quot;#u&quot;: &quot;usage&quot; } }) .promise(); 이제는 동작 해야 하는데, 테스트 하기 위해서 엔드포인트로 들어갔을 때 크롤링을 해서 데이터를 넣는 람다 함수도 만들어서 테스트를 해봤다. 서버리스 배포는 간단하게 sls deploy --stage development로 할 수 있다. 후기서버리스가 크론잡을 돌리기엔 정말 간단하게 만들 수 있어서 좋았다. 앞으로도 이런 식으로 크론잡으로 데이터 모으는 프로그램은 별도로 람다를 만들 것 같다. 그렇지만 다이나모디비에 대해서는 그렇게 좋지 못한 경험이었다. 정말 간단한 것이지만 삽질한 것도 있고, 테이블 설계가 일반적이지 않아서 상당히 애를 먹었다. 물론 몇 번 더 시도 해보고 Secondary Indexes 등 구성해보는 시도를 하겠지만, 특별히 이점을 가져다주는 게 있는지 모르겠다. RDS를 연결 할 때 VPC 등 네트워크 설정을 해줘야 한다는 점? 그냥 그런식으로 연결 해주는게 오히려 정신 건강에 좋겠다 싶은 수준이었다. 물론 프로덕트에서도 많이들 쓰고 있는 것을 봤지만, 실제 메인 서비스에서 핵심 데이터베이스로 사용하고 있는 팀이 있는지 궁금하다. 다이나모디비는 여러번 더 봐야 할 필요를 느꼈다… 앞으로 몇 가지 태스크를 다이나모디비로 깨야 하는데 벌써 피로감이 드는 것 같다. Reference https://serverless.com/blog/cron-jobs-on-aws/ https://docs.aws.amazon.com/ko_kr/AmazonCloudWatch/latest/events/ScheduledEvents.html https://iseongho.github.io/posts/cron-expression/ https://docs.aws.amazon.com/ko_kr/AWSCloudFormation/latest/UserGuide/aws-properties-dynamodb-keyschema.html https://docs.aws.amazon.com/ko_kr/AWSCloudFormation/latest/UserGuide/aws-resource-dynamodb-table.html https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/SecondaryIndexes.html","link":"/posts/serverless/serverless-cronjob-crawler-demo/"},{"title":"서버리스 배포 베스트 프랙티스(번역)","text":"written by Fernando Medina Corey 이 튜토리얼의 대부분을 만들기 위해서는 Serverless Framework’s의 대시보드에 로그인 해야 한다: https://dashboard.serverless.com. 본인은 글을 모두 읽은 다음에 이 주석을 달고 있다. 베스트 프랙티스를 실행할 방법을 알려준 다음 Safeguard라는 Serverless Framework의 대시보드, 기타 등등 대시보드 기능으로 베스트 프랙티스를 실행하는 내용을 알려준다. 문제를 먼저 알려주고 해결하고 싶으면 우리 제품을 써보는 게 어떻냐는 약팔이 느낌도 있긴 하다. Overview너가 지속적으로 서버리스 애플리케이션들을 만들 때, 앱들의 복잡성과 범위는 계속 증가할 것이다. 그 성장은 버그를 최소화 하고, 앱의 보안을 유지하고, 개발을 더 빠르게 만들어주는 well-structured 된 practices들이 필요하게 만든다. 이 포스트는 다양한 서버리스 최선의 배포 방식들을 보여줄 것이다. 우리가 그것들을 복습하게 되면, 나는 또한 어떻게 너가 쉽게 이러한 케이스들을 너만의 서버리스 프레임워크 앱에 적용하기 위해 새로운 대시보드 Safeguards를 쓸 것인지 보여줄 것이다. 서버리스 대시보드에 익숙하지 않다면 이 문서를 참조해라 그럼, 몇 가지 배포의 Best Practices를 확인해보자. 보안적절하게 비밀키를 관리해라API 키, 데이터베이스 Credentials, 또는 다른 secrets들은 안전하게 저장되고, 안전하게 애플리케이션에 의해 접근될 필요가 있다. 몇가지 다양한 방법이 있는데 가장 중요한 것은 다음과 같다. 너의 secrets을 버전 관리(source control) 밖에 있도록 해라 secrets에 접근을 제한해라 (최소 권한의 원칙) 애플리케이션의 다른 스테이지에 대해 secrets를 분리해라 우리는 이전에 서버리스 프레임워크를 사용할 때 secrets를 관리하는 몇 가지 방법들을 논의한 적이 있는데, 이것은 아마 너에게 좋은 옵션이 될 것이다. 더 최근에는 우리는 너의 secrets를 다양한 서비스들, AWS 계정들, 애플리케이션의 스테이지를 포괄하는 관리를 할 수 있도록 Parameters를 붙였다. 너는 또한 애플리케이션의 serverless.yml에 환경 변수가 일반 텍스트로 설정되었을 때 배포가 안되도록 막기 위해 Safeguard polices를 사용할 수도 있다. IAM 정책 허용을 제한하라또 다른 중요한 best practice는 너가 너의 애플리케이션들의 허용 범위를 제한하는 것이다. AWS의 경우, 너가 너의 서비스들을 위한 IAM 정책들을 만들 때, 역할들을 애플리케이션을 수행할 수 있는 최소한의 권한만 줘야 한다. 이 부분의 일부로, 너는 와일드 카드(*: asterisk) 사용하는 것을 줄여야 한다. Safeguard policy를 활용하면, 와일드카드가 담긴 IAM permissions를 제한할 수 있다. 이 Safeguard policy를 통해 배포 자체를 막을 수도 있지만, 개발자에게 다른 방법을 찾아보라고 경고를 주는 정도로 할 수도 있다. 배포 시간을 제한해라너가 블랙프라이데이 러쉬를 준비하는 이커머스 팀인 것을 상상해보자. 너는 너의 코드의 상태를 확신하지만, 너는 바쁜 기간의 새로운 버그가 생길 작은 가능성조차 제한하고 싶을 것이다. 한 가지 일반적인 방법은 이 기간 동안에는 너의 배포를 중단하고 걸어 잠그는 것이다. 주말에 전화 알람을 받고 싶지 않은 조직도 비슷한 상황이 발생하므로, 그들은 금요일과 월요일 오전에는 배포를 중단할 수 있다. Safeguard는 너의 환경에 이 정책을 적용할 수 있게 해준다. 일관적인 컨밴션들Stages컨벤션 락. 그들은 개발자들이 기준점을 배우고, 직관적으로 시스템의 다른 부분들을 이해하도록 돕는다. 어디에나 있는 개발 컨벤션 중 하나는 소비자들이 보는 코드(프로덕션 스테이지)와 개발자들이 작업하는 코드(개발 스테이지)를 분리하는 것이다. 이렇게 스테이지를 분리하는 것은 너의 코드가 일관적인 방법으로 고객에게 이동하게 된다 (개발 스테이지 -&gt; 프로덕션 스테이지). 서버리스 프레임워크에서는 기본적으로 너가 작업한 내용들이 너의 애플리케이션들이 dev 스테이지에 푸시된다. 그 다음 production을 위한 배포 준비가 됐을 때 serverless.yml을 업데이트 하는 것을 통해서 prod와 같은 스테이지로 배포할 수 있게 된다. 각 스테이지들에서, 너는 아마 아주 다른 설정들을 사용하고 싶을 것이다. 다행히도, 스테이지들과 상호작용할 때 너가 서버리스 대시보드로 할 수 있는 새로운 세분성들이 있다. 스테이지당 설정들은 아래와 같은 것들이 포함될 수 있다. 배포를 해야하는 AWS 계정, 지역, 스테이지들 배포에 대해서 Safeguard가 평가해야 하는 것들 어떤 파라메타들, 비밀키들이 사용되었는지 Safeguard를 사용해서 개발 단계의 배포를 차단하는 것 부터, 프로덕션 스테이지의 AWS 계정 까지 범위의 것들을 수행(설정 하는 것들)하거나, 너의 프로덕션 API 키들이 항상 Production에 묶이도록 할 수 있다. 이러한 옵션들은 필요와 워크플로우에 따라 너에게 지원을 주는 것에 대해서 아주 유연하게 된다. 승인된 지역들지역적으로 분산된 팀에서 일하고 있을 때, 각 리전의 개발자들을 위한 기본적인 AWS 리전은 같지 않을 것이다. 시애틀에 있는 사람은 us-west-2에 잇고, 필라델피아에 있는 사람은 us-east-1을 사용할 것이다. 독립적으로 개발하고 있다고 하면, 이러한 차이들은 부주의에 따라서 어떤 이슈를 만들게 될 것이다. 하나의 서비스는 하나의 리전을 아마 참조하고 있다. 하지만 실제로 배포는 각자 될 필요가 있다. 또는, 다양한 리전들은 아마도 그에 맞게 지원되는 특징들, 제한들이 있을 수 있다. 위와 같은 이슈들을 피하기 위해서, 우리는 개발자들에게 하나의 지역 또는 하나의 지역의 서브넷만을 사용할 것을 요구한다. 그리고 당연히, Safeguard가 이런 보호막을 해줄 수 있다. (역주: … 기승전 Safeguard) 배포 시간에 너의 서비스가 특정 리전, 또는 리전들의 리스트에 배포되도록 제한해준다. 함수 이름과 설명들스테이지와 리전 컨트롤의 조합 속에서, 너의 인프라 안에서 일관적인 이름과 설명들을 유지하는 것은 새로운 개발자들이 빠르게 서비스들을 파악하고, 어떻게 다른 앱 스테이지에 그것들을 연결할지 이해하고, 그리고 더 쉽게 빌드하고 디버깅할 수 있도록 도울 수 있다. 하나의 일반적인 패턴으로는 너의 람다 함수들이 모두 일관적으로 서비스 이름, 스테이지 그리고 함수 이름을 갖고 있는 것이다. 이것은 만약 그들이 같은 AWS 계정에 있다면 너에게 더 쉽게 관련된 함수들을 찾을 수 있게 한다. 그리고 더욱 빠르게 여러 함수들을 특정 서비스에 묶을 수 있도록 해준다. 유저에게서 컨텐츠를 제출받고 처리하고, 데이터베이스에 저장하고 인덱싱하는 하나의 서비스를 상상해보자. 그것은 아마 하나의 람다 함수가 제출에 대해 승인/거절하고 DynamoDB에 저장한다. 그리고 다른 하나는 ElasticSearch에 인덱싱한다. 만약 너가 이러한 간단한 아키텍쳐를 취하고, 개발, 운영 환경 모두에 퍼져있다고 하면, 너는 이미 네 개의 함수를 가지고 있는 것이다. 우리가 람다 함수 네이밍 컨벤션(serviceName-stage-fucntionName)을 따르고 있을 때, 위와 같이 하는 것을 더 쉽게 만든다. 이런 상황에서 네이밍은 다음과 같이 될 것이다. newSubmissions-prod-submissionGrader newSubmissions-prod-elasticsearchIndexer newSubmissions-dev-submissionGrader newSubmissions-dev-elasticsearchIndexer 이 방법으로 너는 정확하게 어떤 함수가 너가 필요한 것인지 알 수 있다. 만약 너가 새로운 개발자가 부적절하게 이름 지어진 서비스를 배포하지 않을까 걱정하고 싶지 않다면, 마찬가지로 Safeguard를 사용해 네이밍을 제한할 수 있다. 후기Safeguard 광고를 번역했다. 유료 서비스는 아니라고 한다. 디테일한 프랙티스, 조금 Advanced한 Serverless Framework의 베스트 프랙티스를 위한 설정들 등을 기대하면서 읽었는데 아쉬움이 크다. 마지막에 Takeaways라는 “Safeguard와 우리 대시보드의 기능은 이게 다가 아니야~” 라는 내용이 있는데 이 부분은 그냥 생략했다. 도커 베스트 프랙티스와는 비교할 수 없는 의미없는 번역이 된 것 같다. 원본https://serverless.com/blog/serverless-deployment-best-practices/","link":"/posts/serverless/serverless-deployment-best-practices/"},{"title":"서버리스 아키텍처 패턴","text":"이 글은 AWS 기반 서버리스 아키텍처 책 중에 서버리스 아키텍처 패턴 부분을 간단하게 정리한 글이다. 앞으로 소개되는 패턴들은 서버리스 아키텍처에 적용 가능 한 것이라고 배타적으로 표현하지 않아도 된다. 이 패턴은 서버리스 이전에 분산 시스템에서 사용되던 패턴이다. 또한 여기서 정리한 패턴 말고도, 인증과 관련된 패턴, 데이터 관리(CQRS, 이벤트 소싱, 구체화된 뷰, 샤딩) 및 오류 처리와 관련된 패턴(재시도 패턴)을 잘 알고 있는 것이 좋다. 명령 패턴소프트웨어 엔지니어링에서 명령 패턴(command pattern)은 요청되는 작업 또는 요청 수신자에 대해 알지 못해도 객체에 요청할 필요가 있기 때문에 요청을 객체로 캡슐화 해서 여러 다른 요청을 가진 클라이언트를 매개 변수화하고, 요청을 큐에 넣거나 로그로 남기고, 실행 취소 가능한 작업을 지원할 수 있다. 명령 패턴에 대한 자세한 설명은 여기 링크에서 확인할 수 있다. 이 패턴은 발신자와 수신자를 분리하려는 경우에 유용하다 (요청 발신자와 수신자 사이에 명령을 받고 전달해주는 중간자를 추가하는 것이기 때문에). 단일 함수에서 여러 함수와 서비스를 호출하고 제어하는 데 사용한다. 인수를 객체로 전달하고 클라이언트가 여러 다른 요청으로 매개 변수화 될 수 있게 해 구성 요소 간의 결합을 줄이고 시스템을 더욱 확장 가능하게 만들 수 있다. API Gateway로 응답을 돌려줘야 하는 경우라면 이 접근 방법을 사용한다. 메시징 패턴메시징 패턴은 개발자가 함수 및 서비스 사이에 직접적인 의존성을 분리하고 큐에 이벤트 / 레코드 요청을 저장할 수 있게 해 확장 가능하고 견고한 시스템을 구축할 수 있게 했으므로 분산 시스템에서 널리 사용된다. 큐에 있는 메시지를 처리하는 서비스가 오프라인 상태가 되면 메시지는 큐에 보관되어 나중에 처리될 수 있기 때문에 신뢰성이 제공된다. 이 패턴에는 큐에 게시할 수 있는 발신자와 큐에서 메시지를 읽을 수 있는 수신자가 있는 메시지 큐가 있다. AWS에서는 Simple Queue Service로 이 패턴을 구현할 수 있다. 여기 링크에서는 SQS가 Lambda의 트리거 역할을 하도록 하는 예시가 있다. 시스템 설계 방법에 따라서, 메시지 큐는 [단일 발신자 / 수신자] 또는 [다중 발신자 / 수신자]를 가질 수 있다. 위 링크에 SQS에서는 아래와 같은 참고 사항을 적어뒀다. 대기열과 Lambda 함수는 같은 AWS 리전에 있어야 합니다. FIFO 대기열은 Lambda 함수 트리거를 지원하지 않습니다. 하나 이상의 Lambda 함수에 하나의 대기열만 연결할 수 있습니다. Amazon SQS용 AWS 관리형 고객 마스터 키를 사용하는 암호화 대기열을 다른 AWS 계정의 Lambda 함수와 연결할 수 없습니다. 위 내용을 보면 SQS는 하나 이상의 수신자를 붙일 수 있는 것 같다. 이 패턴은 워크로드 및 데이터 처리를 다루는 데 사용되기 좋다. 큐가 버퍼 역할을 하므로 메시지를 처리하는 서비스가 중단되더라도 데이터가 손실되지 않는다. 메시지는 서비스가 다시 시작되고 처리되기 전까지 큐에 남아 있다. 팬아웃 패턴일반적으로 팬아웃 패턴은 특정 큐 또는 메시지 파이프 라인의 모든 클라이언트에게 메시지를 전달하는 데 사용된다. AWS에서 이 패턴은 보통 SNS 주제를 사용해 새 메시지가 토픽에 추가될 때 여러 가입자를 호출할 수 있게 구현된다. SNS 토픽은 여러 게시자 및 가입자를 가질 수 있는 통신/메시지 채널이다. 새로운 메시지가 토픽에 추가되면 모든 가입자가 병렬로 호출되기 때문에 이벤트가 모든 가입자에게 전달된다. 이 패턴은 여러 Lambda 함수를 동시 호출해야 하는 경우에 유용하다. SNS 토픽은 메시지를 전달하지 못하거나 함수가 실행되지 않는 경우 Lambda 함수를 호출하고 재시도한다. 파이프 및 필터 패턴복잡한 처리 작업을 파이프라인으로 구성된 일련의 관리 가능한 개별 서비스로 분해하는 것을 목적으로 하는 패턴이다. 데이터를 변환하도록 설계된 구성 요소를 필터라고 하고, 한 구성 요소에서 다음 구성 요소로 데이터를 전달하는 연결자를 파이프라고한다. 이 패턴은 결과를 얻기 위해 여러 단계가 필요한 어떤 작업에서든 유용하게 사용할 수 있다. Reference https://freecontent.manning.com/patterns-for-solving-problems-in-serverless-architectures/ https://docs.aws.amazon.com/ko_kr/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-lambda-function-trigger.html","link":"/posts/serverless/serverless-architecture-pattern/"},{"title":"Serverless 프레임워크 빠르게 배우기 (1)","text":"인프라에 대한 걱정을 줄여주는 서버리스는 배우고 나면 정말 편하겠다고 생각한 지가 대충 한 3개월 정도 되어가는 것 같다. 처음 람다를 경험한 건 AWS 아마톤 행사에서 사용했는데, 배포가 정말 코드 한 줄로 이루어 지는 것을 경험하고 (같이 하신 분이 환경 구성을 정말 잘 해주신 것이라고 생각하지만) 나중에 따로 프로젝트에서 꼭 써봐야지 했는데, 이제서야 점점 공부를 시작한다. 우선 람다에 대한 개념은 대충 알고 있으니 서버리스 배포를 편하게 도와주는 몇 서버리스 프레임워크 중에, 그 당시에 그나마 진입 장벽이 낮다고 추천 해주셨던 Serverless 라는 서버리스 프레임워크를 사용해보려고 한다. 일단 이 글은 AWS provider를 중심으로 공부한 내용을 정리할 예정이고, 내가 실습한 내용 보다는 Serverless에서 제공해주는 유저 가이드를 공부한 내용 위주로 정리할 생각이다. 핵심 개념들Functions여기서 함수들은 AWS 람다를 의미한다. 이것들은 마이크로서비스처럼 독립적인 배포 단위이다. 그리고 클라우드에서 배포되는 단순한 코드이다. 또 보통 단일한 역할을 수행하도록 설계된다. EventsAWS 람다를 실행하도록 트리거 역할을 하는 것을 말한다. AWS에서는 보통 아래와 같은 이벤트가 있다고 보면 된다. API Gateway S3 CloudWatch timer AWS SNS topic CloudWatch Alert 기타 등등 Serverless 프레임워크(이하 serverless)에다가 이벤트를 정의하면 serverless는 알아서 AWS 이벤트를 생성하고 이벤트를 람다와 연결되게 설정한다. Resources리소스는 람다가 사용하는 AWS 인프라라고 볼 수 있다. 아래와 같은 것들을 의미한다. DynamoDB Table S3 SNS Topic CloudFormation에서 정의될 수 있는 어떤 것이든 가능함 마찬가지로 리소스들도 serverless를 사용해서 구성할 수 있다. Services함수들을 모아둔 것이다. 함수는 위의 리소스와 이벤트를 가지고 하나의 단일 역할을 한다면 서비스는 이런 역할들을 모아서 하나의 서비스를 이룰 수 있다. Pluginsserverless의 기능을 확장하거나 덮어 씌울 때 사용한다. 모든 serverless.yml은 plugins를 포함할 수 있고 여러개를 사용할 수 있다. 설치일단 serverless는 Node 패키지로 구성되어있다. 아래 명령어로 CLI를 설치할 수 있다. 123456789$ npm i -g serverless$ serverless --versionFramework Core: 1.58.0Plugin: 3.2.5SDK: 2.2.1Components Core: 1.1.2Components CLI: 1.4.0 설치 한 다음 AWS 계정을 연결해주면 되는데 간단하게 프로젝트를 해 봤을 때 serverless.yml에 aws cli에 등록된 profile을 적어주는 방법으로 해서 따로 설정해주지 않았다. aws cli에 등록된 profile을 이용하거나 default profile를 serverless에 등록해줄 수도 있다. 여기 링크에서 AWS 계정을 연결하는 방법을 설명하고 있다. 만약 그 전에 AWS IAM을 사용해 사용자를 추가하는 방법을 모른다면 간단하게 벨로퍼트님의 자료를 확인해보면 좋을 것 같다. Services 알아보기서비는 하나의 프로젝트같은 것이다. serverless를 사용하기 위해서는 service를 만들어야 한다. serverless에서 service를 만드는 것은 yml 파일을 조작하는 것으로써 가능하다. 간단한 예제를 만들어보자. 프로젝트 폴더를 만들고, 최상단에 serverless.yml를 만들었다. 12myProject/ serverless.yml 위 파일에 모든 함수들과 리소스들을 담아내면 된다. 그런데 앱은 계속 커지기 마련이니, 하나의 파일에 모든 함수들을 정의하다 보면 정말 복잡한 파일이 구성된다. serverless는 yml파일을 서비스 단위로 쪼갤 수 있다. 예를 들어서 아래와 같이 구성할 수도 있다. 123456users/ serverless.ymlposts/ serverless.ymlcomments/ serverless.yml 위와 같이 서비스를 만들 때 create 명령어를 사용할 수 있는데, 아래와 같이 사용할 수 있다. (수동으로 구성할 수도 있겠지만, serverless에서는 강하게 권하고 있는 것 같다.) 12# ./serverless-test 아래 Node 탬플릿을 가지고 서비스를 만든다.serverless create --template aws-nodejs --path serverless-test 템플릿은 serverless에서 만들어둔 틀인데 노드를 선택한 것이다. (간단하게 테스트 해봤을 때 타입스크립트 템플릿에는 현재 날짜 기준으로 버그가 있는 것 같다. 웹팩이 handler export 이름을 없애서 람다에서 함수를 가져오지 못 하는 것 같다.) create 커맨드에 대해서 자세하게 정리한 문서는 이 링크에서 확인 가능하다. 이렇게해서 만들어진 내용을 확인해 보면 아래와 같다. 123serverless-test/ handler.js serverless.yml 먼저 serverless.yml를 확인해보면 서비스 설정 값들이 관리되고 있는 파일이란 걸 알 수 있다. 정확하게는 아래와 같은 역할을 한다고 볼 수 있다. 서버리스 서비스를 선언 서비스에 들어갈 한 개 이상의 함수를 정의 Provider 정의 (AWS, GCP, Azure …, 여기선 AWS) Plugin 정의 함수들을 실행할 이벤트들을 정의함 함수에서 사용되는 리소스들을 정의함 이벤트 섹션에 나열된 이벤트들이 개발시에 이벤트들이 요구하는 리소스들을 자동적으로 생성하도록 허용함 서버리스의 변수들을 사용해서 유연한 설정값을 만들도록 허용함 현재 만들어진 serverless.yml은 주석을 다 빼면 다음과 같은 모습을 하고 있다. 123456789service: serverless-testprovider: name: aws runtime: nodejs12.xfunctions: hello: handler: handler.hello 그리고 정말 고맙게도 사용 되지 않은 기능들을 비교적 구체적인 예시들로 주석을 채워놨다. 문서에서 보여주고 있는 사용 가능한 기능, 옵션 등은 아래와 같다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# serverless.ymlservice: usersprovider: name: aws runtime: nodejs12.x stage: dev # Set the default stage used. Default is dev region: us-east-1 # Overwrite the default region used. Default is us-east-1 stackName: my-custom-stack-name-${self:provider.stage} # Overwrite default CloudFormation stack name. Default is ${self:service}-${self:provider.stage} apiName: my-custom-api-gateway-name-${self:provider.stage} # Overwrite default API Gateway name. Default is ${self:provider.stage}-${self:service} profile: production # The default profile to use with this service memorySize: 512 # Overwrite the default memory size. Default is 1024 deploymentBucket: name: com.serverless.${self:provider.region}.deploys # Overwrite the default deployment bucket serverSideEncryption: AES256 # when using server-side encryption tags: # Tags that will be added to each of the deployment resources key1: value1 key2: value2 deploymentPrefix: serverless # Overwrite the default S3 prefix under which deployed artifacts should be stored. Default is serverless versionFunctions: false # Optional function versioning stackTags: # Optional CF stack tags key: value stackPolicy: # Optional CF stack policy. The example below allows updates to all resources except deleting/replacing EC2 instances (use with caution!) - Effect: Allow Principal: &quot;*&quot; Action: &quot;Update:*&quot; Resource: &quot;*&quot; - Effect: Deny Principal: &quot;*&quot; Action: - Update:Replace - Update:Delete Resource: &quot;*&quot; Condition: StringEquals: ResourceType: - AWS::EC2::Instancefunctions: usersCreate: # A Function handler: users.create events: # The Events that trigger this Function - http: post users/create usersDelete: # A Function handler: users.delete events: # The Events that trigger this Function - http: delete users/delete# The &quot;Resources&quot; your &quot;Functions&quot; use. Raw AWS CloudFormation goes in here.resources: Resources: usersTable: Type: AWS::DynamoDB::Table Properties: TableName: usersTable AttributeDefinitions: - AttributeName: email AttributeType: S KeySchema: - AttributeName: email KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 serverless.yml는 하나의 AWS CloudFormation stack을 구성하게 된다. 그렇다면 서비스마다 CloudFormation stack을 구성하는 셈이다. handler.js는 순수 JS 코드를 담고 있다. Monolithic하게 구성된 백앤드에서도 services를 API 바로 전에 두는 편이라 전반적으로 serverless가 어떻게 구성되는지 이해하기 쉬웠다. handler.js는 여러 함수를 export 할 수 있고, 내보내진 함수를 serverless.yml이 가르키고 있는 구조이다. 마치며배포 방법은 간단하게 deploy 명령어로 한다. 이와 관련된 가이드와 명령어 문서도 있긴 한데, 자세하게 공부하지는 않았다. 더 디테일한 내용은 필요에 따라서 보려고 한다. 일단 서버리스 프레임워크를 사용해도 기존에 Monolithic 사용하던 3 Layer architecture를 사용할 수 있을 것 같다(그게 베스트가 아닐 수도 있지만). Serverless 프레임워크 빠르게 배우기 (2) Reference https://serverless.com/ https://velopert.com/3549 https://serverless.com/framework/docs/providers/aws/guide/credentials/) https://serverless.com/framework/docs/providers/aws/cli-reference/create/","link":"/posts/serverless/serverless-framework-quicklearn-(1)/"},{"title":"Serverless + S3 + DyanamoDB VPC에 배포하고 외부와 연결하기","text":"서버리스에서 VPC 내부로 배포하는 방법에 대해서 알아보려고 한다. 현재 진행 중인 프로젝트는 빠르고 간단하게 홍보용으로 만들고 있는 프로젝트라, 별다른 옵션 없이 퍼블릭하게 오픈된 API를 만들어놓고 개발 중이다. 그런데, 개발 서버를 분리해서 일정 기간동안 유지 보수 하면서 배포도 몇 번 더 해야 할 필요가 있어서 내부 VPC에 배포하는 방법을 확인해보려고 한다. 추가적으로, VPC 내부로 배포하게 되면 S3와 DynamoDB에 접근하기 위해 NAT Gateway 또는 VPC Endpoint를 만들어줘야 한다. 이번 글에서는 NAT Gateway를 설정해주는 걸 해볼 것이다. 사전 준비핵심에 집중하기 위해서, 데모는 아래와 같은 준비가 되어있다고 가정한다. 프라이빗 서브넷, (NAT Gateway 연결을 위한)퍼블릭 서브넷을 가지고 있는 VPC serverless create -t aws-nodejs-typescript 명령어로 만들어진 샘플 앱 (region은 한국) 위 샘플 앱에서 region 설정만 해준 다음 바로 배포를 하게 되면 VPC 설정이 안되고, 오픈된 API Gateway가 구성되게 되어있다. 현재 상황은 배포가 완료되어서 API Gateway로 접근해서 확인할 수 있는 상황이다. 현재 초기 상황은 여기 링크에서 확인할 수 있다. 초기 앱 설정일단 간단하게 리소스를 설정해주고 DynamoDB와 S3에 접근하는 코드를 만들어보자. aws-sdk를 설치해준 다음 handler.ts 파일에 s3의 목록을 읽어오는 것과 만들어놓은 데모 테이블을 스캔하는 코드를 작성했다. 아래는 serverless.yml 파일이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657service: name: serverless-sample-apicustom: webpack: webpackConfig: ./webpack.config.js includeModules: trueplugins: - serverless-webpackresources: Resources: demoBucket: Type: AWS::S3::Bucket Properties: BucketName: vpc-demo-bucket demoTable: Type: AWS::DynamoDB::Table Properties: TableName: vpc-demo-table AttributeDefinitions: - AttributeName: id AttributeType: S KeySchema: - AttributeName: id KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1provider: name: aws runtime: nodejs12.x region: ap-northeast-2 apiGateway: minimumCompressionSize: 1024 environment: ${file(env.yml)} iamRoleStatements: - Effect: Allow Action: s3:ListBucket Resource: - &quot;arn:aws:s3:::vpc-demo-bucket&quot; - Effect: Allow Action: dynamodb:Scan Resource: - &quot;arn:aws:dynamodb:ap-northeast-2:*:table/vpc-demo-table&quot;functions: hello: handler: handler.hello events: - http: method: get path: hello 아래는 S3 리스트를 보는 코드, DynamoDB를 스캔하는 코드가 있는 handler.ts이다. 1234567891011121314151617181920212223242526import { APIGatewayProxyHandler } from &quot;aws-lambda&quot;;import { S3, DynamoDB } from &quot;aws-sdk&quot;;import &quot;source-map-support/register&quot;;export const hello: APIGatewayProxyHandler = async (_event, _context) =&gt; { const s3 = new S3(); const dynamodb = new DynamoDB(); const s3Data = await s3 .listObjectsV2({ Bucket: &quot;vpc-demo-bucket&quot; }) .promise(); const { Items: dynamoData } = await dynamodb .scan({ TableName: &quot;vpc-demo-table&quot; }) .promise(); return { statusCode: 200, body: JSON.stringify( { s3Data, dynamoData }, null, 2 ) };}; env.yml 파일에는 AWS_ACCESS_KEY와 AWS_SECRET_KEY를 담고 있는데, iamRoleStatements에 적힌 권한으로 인해, 사실상 aws-sdk의 config.update({...}) 부분이 없었어도 돌아갔을 것 같다. 우선 이렇게 설정을 해주고 다시 배포를 해줬다. Bucket과 Table에 데이터를 하나씩 넣어주고 작동해서 확인해봤다(캡쳐는 못 찍음). 이제 대충 VPC없는 환경에서의 일반 앱의 구조처럼 배포가 된 상태이다. VPC 설정serverless.yml에서 관리하는 모든 함수들에 대해서 VPC 설정을 해줄 수도 있고 특정 함수에 대해서만 VPC 설정을 해줄 수도 있다. 기본적으로 vpc라는 프로퍼티를 추가해주면 되는데, 이 프로퍼티는 securityGroupIds와 subnetIds라는 배열 형태의 속성을 갖는다. 공식 문서에 나온 예시는 아래와 같다. 123456789101112131415161718192021222324# serverless.ymlservice: service-nameprovider: name: aws vpc: securityGroupIds: - securityGroupId1 - securityGroupId2 subnetIds: - subnetId1 - subnetId2functions: hello: # this function will overwrite the service level vpc config above handler: handler.hello vpc: securityGroupIds: - securityGroupId1 - securityGroupId2 subnetIds: - subnetId1 - subnetId2 users: # this function will inherit the service level vpc config above handler: handler.users 예시의 주석에 설명 되어있듯, 세부적인 옵션일수록 우선순위가 높게 적용된다. 전체에 해당하는 옵션은 세부적인 함수에 적용되는 옵션보다 우선순위가 낮다. 그렇다면 우리 프로젝트에도 적용해보자. 아래 env.yml에서는 프라이빗 서브넷 아이디를 값으로 두 개 넣어주고, default vpc securitygroup 아이디로 설정해줬다. 12345678#...vpc: subnetIds: - ${file(env.yml):SBNID1} - ${file(env.yml):SBNID2} securityGroupIds: - ${file(env.yml):SCGID1}#... 위와같이 설정 해주고 다시 배포를 하면, 설정해준 VPC의 서브넷에 배포가 되고, 다시 같은 엔드포인트로 접속했을 때 타임아웃이 뜬다. NAT Gateway 설정AWS 리소스 연결과 관련해서 처음엔 상당히 당황스러운 경험이 있었다. 문서에서 확인했을 때는, VPC 내부에 람다를 설정했을 때 S3, DynamoDB와 같은 서비스에는 람다와 같은 서비스를 VPC 내부에서 사용하거나, 서비스의 기본 설정이 VPC인 서비스는 다른 AWS 리소스와 통신하기 위해 VPC Endpoint를 설정 해줘야 한다고 되어 있었는데, 로컬에서 람다 작동을 확인해보기 위해 NAT Gateway만 사용했더니 DynamoDB, S3가 모두 정상 작동했다. 이유를 잘 모르겠어서 AWSKRUG 그룹에 상황과 함께 질문을 드렸는데 정말 친절하신 고수님께서 추측을 해주셨는데 그 이유는 다음과 같다. 람다같은 서비스를 VPC로 설정해서 쓰거나, 서비스의 기본 설정이 VPC인 경우 다른 AWS 리소스와 통신을 위해서 VPC Endpoint 설정을 해줘야 한다. 다만, 만들어주는 이유는 “통신”을 하기 위함이기 때문에 NAT Gateway를 통해 VPC를 Public 망으로 나갈 수 있는 경로를 지정해주면, NAT Gateway에서 지정된 Public IP를 통해서 AWS 리소스들과 “통신” 가능하게 되어 결국 리소스들과 연결이 된 것이 아닐까라는 추측이었고, 검색 해보니 NAT Gateway와 VPC Endpoint는 서로 대체제처럼 이용되고 있는 경우가 많이 있었다. NAT Gateway를 이용하게 되면 Public 망을 거쳐서 통신, VPC Endpoint는 내부 통신이 이루어 진다는 차이가 생기긴 한다. 그리고 금액도! (감사합니다 갓 AWSKRUG…). NAT Gateway를 만드는 건 간단하다. Public Subnet 안에 Elastic IP를 붙여서 만들면 된다. 그리고 만들어진 NAT Gateway를 프라이빗 라우팅 테이블에 Internet Gateway 설정해주듯, 설정해주면 된다. 0.0.0.0/0을 NAT Gateway를 통해 찾도록 만들면 된다. 조금 더 자세한 내용을 확인하려면 이 링크를 확인해보면 좋을 것 같다. 이렇게 설정해주고 나면, VPC 내부에서 배포하게 된다. 최종 버전은 이 링크에서 확인할 수 있다. 후기외부와 통신 과정을 만들면서 의도치않게 NAT Gateway를 통해 VPC Endpoint를 대체하는 경우에 대해서 알게 되었고, 현재 의도한 경우와는 달라서 NAT Gateway는 사용하지 않을 것 같지만, 당연히 언젠간 쓸 것 같다. 금액도 알아보고… Reference https://serverless.com/framework/docs/providers/aws/guide/functions#vpc-configuration https://medium.com/@philippholly/aws-lambda-enable-outgoing-internet-access-within-vpc-8dd250e11e12","link":"/posts/serverless/serverless-vpc-deploy-demo/"},{"title":"Serverless 프레임워크 빠르게 배우기 (2)","text":"지난 포스트에서 serverless의 기본적인 핵심 개념들과 그 중 Services까지 정리를 했다. 이번에는 Functions에 대해서 정리해보자. (Events까지 보려고 했는데 생각보다 함수파트 내용이 복잡하고 많으니까 천천히 다시 볼 필요가 있는 것 같음) Functions이 글에서는 Provider가 AWS이므로 Functions는 Lambda를 뜻한다. 설정모든 함수들은 serverless.yml의 functions 속성 아래서 발견할 수 있다. 아래 예시를 확인해보자 123456789101112131415# serverless.ymlservice: myService# ...functions: hello: handler: handler.hello # required, handler set in AWS Lambda name: ${self:provider.stage}-lambdaName # 배포될 람다 이름 (옵션) description: 이 함수가 무슨 일을 하는지에 대해서. runtime: python2.7 # 기본적으로는 provider에서 정의한 runtime memorySize: 512 # 디폴트는 1024 MB timeout: 10 # 타임 아웃 디폴트는 6초 reservedConcurrency: 5 # optional, reserved concurrency limit for this function. By default, AWS uses account concurrency limit tracing: PassThrough # Active or PassThrough가 가능함 handler라는 속성은 실행하려고 하는 함수를 담고 있는 모듈을 가리키고 있다. functions라는 속성에 여러 함수들을 담을 수 있다. 여러 함수들을 작성한 경우 함수들은 그 세팅을 provider 속성에서 많이 상속 받아 온다. 예를 들어서 아래의 경우, 모든 함수들의 memorySize는 512가 된다. 그리고 함수 단위에서 같은 이름으로 새로 정의할 수 있다. 12345678910111213service: myServiceprovider: name: aws runtime: nodejs12.x memorySize: 512 # 아래 함수들에게 기본으로 적용됨functions: functionOne: handler: handler.functionOne functionTwo: handler: handler.functionTwo memorySize: 1024 # provider에서 받은 속성을 덮어 씀 함수들을 다른 파일들에 분리할 때는, 함수들을 배열의 형태로 정의하는 것이 유용하다. 123functions: - $(file(../foo-functions.yml)) ... 12345# foo-functions.ymlgetFoo: handler: handler.foodeleteFoo: handler: handler.foo 이제 구체적인 설정들에 대해서 하나씩 살펴볼 것이다. 권한 (Permissions)모든 AWS 람다 함수는 다른 AWS 인프라 리소스들과 상호작용 하기 위해서 권한이 필요하다. 이 권한들은 IAM Role을 통해서 설정하는 것인데, 이런 권한 정책을 provider.iamRoleStatements 속성을 통해서도 설정할 수 있다. 아래 몇 가지 예시를 확인해보자, 첫 번째는 serverless.yml에 정의되는 모든 함수들이 DynamoDB에 대한 권한을 갖게 되는 예시이고, 두 번째는 S3 버킷에 오브젝트를 놓는 것을 허용하는 권한이다. 첫 번째 예시12345678910111213141516171819202122# serverless.ymlservice: myServiceprovider: name: aws runtime: nodejs12.x iamRoleStatements: # 모든 람다 함수의 권한 - Effect: Allow Action: # 특정 리전에 DynamoDB 테이블 권한을 줌 - dynamodb:DescribeTable - dynamodb:Query - dynamodb:Scan - dynamodb:GetItem - dynamodb:PutItem - dynamodb:UpdateItem - dynamodb:DeleteItem Resource: &quot;arn:aws:dynamodb:us-east-1:*:*&quot;functions: functionOne: handler: handler.functionOne memorySize: 512 두 번째 예시1234567891011121314151617181920212223242526272829# serverless.ymlservice: myServiceprovider: name: aws iamRoleStatements: - Effect: &quot;Allow&quot; Action: - &quot;s3:ListBucket&quot; # 여기에 CloudFormation 문법을 적어 둘 수 있다. # 여기에 모든 내용들이 CloudFormation으로 전환되기 때문에. Resource: { &quot;Fn::Join&quot;: [&quot;&quot;, [&quot;arn:aws:s3:::&quot;, { &quot;Ref&quot;: &quot;ServerlessDeploymentBucket&quot; }]], } - Effect: &quot;Allow&quot; Action: - &quot;s3:PutObject&quot; Resource: Fn::Join: - &quot;&quot; - - &quot;arn:aws:s3:::&quot; - &quot;Ref&quot;: &quot;ServerlessDeploymentBucket&quot; - &quot;/*&quot;functions: functionOne: handler: handler.functionOne memorySize: 512 serverless.yml에 정의하지 않고 ARN을 사용해 원래 있던 IAM Role을 사용할 수 있다. 12345# serverless.ymlservice: new-serviceprovider: name: aws role: arn:aws:iam::YourAccountNumber:role/YourIamRole VPC 설정serverless.yml에 vpc 객체 속성을 함수 설정 부분에 추가함으로써, 구체적인 함수에 적용되는 VPC 설정을 추가할 수 있다. 이 객체는 VPC가 구성되기 위해 필요한 securityGroupIds와 subnetIds 배열 속성들을 포함해야 한다. 12345678910111213service: service-nameprovider: awsfunctions: hello: handler: handler.hello vpc: securityGroupIds: - securityGroupId1 - securityGroupId2 subnetIds: - subnetId1 - subnetId2 구체적으로 말고 해당 서비스 안에 사용되는 모든 함수에 적용하고 싶다면 항상 그랬듯, provider 부분에 설정을 해줘도 된다. 1234567891011121314151617181920212223service: service-nameprovider: name: aws vpc: securityGroupIds: - securityGroupId1 - securityGroupId2 subnetIds: - subnetId1 - subnetId2functions: hello: # 이 함수는 상위 VPC 설정을 덮어 쓸 것임. handler: handler.hello vpc: securityGroupIds: - securityGroupId1 - securityGroupId2 subnetIds: - subnetId1 - subnetId2 users: # 이 함수는 그냥 위의 VPC 설정을 그대로 사용할 것임 handler: handler.users 위 설정에 따라서, 배포하면 람다 함수와 함께 VPC 설정이 배포될 것이다. VPC IAM Permissions(이 부분은 사실 배경 지식 부족으로 매끄럽게 읽히지가 않았다. 이후 공부 해보면서 알게 되면 보충해보도록 하자. 아마 RDS 연결하면서 VPC 설정은 더 자세하게 공부해야 할 것 같다.) 람다 함수의 실행 역할은 반드시 Elastic Network Interface(ENI)를 만들고, 삭제하는 권한이 있어야 한다. VPC 설정이 구성될 때, 기본 AWS AWSLambdaVPCAccessExecutionRole이 람다 실행 역할과 연관지어진다. 커스텀 역할이 제공 되는 경우엔, 적절한 ManagedPolicyArns가 포함되어 있어야 한다. 자세한 정보는 여기 링크에서 확인해보자. VPC Lambda Internet Access기본적으로, 람다 함수가 VPC 내부에서 실행될 때, 함수는 인터넷 접근을 잃고 몇 AWS 내부의 리소스들을 사용할 수 없게 된다. S3 리소스와 DynamoDB 리소스가 VPC 내부의 람다에서 동작하게 하기 위해서는 하나의 VPC 엔드 포인트가 만들어져야 한다. 더 자세한 정보는 여기 링크에서 확인해보자. 다른 Kinesis streams 같은 서비스들을 사용할 수 있게 하려면, 하나의 NAT 게이트웨이가 람다를 돌릴 때 사용되는 서브넷 안에서 설정 되어 있어야 한다. 더 자세한 정보는 여기 링크에서 확인해보자. 환경 변수환경 변수도 마찬가지로 전역적으로 provider에 설정해주거나 함수 단위로 설정해줄 수 있다. environment 속성에 담아주면 된다. 기본적으로 provider와 함수 쪽에 설정된 환경 변수는 merge되지만, 같은 key값을 가진 환경 변수는 함수 쪽에 설정된 값으로 덮어 씌워진다. 1234567891011121314151617# serverless.ymlservice: service-nameprovider: name: aws environment: SYSTEM_NAME: mySystem TABLE_NAME: tableName1functions: hello: # 이 함수는 SYSTEM_NAME, TABLE_NAME 환경 변수 사용 가능함 handler: handler.hello users: # 이 함수도 둘 다 사용 가능하지만, TABLE_NAME은 함수 부분에서 정의된 변수를 사용함 handler: handler.users environment: TABLE_NAME: tableName2 만약 함수의 환경변수가 machine(개발 하고 있는 호스트를 말하는 것 같음)의 환경 변수들과 일치하기를 바란다면, 여기 문서를 확인해 보자 Tags키/값 태그를 함수에 붙이는 설정값이다. 이렇게 해서 붙여진 태그는 AWS 콘솔에 나타나고, 함수들을 태그로 찾거나 그룹 짓기 쉽다. 이 값도 위와 마찬가지로 functions 레벨에서 정의할 수도 있고, provider 레벨에서 정의할 수도 있지만, 환경 변수 처럼 같은 키 값을 가진 태그는 구체적인 함수 레벨 것으로 덮어 씌워진다. 1234567891011121314151617# serverless.ymlservice: service-nameprovider: name: aws tags: foo: bar baz: quxfunctions: hello: # 이 함수는 foo, baz 태그가 적용 됨 handler: handler.hello users: # 이 함수도 foo, baz 태그가 적용 되는데, foo 값은 함수 레벨에서 정의된 값으로 사용된다. handler: handler.users tags: foo: quux 태그를 사용하는 좋은 예는 아래와 같다. 비용 측정용 (env: prod로 붙여두고 체크) 레거시 코드 트랙킹 (runtime: node0.10 붙여두면 레거시 코드 찾기 쉬움) Layers람다 Layers를 사용하기 위한 설정 값도 정의할 수 있다. 람다 Layer는 2018년 말에 나온 것 같은데, serverless.yml에서 아래와 같이 설정해주면 사용할 수 있다. 12345functions: hello: handler: handler.hello layers: - arn:aws:lambda:region:XXXXXX:layer:LayerName:Y Layers에 대한 설명이 따로 있어서 자세하게 보는 건 다음으로 미루자. 로그 그룹 리소스기본적으로 serverless는 람다 함수들을 위한 로그 그룹을 만든다. 이렇게 함으로써 나중에 서비스를 삭제할 때 로그 그룹을 정리하기 쉽게 해주고, 람다 IAM 권한을 더 구체적이고 안전하게 만들어준다. Versioning Deployed Functions마찬가지로 기본적으로 serverless는 배포시마다 함수의 버전을 만든다. 이건 선택적인데, provider 레벨에서 versionFunctions 속성을 true, false로 두는 것으로 켜거나 끌 수 있다. 12provider: versionFunctions: false DLQ (Dead Letter Queue)DLQ는 람다 함수가 실행에 실패하면 해당 정보를 SNS topic이나 SQS queue를 통해 전달해주는 것을 말한다. 이 정보로 실패한 람다에 대한 진단을 내리고 반응할 수 있다. serverless 에서는 DLQ에 대한 설정을 SNS topic과 onError 파라메타를 통해서 설정할 수 있다. 이 설정은 함수 레벨에서 arn을 지정해주는 것으로 동작한다. (SQS는 문제가 있나봄) 1234567891011service: serviceprovider: name: aws runtime: nodejs12.xfunctions: hello: handler: handler.hello onError: arn:aws:sns:us-east-1:XXXXXX:test # Ref, Fn::GetAtt and Fn::ImportValue are supported as well KMS Keys람다는 시크릿 키를 위해서 KMS를 사용한다. serverless에서는 awsKmsKeyArn 속성 값으로 KMS를 지정할 수 있다. 1234567891011121314151617service: name: service-name awsKmsKeyArn: arn:aws:kms:us-east-1:XXXXXX:key/some-hashprovider: name: aws environment: TABLE_NAME: tableName1functions: hello: # 전역 설정된 KMS를 사용하지 않고 함수 레벨에서 정의된 KMS 사용 handler: handler.hello awsKmsKeyArn: arn:aws:kms:us-east-1:XXXXXX:key/some-hash environment: TABLE_NAME: tableName2 goodbye: handler: handler.goodbye 환경 변수에 시크릿키를 저장할 때 AWS는 민감한 정보를 암호화 할 것을 추천한다. 그래서 KMS를 사용하는 튜토리얼도 제공해주고 있다. AWS X-Ray Tracingserverless에서 tracing 속성 값을 적어주는 것으로, 람다 함수에 AWS X-Ray Tracing 서비스를 사용할 수 있다. 1234567service: myServiceprovider: name: aws runtime: nodejs12.x tracing: lambda: true 그리고 또 이 변수를 함수 단위를 기본으로 설정할 수 있다. 위와 마찬가지로 provider 레벨에서 설정한 값을 덮어 쓴다. 1234567functions: hello: handler: handler.hello tracing: Active goodbye: handler: handler.goodbye tracing: PassThrough 마치며VPC와 다른 리소스를 함께 쓰는 것에 대해서 이론적으로 감은 온 것 같다. 구체적인 상황별 문서도 같이 붙여놔서 나중에 참고하기 좋을 것 같다. Serverless 프레임워크 빠르게 배우기 (1) Reference https://serverless.com/framework/docs/providers/aws/guide/services/","link":"/posts/serverless/serverless-framework-quicklearn-(2)/"},{"title":"메모리 할당과 매핑","text":"우리는 코드를 짤 때, 실제 메모리 위치를 알고 코드를 쓰지 않는다. count := 10 이라는 코드를 쓰면, count라는 변수가 메모리 어디에 위치하는지를 알고 짜는 것은 아니다. 프로그램을 동작시키려면 메모리에 올려야 하는데, 이 과정에서 발생하는 우리가 짠 코드가 실제 메모리에는 어떻게 올라가고, 어떻게 주소를 찾아가는지에 대한 이야기이다. Continuous Memory Allocation연속 메모리 할당 방식은 초기 버전의 메모리 할당 방식에 해당한다. 가장 쉬운 방법으로는 고정된 크기로 메모리를 나눠 프로세스에게 할당해주는 방식이 있고, 효율적인 메모리 분배를 위해 파티션을 프로세스 크기에 따라 나누는 방법이 있다. Fixed Partition (FPM, 고정 분할)이름대로, 고정된 크기로 메모리를 나누는 방식이다. 각 분할마다 한 프로세스를 가지게 되며, 이때 분할의 개수를 다중 프로그래밍 정도(Multiprogarmming Degree)라고 한다. 한 분할이 비게 되면 프로세스가 입력 큐(input queue)에서 선택되어 빈 분할로 들어오게 된다. 이 방식은 내부 단편화와 외부 단편화 모두 발생할 수 있다. Variable Partition (VPM, 가변 분할)가변 분할 방식에서는 어떤 부분이 사용되었는지를 파악하는 테이블을 사용해야 한다. 초기에는 하나의 큰 사용 가능한 블록, hole이 있는 상태라고 한다. 프로세스가 input queue에 들어오면 프로세스가 사용할 메모리를 확인하고, 남은 공간이 있다면 필요한 메모리만큼 할당해준다. 이 방식에서는 내부 단편화가 발생할 수 없지만, 여러 프로세스에 메모리를 할당하고 빼주는 과정을 거치다 보면 외부 단편화 문제는 발생할 수 있다. 예를 들어, 다음 시나리오를 생각해보자. 123456789매모리 공간 55MB1. P1 적재 -&gt; 20MB2. P2 적재 -&gt; 10MB3. P3 적재 -&gt; 10MB3. P4 적재 -&gt; 10MB4. P1, P3, P4 종료5. P5 적재 -&gt; 30MB 위 시나리오에서 초기 상태, 메모리 사용 상태를 파악하는 파티션 테이블은 다음과 같다. P4까지 적재된 후의 상태는 다음과 같다. 이제 P1, P3, P4가 종료 되고 나면 Partition 1, 3, 4는 빈 공간이 되고, 3, 4번 파티션이 합쳐진다. 이렇게 빈 영역을 하나의 파티션으로 합치는 것을 Coalescing holes(공간 통합)라고 한다. 현재 남은 공간은 총 45MB이지만, 이는 연속되지 않았기 때문에 P5를 적재할 수 없는 상태이다. 즉, 외부 단편화가 발생하는 상황인 것이다. 메모리 배치 전략위와 같은 예시 상태에서, P5가 10MB라고 가정해보자. 현재 남은 파티션은 1, 3이고 두 파티션 모두 적재할 수 있는 크기이다. 그렇다면 어디에 배치할 수 있을까? 이것을 결정하는 문제의 해결책은 대표적으로 최초 적합(First-fit), 최적 적합(Best-fit), 최악 적합(Worst-fit)이 있다. 최초 적합 (First-fit)메모리 가용 파티션 중 첫 번째로 사용 가능한 공간을 할당해준다. 검색 시작은 집합의 시작에서부터 하거나, 지난 번에 검색이 끝났던 위치부터 시작할 수 있다. 지난 번에 검색이 끝났던 위치부터 시작하는 경우를 Next-fit으로 구분하기도 한다. 이 방식을 사용하면 메모리가 한 쪽만 지나치게 사용되는 문제를 해결할 수 있다. 최적 적합 (Best-fit)사용 가능한 공간들 중에서 가장 작은 것을 선택한다. 리스트가 크기 순서로 되어있지는 않으므로 모든 리스트를 탐색해야 하는 오버해드가 존재한다. 이 방식은 아주 작은 파티션을 만들어낸다. 최악 적합 (Worst-fit)가장 큰 가용 공간을 선택한다. 마찬가지로 리스트가 크기 순서로 정렬되어있지 않다면 모두 탐색해야 한다. 이 방식은 남게 되는 공간이 비교적 클 확률이 높다. 이 세 가지 방식 중 어떤 것이 더 낫다는 이론적으로 확정할 수는 없지만 모의 실험을 해봤을 때, 시간과 메모리 이용 효율 측면에서 최악 적합이 가장 안 좋았고, 최초 적합과 최적 적합이 공간 효율성은 비슷했지만 속도 면에서 최초 적합이 더 빠르게 나타났다고 한다. Non-Continuous Memory Allocation어떻게 하면 외부 단편화 문제를 완화할 수 있을까? 가변 분할 방식에서 외부 단편화는 프로세스가 메모리에 적재 되었다가, 빠지는 과정을 반복하면서, 불연속적인 공간이 남는 것으로 인해 발생했다. 그렇다면, 이런 불연속한 부분을 사용 중인 영역을 밀어 올려 없애는 걸 생각해볼 수 있을 것 같다. 이 방식을 Storage Compaction(메모리 압축)이라고 한다. 프로세스 처리에 필요한 적재 공간을 확보해야 할 때 사용할 수 있다. 예를 들어서, 위 테이블을 재배치 하면 다음과 같이 변경된다. 그러나 이 방식은, 프로세스를 모두 중지 해야 하고, 많은 시스템 자원을 소비하는 방식이다.이 문제를 해결하는 다른 접근 방식은 한 프로세스의 논리 주소 공간을 여러 비연속적인 공간으로 나눠 필요한 크기의 공간을 사용할 수 있을때 메모리에 할당해주는 방식이다. 이를 Non-Continuous Memory Allocation이라고 한다. 정확하게는, 사용자 프로그램을 여러개의 블록으로 분할하고, swap-device에 모두 두고, 실행시 필요한 블록만 메인 메모리에 적재하는 방식이다. 이를 구현한 방식이 페이징(Paging)과 세그먼테이션(Segmentation)이다. 두 방식은 결합되어 사용될 수도 있다 Address MappingAddress Mapping은, 연속 할당 방식에서는, 상대 주소를 물리 주소로 “재배치”하는 작업을 뜻했다. 불연속 메모리 할당 방식에서는 Virtual Address (가상 주소) 개념이 등장한다. 이는 연속 메모리 할당 방식의 “상대 주소”와 같다고 볼 수 있다. Real Address는 물리 주소와 같은 소리이고, 실제 메모리 주소를 뜻한다. 불연속 메모리 할당 방식의 메모리 매핑은 가상 주소를 실제 주소로 바꿔주는 과정이다. 1[User process의 Virtual Address] -&gt; (Address Mapping) -&gt;[Real Address] 논리적, 가상적의 뉘앙스의 메모리는 프로그램이 가지고 있는 연속을 가정한 가상의 주소 형태라고 볼 수 있다. 대체적으로, 시작점을 찾을 수 있는 단서와 시작점으로부터 얼마나 많이 떨어져 있는가와 같은 튜플로 구성된다. 물리적, 실제의 뉘앙스의 메모리는 메인 메모리의 위치이다. Block Mapping사용자 프로그램을 블록 단위로 분할하고 관리하는 시스템에서의 매핑 방식이다. 가상 주소를 블록 숫자와 얼마나 시작점에서 떨어져있는지에 대한 정보를 가지고 표현한다. 즉, V = (b, d)로 표현하는데, b는 block number를 뜻하고, d는 displacement를 의미한다. displacement는 쉽게 말해 offset이다. block number로부터 블록의 실제 메모리 주소를 가지고, 해당 Instruction이 시작점으로부터 얼마나 떨어진 명령인지 알려주는 역할을 한다. Address Mapping 정보는 Block Map Table(BMT)에서 관리된다. 프로세스마다 하나의 BMT를 커널 공간에 가지고 있다. BMT가 관리하는 정보는 대략 다음과 같다. residence bit는 해당 블록이 메모리에 올라간 상태인지를 나타내주는 비트이다. V = (b, d)로 표현되면, 먼저 BMT에서 b에 해당하는 열을 찾는다. 해당 열에서는 블록이 메모리에 적재된 상태인지 아닌지 확인 후, 적재된 상태라면 Real Address를 받아와서 d와 함께 명령어 주소를 찾는다. 적재되지 않은 상태라면 해당 블록을 메모리에 올리고 BMT를 업데이트 하고 Real Address를 받아와 명령어 주소를 계산한다.사용자 프로그램을 여러 개의 블록으로 분할하고, swap-device에 모두 두고, 실행 시 필요한 블록만 메인 메모리에 적재하는 방식이다.&gt; 이 방식은 구체적인 구현이라기 보단, Non-continuous Memory Allocation에서 주로 사용하는 메모리 매핑 방법이다. 이후, 구체적으로 페이징 방식이나, 세그먼테이션 방식에서 어떻게 주소를 찾아주는지 나온다. Paging System프로그램을 같은 크기의 블록으로 분할하는 방식이다. 나누어진 블록을 Page(페이지)라고 부르고, 메모리의 분할 영역을 Page Frame(페이지 프레임)이라고 부른다. 페이지와 페이지 프레임은 같은 크기이다. 프로세스는 분할된 페이지로 나눠져 예비 저장 장치 또는 파일 시스템에 놓여진다. 나눠진 페이지 중 사용되는 페이지를 메모리에 올리는 방식이다. 교과서와 다르게, 강의에서는 예비 저장 장치, 파일 시스템은 Secondary Storage / Swap Device로 지칭되었는데 같은 의미로 봐도 될 것 같다. 우선 가장 큰 특징은 논리적 분할이 아니라 크기에 따른 분할이다. 이러한 특징으로 인해, 이후 세그먼테이션에서 확인할 수 있겠지만, 페이지 공유나 보호 과정이 복잡하다. 그러나 단순한 설계 방식으로 효율적으로 관리할 수 있다. 또한 외부 단편화는 발생할 수 없지만, 내부 단편화는 발생할 수 있다. OSX의 기본 페이지 사이즈는 4096Bytes인 것 같다. vm_stat 명령어로 터미널에서 현재 가상 메모리 상태를 확인할 수 있다. 나타내는 구체적인 정보는 이 링크에서 확인할 수 있다. Address Mapping위의 블록 매핑 방식과 유사하다. 블록이라는 이름 대신 페이지를 사용하는 것이라고 볼 수 있다. 가상 주소를 다음과 같이 표현할 수 있다. 123V = (p, d)p: page numberd: displacement 위 정보와 함께, 물리 주소를 찾기 위해 페이지 PMT(Page Map Table)를 사용한다. 페이지 테이블은 대략 다음과 같이 생겼다. 페이지 맵 테이블은 커널 안에(메모리 안에) 있다. 다음과 같은 프로세스를 따른다고 볼 수 있다. 프로세스의 PMT가 저장된 주소 b에 접근 PMT에서 page p의 엔트리를 찾는다. (b + p * entrySize) 찾아진 entry의 residence bit 검사 residence bit == 0인 경우, page fault라고 부른다. swap device에서 해당 page를 메모리에 적재하고 PMT를 갱신 후 3-2 수행 이 과정은 컨텍스트 스위칭이 발생하고, 오버해드가 크다. residence bit == 1인 경우, 해당 entry에서 page frame number p’를 확인 p’의 가상 주소의 d를 사용해 실제 주소 r을 만든다. (r = p' * pageSize + d) 지금까지 소개한 방법은 Directed Mapping이라는 이름으로 불린다. 커널 위의 매핑 테이블을 메모리에서 찾아오는 방식인데, 이 방식에서는 한 가지 문제점이 있다. 실제 주소를 얻기까지 메모리 접근을 두 번 해야한다는 것이다. 첫 번째는 PMT에 접근하기 위해서, 두 번째는 실제 메모리에 접근하기 위해서. 이 문제를 해결하기 위해서 Associative Mapping이라는 방법을 사용하는데, 간단히 말해서 TLB를 사용하는 방식이다. TLB(Translation Look-aside Buffer)라는 특수한 캐시 하드웨어를 사용해, 이곳에 PMT를 적재하는 방식이다. TLB는 page number를 받으면, 페이지 테이블을 병렬적으로 탐색해 굉장히 빠르게 page frame number를 가져올 수 있다. 다만, TLB는 아주 비싸기 때문에, 작은 크기이다. 따라서, 큰 PMT를 다루기는 어렵다. 따라서, 일반적으로 PMT와 TLB를 함꼐 사용한다. PMT는 그대로 메모리 공간에 올려두고, PMT 일부만 TLB에 올리는 방식이다. 만약 찾고자 하는 Page가 TLB에 없다면, PMT에서 가져와야 한다. 이때, 이미 TLB가 모두 차있다면, 대체해야 하는데 이 정책 중 일반적인 것이 최근에 가장 적게 사용된 엔트리를 빼는 것이다. 이는 메모리 지역성과 유관하다. Segmentation페이징 시스템은 논리적 단위가 아닌, 크기에 따라 프로세스를 나누기 때문에, 페이징을 공유하고 보호하는 것에서 어려움이 있다. 세그멘테이션 시스템은 프로세스를 논리적 블록으로분할한다. 즉, 프로그래머가 생각하는 모양대로 메모리를 분할해 적재해준다. 따라서, 블록의 크기는 서로 다를 수 있고 페이징 시스템에서 처럼 메모리를 미리 분할할 수도 없다. 이 방식은 내부 단편화가 발생할 일이 없지만, 외부 단편화는 발생할 수 있다. 또한 세그멘트를 공유하거나 보호하는 작업을 하기 쉽지만, Address Mapping이나, 메모리 관리에서 오버헤드가 비교적 크다. Address MappingNon-continuous Allocation 방식의 Address Mapping의 방식과 마찬가지로, 가상 주소를 다음과 같이 표현한다. 123V = (s, d)s: segment-numberd: displacement (offset) 페이징 시스템에서 사용한 것처럼 SMT(Segment Map Table)라는 매핑 테이블을 사용한다. 매커니즘도 PMT와 유사하다. PMT와 비교하자면, segment length와 protection bits가 추가되어있다. 위와 같은 형태라고 볼 수 있는데, 추가된 두 필드는 다음과 같은 역할을 한다. segment length: 세그먼트의 크기를 기록한다. 실제 주소를 찾을때, 세그먼트 사이즈를 초과해 접근하지 않도록 만들어준다. protection bits: 세그먼트(예를 들어 함수나 데이터)에 대한 프로세스의 권한을 적는다. 읽기: R / 쓰기: W / 실행: X / 추가: A 비트가 있다고 한다. 위에서는 추가된 두 필드라고는 하지만, 실제로 페이징 시스템에도 protection bits는 있는 것으로 설명한다. 매핑을 할때는 다이렉트 매핑 과정을 거친다. V = (s, d)와 함께 SMT가 저장된 주소 b에 접근해 필요한 entry를 계산해낸다. (b + s * entrySize*) SMT의 entry에 대해 다음 단계를 순차적으로 수행한다. residence bit가 0인 경우 (segment fault), swap-device로부터 해당 segment를 메모리에 적재하고, SMT를 갱신한다. d가 segment length의 길이보다 크다면, segment overflow exception 처리 모듈을 호출한다. protection bits를 확인해, 허가 되지 않은 연산인 경우 segment protection exception 처리 모듈을 호출한다. 실제 주소 r을 찾아 명령어를 처리한다. Paging System에서 처럼 TLB를 사용해서 메모리에 두 번 접근하는 오버헤드를 줄일 수 있다. Memory ManangementVPM과 유사하게, 세그먼트를 적재할 때, 그 크기에 맞춰 동적으로 메모리를 분할한 후 적재한다. 이를 관리하는 파티션 테이블이 요구된다. 아래 구성과 같이 파티션 테이블을 관리한다. Hybrid System페이징 시스템과 세그멘테이션 시스템의 장점을 결합한 시스템이다. 프로그램을 다음과 같이 분할한다. 논리 단위의 Segment로 분할 각 Segment를 고정된 크기의 Page들로 분할 메모리에 적재할 때는 페이지 단위로 적재하게 된다. Address Mapping가상 주소는 다음과 같은 형태로 주워진다. 1234V = (s, p, d)s: segment numberp: page numberd: offset 매핑을 위해 SMT와 PMT를 모두 사용해야 한다. 각 프로세스마다 하나의 SMT가 존재하고, 하나의 세그먼트마다 하나의 PMT를 갖는 구조이다. SMT 테이블은 마지막에 실제 주소 대신, PMT의 베이스 주소를 알려준다. 아래는 하이브리드 시스템에서의 SMT이다. 이전 SMT테이블과 다른 모습은, 첫 번째로 resident bit가 없다는 점이다. 실제 메모리에 올라가는 것은 Page이기 때문에 residence bit는 불필요하다. 두 번째는, 위에서 언급한 것처럼 실제 주소를 매핑하고 있지 않고, 해당 세그먼트의 PMT 메모리 주소를 매핑한다. PMT는 페이징 시스템에서 봤던 것과 동일한 모습이다. 이런 형태의 테이블 구조로, 아래와 같이 프로세스를 나눈다. 아래는 다이렉트 매핑 방식에서의 메모리 매핑 플로우이다. 이런 시스템을 사용했을 때, 실제 메모리에 접근하는 것을 포함해서 세 번을 접근해야 한다. 또한, 테이블 수도 증가하므로, 메모리 소모도 비교적 커지고, 매핑 과정 자체가 길어진다. 이런 오버헤드가 있지만, 사용함으로써 얻는 장점이 크기 때문에 사용되는 것이라고 볼 수 있다고 한다. Page sharing, protection에 강점이 있고, 메모리 할당 및 관리에 드는 오버헤드가 작다. Reference https://www.youtube.com/playlist?list=PLBrGAFAIyf5rby7QylRc6JxU5lzQ9c4tN 운영체제 교과서","link":"/posts/os/memory-allocation-and-mapping/"},{"title":"Husky로 Git hooks 관리하기","text":"Husky는 git commit, git push의 앞이나 뒤에 동작하는 hook을 쉽게 만들어주는 툴이다. 최근 Electron Bolierplate를 사용하려고 하는데, Repository에 정말 많은 개발 툴이 사용되고 있는 걸 봤고, 그 와중에 Husky라는 것을 처음 알게 되었다. Husky로 Git hooks 사용하기특히 최근에는 Prettier가 온전하게 작동하지 않는 (조금 더 자세한 증상을 설명하자면, 일정 시간 동안 잘 작동 하다가, 또 안 되면 프로젝트 안에 있는 .prettierrc를 열고 저장(ctrl + s)를 해서 VSCode에 인식해줘야 다시 정상 작동하는) 현상이 개인적으로 발생했다. 이유는 모르겠지만 아무튼 그 문제로 커밋하고 코드리뷰를 할 때, 팀의 컨벤션이 안 맞는 코드가 보이게 되고, 그 스타일을 체크하고 다시 커밋을 해야 하는 경우가 생겼다. 그래서 commit 하기 전에 formatting 정도는 진행하면 좋겠다고 생각이 되어서 개발 툴로서 Husky를 사용하려고 한다. Husky의 사용법은 그야말로 완전히 단순하다. 우선 패키지를 설치한다. 1yarn add husky -D 그 다음 package.json에 Husky의 hook과 관련된 설정을 해주면 된다. .huskyrc, .huskyrc.json, .huskyrc.js, husky.config.js와 같은 별도의 설정 파일을 만들 수도 있다. 그렇지만 이번 글에서는 그렇게 많은 설정이 들어가지 않기 때문에 간단하게 package.json을 사용한다. 12345678{ ... &quot;husky&quot;: { &quot;hooks&quot;: { &quot;pre-commit&quot;: &quot;yarn format &amp;&amp; yarn lint;&quot; } }} 이렇게 간단하게 git hook을 관리할 수 있다. 오픈 소스의 경우는 push하기 전에 test를 돌려보게 할 수도 있을 것 같다. 후기좋고, 간단한 툴. 신경써야 할 코드리뷰를 줄여주는 기능을 할 것 같다.","link":"/posts/etc/husky/"},{"title":"React UI Kit 만들기 대작전","text":"리액트 프로젝트를 새로 시작하기 앞서, 프로젝트 전반에 걸쳐서 사용되는 UI Kit을 만들기 위해서 두 가지 시도를 했던 내용에 대해서 적어보려고 한다. 한 가지는 Private Package이고, 다른 한 가지는 StoryBook이다. 프라이빗 패키지는 npm에 공개된 패키지가 아니라, 특정 권한이 있어야 설치가 가능한 패키지를 말한다. 프라이빗 패키지는 클라이언트, 백앤드 작업을 모두 TS로 하면서 가끔 공유하고 있는 타입이나 클래스를 일종의 패키지로 구성한 적이 있다. StoryBook은 클라이언트 개발자들에게 Swagger와 같은 역할을 해주는 컴포넌트 뷰어라고 볼 수 있다. 스토리북에 대해서는 이전부터 알고는 있었지만, 클라이언트 개발에 항상 반만 진심이었던 필자는, 사용할만한 상황이 오지 않았다. 이런 UI 킷을 만들어야 하는 경우가 생기면, UI가 어떻게 생겼는지 보면서 개발하기가 애매한 경우가 있는데, 이런 문제를 깔끔하게 해결해줬다. 스토리북 메인에 등장하는 소개 영상 Private Package를 관리하는 것 보다, Submodule을 만드는 것도 하나의 방법이 될 수 있을 것 같다. Private UI Library Package 만들기필자는 프라이빗 패키지를 Github에 배포하는 편이다. Github에서 패키지 관리를 할 수 있는 기능을 추가하면서 가능해졌는데, 이를 위해서는, 기본으로는 NPM 패키지로 배포가 되기 때문에, 배포할 때 패키지가 어디에 배포되는지 명시해야 한다. 물론 패키지를 가져올 때에도 그렇다. 실제로 가장 별로라고 생각했던 점 중에 하나이다. 그 역할을 해주는 것이 바로 .npmrc이다. 이름 그대로 npm 설정 파일이고, 이 파일 안에 배포하는 패키지가 Github package repository를 바라보게 하고, 그 패키지 권한이 있는 토큰을 함께 명시해주면 된다. 아래와 같은 모습을 갖게 된다. 12//npm.pkg.github.com/:_authToken=[string]@[package-scope]:registry=https://npm.pkg.github.com/ 첫 번째 줄에는 npm.pkg.github.com에 접근할 때 권한을 어떤 권한으로 접근하게 되는지를 설정한다. authToken은 권한마다 다르게 발급된다. 권한을 받으려면, 깃헙의 Settings 페이지 아래 Developer settings 페이지에서 받을 수 있다. 두 번째 줄에는 배포시에 레포지토리가 속한 Organization? 이라고 할 수 있는 걸 정의 해둬야 한다. 예를 들어서 toy-program이라는 이름의 Organization에 배포한 경우, @toy-program을 붙여주면 된다. 이 작업이 끝나면, npm.pkg.github.com에 접근할 때는 [string] 권한을 가지고 접근하고, 이 프로젝트의 package.json에서 @[package-scope]에 포함되는 패키지들은 npm.pkg.github.com에서 찾도록 해라! 라고 설정을 마친 것이다. package.json 설정하기기본 설정package.json에도 몇 가지 설정해줘야 할 부분이 있다. 어떤 파일을 대상으로 하는지, 어디 레포지토리에 올리는지, 배포 관련 설정 등이 있다. 아래 내용들을 추가해줬다. 12345678910111213141516171819{ ... &quot;files&quot;: [ &quot;lib&quot; ], &quot;main&quot;: &quot;lib/index.js&quot;, &quot;types&quot;: &quot;lib/index.d.ts&quot;, &quot;repository&quot;: { &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;[github repository url]&quot; }, &quot;private&quot;: false, &quot;publishConfig&quot;: { &quot;directory&quot;: &quot;lib&quot;, &quot;registry&quot;: &quot;https://npm.pkg.github.com/&quot; } // ...} 위 설정을 마쳐야 Typescript로 만든 React 컴포넌트를 lib 디렉토리에 빌드 한 후 배포할 수 있었다. 배포 명령은 yarn publish로 할 수 있다. 주의할 점 몇 가지가 있다면, yarn publish 후, 명시된 버전이 배포된 다음엔 같은 버전으로 배포할 수가 없다. 버저닝에 신중을 가할 필요가 있다. 또한, 타입스크립트 프로젝트의 경우, 배포시 빌드가 완료된 부분을 배포하는데, 웹팩이나 tsconfig에서 Output으로 설정한 부분을 files에 추가해줘야 한다. 필자의 경우는 일반적으로 dist 이름이 붙는 부분을 lib으로 한 것이다. husky, lint-staged 설정하기husky와 lint-staged는 깃 훅을 쉽게 설정해둘 수 있도록 도와주는 패키지이다. 이전 글에서 husky를 간단하게 설명한 바 있지만, 아주 간략한 소개였다. 근데 사실 그 이상 디테일한 내용도 없다. 그냥 깃 훅을 적절하게 조합하면 되는 건데, lint-staged는 깃 훅이 동작할 때, 특정 파일에 대해 특정 커맨드를 실행할 수 있게 해준다. 123456789101112{ ... &quot;husky&quot;: { &quot;hooks&quot;: { &quot;pre-commit&quot;: &quot;lint-staged&quot; } }, &quot;lint-staged&quot;: { &quot;*.{js,ts,tsx}&quot;: &quot;eslint --fix&quot; } ...} 위와같이 package.json에 추가해서, 커밋 전에 린트를 돌려주고 수정 가능하면 수정하도록 설정해뒀다. 이제 협업을 할 때 정한 코드 컨벤션에 맞지 않는 코드가 커밋되는 것을 방어할 수 있다. webpack.config.js 설정하기“웹팩 설정이 뭐 크게 다르겠냐”고 생각했지만, 뼈 아픈 결과를 맛 보고, 꼭 기록해야지 하는 생각이 들게 만든 부분이 있었다. 바로 모듈과 관련된 내용이었다. 웹팩에는 라이브러리를 빌드할 때 빌드 타겟을 설정할 수 있는 옵션이 있다. libraryTarget이라고 하는 옵션인데, 웹팩 문서의 설명에는 라이브러리가 어떻게 exposed 될지 설정하는 값이며, 기본 값은 var이라고 한다. 웹팩이 빌드 할 때, ES 방식으로 import 할 수 있도록 target 설정이제 빌드 하고 사용해봐야겠다고 생각하고, 빌드 후 yarn publish로 배포까지 했지만 리액트 프로젝트에서 사용이 안되는 문제가 있었다. 알아본 결과, 패키지를 빌드할 때, target을 설정해줘야 하는 이슈였다. 아마 그냥 빌드를 하면 ES Module 방식으로 import 할 수 없는 상태로 빌드되는 것 같았다. 12345 ...output: { ... libraryTarget: 'umd'}, Webpack에 output 옵션 중 libraryTarget에 umd 옵션을 붙여주는 방식으로 해결했다. libraryTarget 옵션은 라이브러리가 어떻게 exposed 될지를 결정하는 옵션이다. 기본 옵션은 var이다. 옵션 값이 어떤 것을 의미하는지 웹팩 공식문서 내용을 통해 알아보려고 했지만, 무슨 의미인지 쉽게 알 수 없어 이 링크에서 내용을 학습했다. umd옵션은 ES Module 방식, CommonJS 방식, AMD 방식 모두 모듈을 불러올 수 있도록 번들링 해준다. 위 모듈 임포트 방식에 대해서 정확히 알지 못 한다면, TOAST 개발 블로그 글을 꼭 참고하길 바란다. 정말 최고. 내용은 짧았지만, 이 문제를 경험하면서, 역시 나는 프론트를 제대로 공부하지 못했구나 싶은 생각도 들었다. StoryBook 붙이기What’s a Story스토리는 렌더링 된 형태의 UI 컴포넌트를 뜻한다. 개발자는 여러 스토리들을 한 컴포넌트에 할당해 만들 수 있다. 컴포넌트가 여러 상태를 가질 수 있기 때문에, 지원 중인 모든 상황에 대해 스토리를 만들 수 있게 구성되어있다. 예를 들자면, 버튼의 Primary한 상태, Secondary 상태 등으로 나눌 수도 있고, 사이즈 별로 나눌 수도 있는 것이다. 인스톨이미 컴포넌트 진행이 되어있는 프로젝트의 루트 디렉토리에서 설치할 수 있다. 1npx sb init 매지컬리 아주 깔끔하게 스토리북이 설치된다. 스토리북에 버튼 한 번 붙여보기예를 들어서 만들어진 RoundButton을 스토리북에 올려보자. 먼저 결과적으로는 이렇게 나오고 있다. 스토리북은 stories 디렉토리 안에 있는 모든 **.stories.tsx 형태를 컴포넌트로 올려주는 것 같다. stories/Buttons/RoundButton.stories.tsx라는 파일을 만들어서, 다음과 같이 코드를 짰다. 12345678910111213141516import React from 'react';// also exported from '@storybook/react' if you can deal with breaking changes in 6.1import {Story, Meta} from '@storybook/react/types-6-0';import {RoundButton, RoundButtonProps} from '@/components/RoundButton';export default { title: 'Button/RoundButton', component: RoundButton} as Meta;const Template: Story&lt;RoundButtonProps&gt; = (props) =&gt; &lt;RoundButton {...props} /&gt;;export const Default = Template.bind({});Default.args = { children: 'RoundButtonComponent'}; 이 글에서는 자세하게 설명되어 있지는 않지만, 일반적으로 Typescript 프로젝트를 구성할 때 절대경로를 만들어서 사용하는 편이다. @/* 형태의 절대경로는 루트 디렉토리 아래 packages라는 디렉토리로 경로를 찾게 해두었다. 이때 문제가 발생했는데, 절대 경로를 스토리북에서 제대로 컴파일 하지 못 하는 문제였다. 확인 결과, 스토리북에서 사용하는 webpack config는 따로 설정해줘야 한다는 점이다. 설정은 .storybook안에 두면 된다. 12345678910const ForkTsCheckerWebpackPlugin = require('fork-ts-checker-webpack-plugin');const {TsconfigPathsPlugin} = require('tsconfig-paths-webpack-plugin');const path = require('path');module.exports = ({config, mode}) =&gt; { ... config.resolve.plugins.push(new TsconfigPathsPlugin()); ... return config;}; 위 코드처럼, 스토리북이 웹팩 설정을 적용하기 전에, 주입을 넣는 것 방법으로 작동한다. TsconfigPathsPlugin를 사용해서 tsconfigs.json의 Path 설정을 공유하도록 만들었다. 별 다른 옵션을 안 넣어줬는데, Props에 따라서 컨트롤을 넣어준다. 저렇게 컨트롤러를 사용해볼 수도 있고, Docs처럼 컴포넌트를 정리해주기도 한다. 스크린 뷰를 휴대폰이나 테블릿 사이즈로 맞춰 볼 수도 있고 그리드를 넣어서 볼 수도 있다. 문서만 확인해봐도 다양한 기능들을 쉽게 쓸 수 있게 생기긴 했는데, 하나씩 시도해보고, 쓸만한 거 있으면 도입하면서 알아갈 필요가 있을 것 같다. Reference https://kishu.gitbooks.io/webpack/content/ https://ui.toast.com/weekly-pick/ko_20190418 https://webpack.js.org/configuration/output/#outputlibrarytarget https://avengersrhydon1121.tistory.com/276","link":"/posts/etc/great-operation-creating-react-ui-kit/"},{"title":"React환경에서 Hooks로 Redux 사용 하기","text":"지난 데모를 통해서 리덕스 비동기 처리를 어떤 미들웨어를 사용할지 결정을 했다. 이번에는 리덕스 hooks를 사용해 리덕스를 사용해보려고 한다. 사실 지난 프로젝트에서는 connect 함수와 mapStateToProps, mapDispatchToProps를 사용해서 리덕스를 연결시켰는데, 이러한 방법도 있고 hooks를 사용할 수도 있기 때문에, 방법을 한 가지 더 공부해보고 프로젝트에서 결정해보려고 한다. 패턴은 한 번 지정하면 같은 프로젝트 내에서는 동일하게 작성되는 경향이 있기 때문에… 데모를 만들어보고 확인을 해보려고 한다. 이 데모는 기본적으로 지난 react-async-demo에서 만들었던 프로젝트를 기반으로 만들어져 있다. Hooks를 적용하기 전스크린에서 Redux와 연결하려면 아래와 같은 형태로 연결 해줬던 구조이다. 1234567891011121314151617181920import PostScreen from &quot;./PostScreen&quot;;import { connect } from &quot;react-redux&quot;;import { getPost } from &quot;../../redux/modules/post/thunkReducer&quot;;const mapStateToProps = (state: any, ownProps: any) =&gt; { const { post } = state; return { ...ownProps, ...post };};const mapDispatchToProps = (dispatch: any, ownProps: any) =&gt; { return { ...ownProps, getPost: () =&gt; dispatch(getPost()) };};export default connect(mapStateToProps, mapDispatchToProps)(PostScreen); 위처럼 어떤 스크린이 src/screens/PostScreen/index.tsx파일이 있고 컨테이너와 프레젠터로 구성하는 프로젝트인 경우, 해당 스크린의 입구가 되는 index.tsx에서 리덕스의 dispatcher나 state를 props로 전달하는 방식이다. react-redux의 Hooksreact-redux의 Hooks는 공식 홈페이지에서도 소개하고 있는 부분이 몇 개 없어서 좋았다. useSelector() useDispatch() useStore() 위 세 가지만 공식적으로 있는 상태이다. 하나씩 확인 해보자. useSelectoruseSelector는 리덕스 스토어의 state에 접근할 수 있게 해준다. 공식 홈페이지에 소개된 바로는 아래와 같다. 1const result : any = useSelector(selector : Function, equalityFn? : Function) 먼저 selector는 mapStateToProps를 connect에 넣는 것과 개념적으로 동일하다. selector는 리덕스 스토어 전체를 유일한 인자값으로 받고 함수 내부에서 이 인자값을 토대로 필요한 state만 골라 리턴하면 된다. 아주 간단한 동작 방식. 12// 코드 작성 전 상상의 코딩... 이렇게 하면 되겠지?const { postList } = useSelector(store =&gt; store.post.postList); selector는 컴포넌트가 렌더링 될 때 호출된다. 그리고 useSelector는 리덕스의 store를 subscribe 하는 구조이기 때문에 action이 dispatch되면 마찬가지로 selector를 돌린다. 하지만 mapStateToProps를 사용하는 것과는 차이가 있다고 하는데 차이점이 아래와 같다. 리턴 값으로 객체가 아니라 어떤 값이든 넘길 수 있음. action이 dispatch 되면 이전 결과와 얕은 비교를 한다. 다르다면 무조건 re-render를 하게 된다. ownProps를 인자로 받지 않는다. (다만 closure 형태로 만들 수는 있음) memoizing selector를 사용할 때 더 신경 써야 한다. 기본적으로 === 비교를 통해 동일성을 체크한다. 동등함 비교에 대한 얘기가 많고 이 부분을 주의해야 하는 것 같은데 예를 들어서 결과가 객체 형태라면, selector 결과 값이 항상 다르다고 판단할 거고, 성능면에서 좋지 못한 결과를 줄 것이라고 생각된다. 그리고 함수가 처음 렌더링 될 때는 selector가 무조건 호출 되지만, action이 store에 dispatch될 때는 그 결과가 현재 selector가 호출한 결과와 다른 경우에만 re-render 된다. 그 비교 연산자로 ===를 사용하고 있다는 것이고, 반면 connect의 경우에는 ==를 사용하고 있다는 것 같다. (다만, connect는 반환되는 객체가 새로운 객체인지 판단하지 않고 각 필드를 비교하기 때문에, 항상 re-render 되지는 않는다. - 반면 위에서 말한대로 별다른 옵션이 없다면 useSelector가 객체를 리턴하는 경우 항상 새로운 객체로 판단할 것이고, 그런 경우 객체의 필드값이 같더라도 무조건 re-render 된다.) 위와 같은 이유로, 만약 여러 값을 useSelector를 통해서 스토어에서 값을 가져와야 한다면, 아래와 같은 방법을 생각할 수 있다. 단일 필드를 리턴하는 useSelector를 여러번 사용한다. Reselect, 또는 복수의 값을 하나의 객체로 리턴해주는 memoized selector 라이브러리를 사용 (값이 진짜 바뀌는 경우에만 새로운 객체를 반환하는 형태의 라이브러리) react-redux의 shallowEqual 함수를 equalityFn 자리의 인자값으로 사용한다. 사실 세 번째 얘기를 할려고 앞에 두 가지 방법을 던져 둔 것 같다. 비교와 관련한 이슈가 많이 있어서, 어떻게 비교할 건지 정의하는 함수 자리가 있는 것 같은데, 이 부분에 react-redux에서 제공하는 shallowEqual 함수가 있다고 알려주고 있는 것이다. 문서가 말하는 대로라면, shallowEqual을 사용하면 객체를 리턴하는 경우의 성능상의 이슈를 최소화 할 수 있다고 생각된다. 제공해주는 예시는 다음과 같다. 1234import { shallowEqual, useSelector } from &quot;react-redux&quot;;// laterconst selectedData = useSelector(selectorReturningObject, shallowEqual); useDispatchdispatch 함수의 참조를 리턴하는 hooks이다. 1const dispatch = useDispatch(); 이 부분은 그냥 connect에서 mapDispatchToState를 만들었던 것과 동일하게 그냥 dispatch를 사용하면 된다. 예시는 아래 실제 데모를 만드는 과정에서 볼 수 있다. useStoreProvider를 통해서 들어온 리덕스의 store의 참조를 전달해준다. 자주 사용되는 걸 추천하지 않고 useSelector를 이용하는 걸 추천하고 있다. hooks 내용은 사실상 이 정도에서 끝난다. connect를 대체하지 않는 게 더 좋을 것 같다는 생각은 일단 들긴 하는데 데모 앱을 구성해보자. Hooks로 기존 데모 앱 바꾸기우선 리덕스를 붙이기 전 상태처럼 src/screens/PostScreen/index.tsx를 간단하게 바꿨다. 1export default PostScreen; 다음 간단하게 useDispatch와 useSelector를 사용해서 리덕스 스토어에서 값을 가져와 봤다. 1234567891011121314151617181920212223242526272829303132import React, { useEffect } from &quot;react&quot;;import Presenter from &quot;./Presenter&quot;;import { useDispatch, useSelector } from &quot;react-redux&quot;;import { getPost } from &quot;../../redux/modules/post/sagaReducer&quot;;const PostScreen: React.FC = props =&gt; { const dispatch = useDispatch(); const postList = useSelector&lt;any&gt;( store =&gt; store.post.postList, (left, right): any =&gt; (left as Array&lt;any&gt;).every((value, index) =&gt; { const sameTitle = value.title === (right as Array&lt;any&gt;)[index].title; const sameBody = value.body === (right as Array&lt;any&gt;)[index].body; return sameTitle &amp;&amp; sameBody; }) ) as Array&lt;any&gt;; const fetchPost = (): any =&gt; dispatch(getPost()); useEffect(() =&gt; { // 렌더링이 얼마나 되는지 확인용 console.log(&quot;rendering!!!!&quot;); }); const onClick = () =&gt; { fetchPost(); }; return &lt;Presenter onClick={onClick} postList={postList} /&gt;;};export default PostScreen; shallowEqual을 사용해도 내부적으로 한 번 더 객체 형태라 같은 값을 가져와도 랜더링이 한 번 더 되는 것 같아서, 직접 비교하는 함수를 구성해봤다. 123456789const postList = useSelector&lt;any&gt;( store =&gt; store.post.postList, (left, right): any =&gt; (left as Array&lt;any&gt;).every((value, index) =&gt; { const sameTitle = value.title === (right as Array&lt;any&gt;)[index].title; const sameBody = value.body === (right as Array&lt;any&gt;)[index].body; return sameTitle &amp;&amp; sameBody; })) as Array&lt;any&gt;; 복잡하기는 한데, equalityFn은 이전 상태를 left에, 다음 상태를 right로 둔다 (왜 타입 이름을 이렇게 했을까, prevState, nextState로 했으면 좋겠다 물론 그냥 내가 적을 때 그렇게 하면 되지만, 일단 데모에서는 타입에서 제공하는 이름으로 적었다). 넘어오는 값이 객체 형태로 깊다면, 이런식으로 직접 비교하는 함수를 작성해줘야 랜더링이 두 번 안 된다. 후기Hooks 형태로 작성한다는 점 자체는 상당히 매력적인 것 같다. 그리고 비교 함수를 직접 넣을 수 있다는 점, Container에 로직을 넣는 방식이라는 점, props interface에 대해서 고민하지 않아도 된다는 점, (데모에서는 항상 any를 애용하지만, 실제 타입스크립트 프로젝트에서는 타입에 대해서 자주 고민하게 된다.) 등은 매력적인 것 같다. 다만 hooks 형태로 하게 되면, Container가 지나치게 복잡해지진 않을까 싶기도 하고, 협업할 때 특별한 패턴이나 아키텍처가 아니라서, 리덕스와 관련된 디버깅을 할 때 조금 더 시간 소비가 될 수 있지 않을까 싶기도 하다. Reference https://react-redux.js.org/api/hooks","link":"/posts/etc/react-redux-hooks/"},{"title":"Red-Black Tree","text":"알고리즘 문제를 해결하다가, 이중 우선순위 큐 문제를 만났다. 문제 시간 제한이 6초라고 되어있길래, Binary Search Tree(BST)로 구현하려고, HashMap을 사용했다. 다만 만든 트리 구조가 최악의 경우 더하는 연산이 O(n)이기 때문에, 시간 초과가 났다. AVL이 생각이 났는데, 어떻게 구현하는지를 알지 못해 알아보던 도중 자바에 TreeMap 구조가 Red-Black Tree(RBT)라는 걸 알게되었다. TreeMap 이전에 RBT를 더 자세히 공부하고 싶어 위키트리 내용을 최대한 풀어서 정리했다. Red-Black TreeRBT는 자료의 추가, 삭제, 검색에서 최악의 경우도 일정한 Worst Case를 보장한다. 이런 특성은, 실행 시간이 중요한 경우, 일정 시간 실행을 보장해야 하는 경우 등 유용하게 쓰인다. 앞서 생각했던 AVL Tree는 균형에 대한 더 엄격한 기준이 있어서, 삽입과 삭제시 더 많은 회전이 필요하다 (균형을 위한) 특성 RBT는 각각의 노드가 레드, 블랙 속성을 가지고 있는 BST이다. BST의 조건에 추가적으로 다음과 같은 조건을 만족해야 한다. 노드는 레드 또는 블랙이다. 루트 노드는 블랙이다. 모든 리프 노드들(NIL)은 블랙이다. 레드 노드의 자식노드 양쪽은 언제나 모두 블랙이다. 즉, 레드 노드는 연달아 나타날 수 없고, 블랙 노드만 레드 노드의 부모 노드가 될 수 있다. 어떤 노드로부터 시작되어 그에 속한 하위 리프 노드에 도달하는 모든 경로에는 리프 노드를 제외하면 모두 같은 개수의 블랙 노드가 있다. 네 번째, 다섯 번째 속성때문에, 극단적인 최단 경로의 경우 블랙 노드만 존재하는 경우이고, 극단적인 최장 경로의 경우 레드와 블랙 노드가 섞여 나오는 경우이다. 이 경우를 가정했을 때에도, 최단 경로와 최장 경로의 차이는 최단 경로의 두 배보다 항상 작다. 동작일단, 읽기 작업은 일반적인 BST처럼 진행하면 된다. 다만, 삭제와 삽입은 위 특성을 만족시키기 위해 추가적인 작업이 필요하다. 아래는 위키피디아에서 설명하는 알고리즘을 본인이 읽기 쉽게 정리한 내용이다. 사용된 이미지도, 위키피디아에서 가져왔다. 삽입RBT에서 삽입은, BST의 삽입과 동일한 방식으로 우선 삽입 후, 색을 붉은 색으로 만든다. 그 다음 단계는 그 주위 노드의 색에 따라 다르다. RBT에서는 삼촌노드(uncle node)에 대한 개념이 나온다. 부모 노드의 형제 노드를 뜻한다. 1234567function getUncle(node) { const g = getGrandparent(node); if (!g) return undefined; // 할아버지 노드가 없으면, 형제노드는 당연히 없다. const uncle = g.left === node.parent ? g.right : g.left; return uncle;} 이제 다음 삽입하는 경우의 수를 살펴보자. 처음 루트에 N이 삽입될 때이 경우, RBT의 첫 번째 속성을 위해 N은 검은색이 된다. 그 이후는 RBT가 유지된다. 1234function insertCase1(node) { if (!node.parent) node.color = BLACK; else insertCase2(node);} 새로운 노드의 부모 노드가 검은색인 경우이 경우도 RBT가 유지된다. 1234function insertCase2(node) { if (node.parent.color === BLACK) return; insertCase3(node);} 부모 노드와 삼촌 노드가 모두 붉은색인 경우레드 블랙 트리의 다섯 번째 특성을 유지하기 위해서 부모 P와 삼촌 U를 검은색으로 바꾸고, 할아버지 G를 붉은 색으로 바꾼다. 이렇게 되면, 새로 추가되는 노드 N은 검은 부모를 갖게 된다. 다만 이 경우에, 할아버지 노드에서 두 번째 속성 또는 네 번째 속성을 만족하지 않을 수 있다. 이를 위해 지금까지 설명한 세 가지 케이스를 할아버지 노드에도 적용한다. 12345678910function insertCase3(node) { const u = getUncle(node); if (u &amp;&amp; u.color === RED) { node.parent.color = BLACK; u.color = BLACK; const g = getGrandparent(node); g.color = RED; insertCase1(g); } else insertCase4(node);} 부모는 붉은색인데, 삼촌은 검은색이고, N이 부모의 오른쪽(왼쪽) 자식 노드이고, 부모는 할아버지의 왼쪽(오른쪽) 자식 노드인 경우조건부터 아주 복잡한데, 위 순서대로 따라와보면 확인해야 하는 조건은 새로운 노드의 위치와 부모 노드의 위치라는 사실을 알 수 있다. 이 경우, N과 P의 역할을 변경하기 위해 왼쪽 회전을 해야한다. 1234567891011function insertCase4(node) { const g = getGrandparent(node); if (node === node.parent.right &amp;&amp; node.parent === g.left) { rotateLeft(n.parent); n = n.left; //과거에 부모 노드였던 것을 새로운 노드로 바꿔줌 } else if (n === n.parent.left &amp;&amp; n.parent === g.right) { rotateRight(n.parent); n = n.right; // 위와 마찬가지 } insertCase5(n);} 이런 설명은 없지만, insertCase4는 부모 노드와 새로운 노드의 선을 직선으로 맞추는 느낌이 있다. 이는 다음 케이스로 넘어가기 위한 준비 과정처럼 보인다. 그 이후, 부모 노드였던 P가 RBT의 다섯 번째 특성을 어기는 문제를 해결해야 한다. 그런데, 현재 상황이, 부모 노드였던 P를 새로운 노드로 판단한다면, 4번째 조건 중 “N이 부모의 오른쪽 자식 노드이고,” 부분만 바뀌게 되므로, 다음 다섯 번째 조건에서 해결할 수 있다. 왼쪽회전은 오른쪽 자식 노드를 그 노드의 부모 노드와 바꾸는 과정이다. 대략 아래와 같은 느낌 1234567891011p = n.parent;c = n.right;if (c.left) c.left.parent = p;n.right = c.leftn.parent = c;c.left = n;c.parent = p;if (p) { if (p.left == p) p.left = c; else p.right = c;} 오른쪽 회전은 이와 반대 방향으로 돌리는 걸 의미한다. 부모 노드가 붉은 색이지만, 삼촌 노드가 검은색이고, 새로운 노드 N이 부모의 왼쪽(오른쪽) 자식 노드이고, 부모 할아버지 노드의 왼쪽(오른쪽) 자식인 경우위의 케이스에서 한 번 왼쪽 회전을 통해 이 케이스로 왔든, 처음부터 부모 노드의 왼쪽으로 들어왔든 이 케이스로 들어오게 된다. 이 경우에는 할아버지 노드 G에 대해서 오른쪽 회전을 진행한다. 이 결과로 이전 부모 노드인 P는 새로운 노드 N과 할아버지 노드 G를 자식으로 갖게 된다. P는 붉은색, G는 검은색일 수 밖에 없으므로 P와 G의 색을 바꾸면, RBT의 네 번째 속성을 만족하게 된다. 1234567function insertCase5(node) { const g = getGrandparent(node); node.parent.color = BLACK; g.color = RED; if (node === node.parent.left) rotateRight(g); else rotateLeft(g);} 삭제아주 복잡한 삽입 과정이 끝났다. 이제 삭제 작업을 보자. 삭제 방식 역시 BST의 삭제 방법을 기본적으로 따른 후, 색을 맞춘다. BST에서 노드 N을 삭제할 때, N의 자식이 둘이라면, 왼쪽 자손 중 최대 또는 오른쪽 자손 중 최소 노드를 N 위치로 옮긴다. 이때 문제가 되는 것은, 옮긴 주변의 레드 블랙 특성을 위반하는지이다. 옮긴 노드는 해당 트리에서 최소 또는 최대라는 특성때문에 최대 1개의 자손만을 가질 수 있다. 결국, 삭제 작업은 최대 1개의 자식만 가진 노드를 삭제하는 것과 마찬가지이다. 따라서, 자식이 1개 이하인 상황을 가정하고 삭제를 설명한다. 삭제할 노드는 M, M의 자식을 C라고 하자. 우선 간단한 상황을 먼저 해결하자. 삭제 대상이 붉은 색이라면, 그냥 M을 삭제한 뒤, C를 M대신 치환하면 된다. 또, M이 검은 노드, C가 붉은 노드라고 가정하자, 검은 노드를 삭제하면 경로의 검은 노드의 수가 같아야 하고, 또는 붉은 노드의 부모는 검은 노드여야 한다는 원칙을 위반할 가능성이 있다. 하지만, C를 검은색으로 바꿔주면 모두 해결 가능하다. 어려운 상황은 M, C가 모두 검은 노드일 때 발생한다. 일단 M을 자식 노드 C로 치환한다. 이 상황에서 C를 N으로 명명하고, N의 형제 노드를 S, 부모 노드를 P라고 명명한다. N이 새로운 루트가 될 때이전 삭제한 노드가 루트였음을 의미하고, 이는 모든 경로에서 검은색을 하나 줄인 것이다. 이전 특성이 모두 유지되므로 상황은 종료 1234function deleteCase1(node) { if (!node.parent) return; deleteCase2(node);} S가 붉은 노드인 경우, N이 P의 왼쪽(오른쪽) 자식인 경우현재 상황에서 부모 노드 P가 검은 노드임이 확실한 상황이다. (삼촌 노드가 붉은 색이기 때문에) 이 경우, P와 S의 색을 바꾸고, P에서 왼쪽(오른쪽) 회전하면, S가 N의 할아버지 노드가 된다. 이제 N이 검은색 형제 노드와 붉은 부모 노드를 가지고 있게 된다. 이 상황을 만든 후 다음 케이스에서 나머지 문제(모든 경로에서 검은 노드의 수가 같지 않다.)를 해결하도록 한다. 이후 상황에 대해서 S는 할아버지 노드가 된 기존 S가 아니라, 새롭게 바뀐 N의 형제 노드를 의미한다. 1234567891011function deleteCase2(node){ const s = getSibling(node); if (s.color == RED) { node.parent.color = RED; s.color = BLACK; if (n === n.parent.left) rotateLeft(node.parent); else rotateRight(node.parent); } deleteCase3(node);} P, S, 그리고 S의 자식들이 검은색인 경우S를 붉은 노드로 만들면 된다. S를 지나는 모든 경로에서 검은 노드 수를 하나 줄여, N을 지나는 경로의 검은 노드 수와 맞출 수 있다. 그러나, P 이하의 트리에서 검은 노드의 수가 1개 줄은 것이기 때문에, P를 지나지 않던 경로가 있다면, P를 지나는 경로보다 검은 노드 수가 한 개 더 많게 된다. 따라서, P에 deleteCase1을 재귀적으로 적용할 필요가 있다. 12345678910111213function deleteCase3(node) { const s = getSibling(node); if ( node.parent.color === BLACK &amp;&amp; s.color === BLACK &amp;&amp; s.left.color === BLACK &amp;&amp; s.right.color === BLACK) { s.color = RED; deleteCase1(node.parent); } else { deleteCase4(node); }} S와 S의 자식들은 검은색이지만, P는 붉은색인 경우S와 P의 색을 바꿔주면 된다. 이는 S를 지나는 경로의 검은 노드 개수에 영향을 주지는 않지만, N을 지나는 경로에 대해서는 검은 노드의 개수를 1개 증가시킨다. 이를 통해 삭제된 원래 검은 노드의 개수를 보충한다. 1234567function deleteCase4(node) { const s = getSibling(node); if (node.parent.color === RED &amp;&amp; s.color === BLACK &amp;&amp; s.left.color === BLACK &amp;&amp; s.right.color === BLACK) { s.color = RED; n.parent.color = BLACK; } else deleteCase5(node);} S가 검정, S의 왼쪽(오른쪽) 자식이 빨강, 오른쪽(왼쪽) 자식이 검정이고, N이 부모의 왼쪽(오른쪽) 자식인 경우S를 오른쪽(왼쪽) 회전시켜 S의 왼쪽 자식이 S의 부모 노드이자, 새로운 S(N의 형제)가 되도록 한다. 그리고 S의 색을 부모 노드와 바꿔준다. 이 상태를 만들고 나서, deleteCase6으로 넘겨준다. 123456789101112131415function deleteCase5(node) { const s = getSibling(node); if (s.color === BLACK) { if (node === n.parent.left &amp;&amp; s.right.color === BLACK &amp;&amp; s.left.color === RED) { s.color = RED; s.left.color = BLACK; rotateRight(s); } else if (n === n.parent.right &amp;&amp; s.left.color === BLACK &amp;&amp; s.right.color == RED) { s.color = RED; s.right.color = BLACK; rotateLeft(s); } }}deleteCase6(node); S가 검은색, S의 오른쪽(왼쪽) 자식이 빨강, N이 P의 왼쪽(오른쪽) 자식인 경우P를 왼쪽(오른쪽)으로 회전해서 S가 P와 S의 오른쪽(왼쪽) 자식 노드의 부모 노드가 되도록 한다. 그리고, P와 S의 색을 바꾸고, S의 오른쪽(왼쪽) 자식노드를 검은색으로 만든다. 이렇게 만들어진 트리에서 N은 하나의 검은 조상 노드를 더 갖게 되었고, 따라서 N을 지나는 경로는 검은색을 하나 더 갖게 된다. 반면, N을 통과하는 케이스는 두 가지 경우의 수를 갖는다. N의 새로운 형제 노드를 지나는 경우: 변경되기 전과 같은 순서를 만난다. P -&gt; S -&gt; ... 이던게, S -&gt; P -&gt; ... 로 바뀐 것 뿐 N의 새로운 삼촌 노드를 지나는 경우: 변형 되기 전에 P -&gt; S -&gt; S 오른쪽 자식 -&gt; ... 경로를 가지던 것이, 변형 후, S -&gt; S 오른쪽 자식 -&gt; ...로 바뀌게 되었다. 하지만 붉은 노드가 하나 줄었기 때문에 경로 상에서는 같은 검은 노드를 만나게 된다. 12345678910111213function deleteCase6(node) { const s = getSibling(node); s.color = node.parent.color; node.parent.color = BLACK; if (node == node.parent.left) { s.right.color = BLACK; rotateRight(node.parent); } else { s.left.color = BLACK; rotateRight(node.parent); }} Reference https://ko.wikipedia.org/wiki/%EB%A0%88%EB%93%9C-%EB%B8%94%EB%9E%99_%ED%8A%B8%EB%A6%AC https://ko.wikipedia.org/wiki/%ED%95%B4%EC%8B%9C_%ED%95%A8%EC%88%98 https://coding-factory.tistory.com/557","link":"/posts/etc/red-black-tree/"},{"title":"리팩토링 실습하기","text":"약 한 달 조금 넘는 기간 동안 팀에서 듀데이트가 걸린 프로젝트를 완성하느라 블로그와 기타 공부를 완전히 손 놓고 정말 바쁘게 지냈다. 덕분에 머리도 2달 만에 자른 것 같고, 운동도 단 한 번도 안 했다. 책은 꾸역꾸역 받아서 읽긴 했는데 이전에 여유 있던 때처럼 읽지는 못 했다. 아무튼 현재는 그런 시간이 끝났고, 돌아가는 프로젝트의 코드베이스를 뭇지게 정리해두고 싶어져서 당분간은 기능 개발과 기존 기능들에 대한 리팩토링을 동시에 반반 정도 가져가려고 한다. 물론 새로운 기능들 역시 리팩토링을 진행 하면서! 이 글은 리팩토링 책을 읽은 후 우리 프로젝트의 코드베이스를 대상으로 실습을 진행하기 위해 어떤 리팩토링을 하게 되는지 정리한 글이다. 지난 번에 리팩토링을 읽고 간단한 후기를 작성한 적이 있는데, 해당 책을 읽고 난 후, 팀에서 짠 코드를 정리하는 것으로써 좋은 실습을 경험할 수 있을 것 같다고 생각이 들어서 팀원들과 함께 리팩터링 전과 후를 비교하는 세션? 스터디?를 진행하기로 했다. 리팩토링 책에서 말해주는 리팩토링 기법은 사실 우리가 이름 붙이지 않고, 자주 해오던 방식인 경우가 많다. 다만 해당 책에서 이름을 잘 붙여주고, 절차에 대해서도 잘 안내해주고 있기 때문에 이름과 절차를 차용해서 글을 작성했다. 앞으로 몇 번에 걸쳐서 적용할 예정이지만, 현재는 간단하지만 너무 간단하진 않은 내용들 위주로 리팩터링을 해볼 예정이다. 이번에 읽고 진행하는 부분은 다음과 같다. 함수 추출하기 함수 인라인하기 함수 선언 바꾸기 변수 선언 바꾸기 함수 선언, 변수 선언은 이름만으로도 적당한 방식으로 선언을 변경한다, 이름을 변경한다라는 것을 알기 쉬우니, 특별히 변경된 과정을 서술하지는 않을 것이다. 함수 추출하기는 React로 작성된 코드에서 유사한 컴포넌트에서 Base가 되는 부분들을 추출해 새로운 컴포넌트를 작성하는데 사용될 것으로 보인다. 함수 인라인은 개발과정에서 불필요하게 레이어 하나를 더 사용하고 있는 로직을 한 단계 플랫하게 만드는 과정을 진행할 것 같다. 함수 추출하기우선 함수 추출을 하기 위해서는 아래와 같은 절차를 따라 리팩토링을 진행하라고 권고한다. 절차 함수를 새로 만들고 목적을 잘 드러내는 이름을 붙임 (무엇을 하는 함수인지 잘 드러나게 네이밍 해야 한다.) 원본 함수에서 추출할 내용을 복사 붙여넣기 한다. 추출된 코드 중 참조 하지 못하고 있는 변수를 찾아 매개변수화 한다. 원본 함수에서 추출된 함수를 호출하도록 변경한다. 테스트한다. 적용클라이언트 부분에 복사 붙여넣기 코드의 산물로 다른 이름과 다른 상황에서 쓰이기는 하지만 UI가 동일하여 토시 하나 틀리지 않고 똑같은 코드가 몇 가지가 있다. 필자는 이러한 코드를 병적으로 싫어한다. 왜 컴포넌트를 재활용하지 않고 복사 붙여넣기 해야 하는가? 바쁘기 때문에 적절하게 수정하기 어려웠을 수 있지만, 프로젝트가 배포된 이후기 때문에 리팩토링에 시간을 들이며 현재 프로젝트에 그러한 문제가 있는 부분들을 수정해봤다. 책에서 마틴파울러는 “3 스트라이크”를 기준으로 리팩토링을 하라고 한다. 중복되는 경우가 두 번째 발생 하더라도 참고 넘기자는 건데, 충분히 재활용 될 법 하다고 판단되는 것들은 그냥 두 번 반복이 되어도 리팩토링 했다. 리액트는 함수를 추출한다는 의미가 컴포넌트화한다는 것과 유사한 느낌이 있다. 12345678910111213141516171819202122232425262728293031// src/screens/QnAFileScreen/Presenter.tsximport React from 'react';import AuthImageComponent from '@/components/AuthImageComponent';import DeleteHeaderContainer from '@/containers/DeleteHeaderContainer';import styled from 'styled-components/native';interface Props { token: string; uri: string; activateDelete: boolean; onPressDelete: () =&gt; void;}const Presenter = (props: Props) =&gt; { const { token, uri, onPressDelete, activateDelete } = props; return ( &lt;&gt; &lt;DeleteHeaderContainer title=&quot;첨부 파일&quot; onPressDelete={onPressDelete} /&gt; &lt;Container&gt; &lt;AuthImageComponent token={token} uri={uri} /&gt; &lt;/Container&gt; &lt;/&gt; );};export default Presenter;const Container = styled.View` width: 100%; flex: 1;`; 1234567891011121314151617181920212223242526272829303132// src/screens/LicenseFileScreenimport React from 'react';import AuthImageComponent from '@/components/AuthImageComponent';import DeleteHeaderContainer from '@/containers/DeleteHeaderContainer';import styled from 'styled-components/native';interface Props { activateDelete: boolean; token: string; uri: string; onPressDelete: () =&gt; void;}const Presenter = (props: Props) =&gt; { const { token, uri, onPressDelete, activateDelete } = props; return ( &lt;&gt; &lt;DeleteHeaderContainer title=&quot;첨부 파일&quot; onPressDelete={onPressDelete} /&gt; &lt;Container&gt; &lt;AuthImageComponent token={token} uri={uri} /&gt; &lt;/Container&gt; &lt;/&gt; );};export default Presenter;const Container = styled.View` width: 100%; flex: 1;`; 위 두 가지 놀랍도록 단 한 자도 틀리지 않고 파일 내 모든 코드가 같은 두 예시는 다른 파일에 해당한다. 각각 QnAFileScreen, LicenseFileScreen에 해당하는데, 해당 스크린이 하는 역할은 첨부된 이미지 디테일을 보여주는 스크린이다. 12345678910111213141516171819202122232425262728293031323334353637383940414243// src/containers/ImageDetailContainer/ImageDetailContainer.tsximport React from 'react';import AuthImageComponent from '@/components/AuthImageComponent';import DeleteHeaderContainer from '@/containers/DeleteHeaderContainer';import styled from 'styled-components/native';export interface ImageDetailContainerProps { activateDelete: boolean; headerTitle?: string; token: string; uri: string; onPressDelete: () =&gt; void;}const ImageDetailContainer = (props: ImageDetailContainerProps) =&gt; { const { token, uri, onPressDelete, activateDelete, headerTitle = '첨부 파일' } = props; return ( &lt;&gt; &lt;DeleteHeaderContainer title={headerTitle} activateDelete={activateDelete} onPressDelete={onPressDelete} /&gt; &lt;Container&gt; &lt;AuthImageComponent token={token} uri={uri} /&gt; &lt;/Container&gt; &lt;/&gt; );};export default ImageDetailContainer;const Container = styled.View` width: 100%; flex: 1;`; 위와 같이 내부를 옮겨 넣고, 다른 곳에서의 재사용을 위해서 적절하게 Props를 받아주도록 했다. 이제 이 부분을 위 두 가지 스크린의 Presenter 부분 대신 사용하면 된다. 두 스크린을 사용하는 곳을 ImageFileScreen과 같이 하나로 사용하고 싶은 생각도 들었지만, 묶여있는 Screen 부분의 Container 로직의 차이가 있고 Navigator와 엮여있는 부분도 있어서 이 정도까지만 했다. 함수 인라인하기함수 인라인하기는 추출하기의 반대이다. 잘못 추출되었거나, 불필요한 간접 호출 등을 인라인 하면 된다. 아래와 같은 절차를 책에서 소개한다. 절차 다형 메서드인지 확인 (서브클래스에서 오버라이딩 하고 있다면, 인라인 하지 말 것) 인라인할 함수를 호출하는 곳들 모두 확인 각 호출문을 함수 본문으로 교체 (교체 할 때마다 해당 함수를 테스트한다.) 함수(인라인 된) 정의된 부분을 삭제한다. 적용개발 중인 서버에서 결제하는 부분은 API 호출을 통해 이루어지는데, 해당 API 호출 코드와 데이터베이스 작업을 함께 처리하는 유틸 함수가 있었다. 개발 과정에서 API 호출과 데이터베이스 작업을 분리하게 되면서 API 호출하는 부분은 불필요한 레이어를 가지고 있게 되었다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Injectable()export class PaymentUtil { ... private async payUsingCard( payment: Payment, prescription: Prescription ): ServiceData&lt;PayApi.PayResponse&gt; { try { ... // 실제 API 결제 처리 하는 곳 return { success: true, result: payResponse }; } catch (e) { return { success: false, error: e }; } } // 실제 호출하는 메서드 async pay(payment: Payment, prescription: Prescription): ServiceData&lt;PayApi.PayResponse&gt; { try { const pay = await this.payUsingCard(payment, prescription); if (!pay.success) throw pay.error; // 원래 데이터베이스 작업이 있던 자리 return { success: true, result: pay.result }; } catch (e) { return { success: false, error: e }; } }} 위 상황처럼 실제 호출하는 메서드는 단순히 payUsingCard라는 private 메서드를 간접 호출하고 있는 구조이다. 이 부분을 간단하게 표현하자면 아래와 같은 로직이라는 뜻이다. 나머지 에러처리를 위한 try, catch 부분은 데이터베이스 작업이 포함되어있던 잔재이다. 1234567@Injectable()export class PaymentUtil { async pay(payment: Payment, prescription: Prescription): ServiceData&lt;PayApi.PayResponse&gt; { const ret = await this.payUsingCard(payment, prescription); return ret; }} 인라인 해야 하는 부분은 payUsingCard 메서드이고, 해당 메서드는 private이기 때문에, public으로 변경한 후 이름을 pay로 바꿔주었다. 기존의 pay 메서드는 제거했다. 위와 같은 비슷한 문제가 클라이언트 부분에도 있었다. React Native로 현재 작성된 코드는 Container Presenter 아키텍처를 갖추고 있다. 이 과정에서 불필요하게 컴포넌트를 구성해서 한 줄 짜리 구분을 나누는 등 불필요하게 Props를 내리거나, 또는 컴포넌트를 과하게 분리한 경우가 발생되었다. 아래와 같은 예시가 있다. 123456789101112131415161718192021222324252627282930import React from 'react';import styled from 'styled-components/native';import { Prescription } from 'bdyg-dto';interface Props { historyDataList: Array&lt;Prescription&gt;;}const HistoryHeaderContainer = (props: Props) =&gt; { const { historyDataList } = props; return ( &lt;Wrapper&gt; &lt;ListCountText&gt;{`전체보기(${historyDataList?.length}건)`}&lt;/ListCountText&gt; &lt;/Wrapper&gt; );};export default HistoryHeaderContainer;const Wrapper = styled.View` flex-flow: row; padding: 0 5%; align-items: center; justify-content: space-between;`;const ListCountText = styled.Text` font-size: 14px; font-weight: 600;`; 위 Header가 굳이 분리될만큼 복잡한 컴포넌트가 되어야 할까? 해당 부분은 컴포넌트간의 결합이나, 분리되어야 하는 로직이 존재하지 않는다. 단순하기 Wrapper와 Text만 가지고 있는 컨테이너이다. 팀에서 로직과 뷰를 분리하는 Container - Presenter 구조를 사용하면서도, 컴포넌트, 컨테이너, 스크린으로 구분하는 조합 단위도 존재한다. 이때 컨테이너라는 단어가 겹치기 때문에 팀 내부적으로는 로직 처리 하는 Container는 후자에서 말한 부분으로 이름 붙이고 (즉, 만약 HomeScreen의 로직을 담는 Container는 그냥 HomeScreen이 된다. HeaderContainer의 로직을 담는 Container는 HeaderContainer가 된다.) Presenter는 해당 디렉토리 안에 Presenter로 이름 붙이고 작성한다. 코드를 보면서 헷갈릴 것 같아 주석을 담았다. 이 부분이 굳이 Screen에서 분리되어야 할까? 불필요한 분리인 것으로 판단해서 함수를 인라인하기로 결정했다. 아래는 원래 사용되던 HistoryScreen에서 HistoryHeaderContainer를 인라인한 Presenter의 모습이다. 123456789101112131415161718192021222324const Presenter = (props: Props) =&gt; { const { getHistoryData, historyDataList, onPress, isLoading, count } = props; return ( &lt;Wrapper&gt; &lt;HeaderWrapper&gt; &lt;ListCountText&gt;{`전체보기(${count}건)`}&lt;/ListCountText&gt; &lt;/HeaderWrapper&gt; &lt;ListWrapper&gt; &lt;FlatList data={historyDataList} renderItem={renderItem(onPress)} refreshing={isLoading} onRefresh={getHistoryData} onEndReached={getHistoryData} onEndReachedThreshold={1} style={styles.flatList} contentContainerStyle={styles.container} keyExtractor={(item, index) =&gt; String(index)} ListEmptyComponent={() =&gt; &lt;Text&gt;아직 접수 내용이 없습니다.&lt;/Text&gt;} /&gt; &lt;/ListWrapper&gt; &lt;/Wrapper&gt; );}; 이번 리팩토링에서 시도한 함수 인라인은 아주 간단한 리팩토링이었기 때문에 (책에서는 간단하지 않은 경우가 많다고 했지만!..) 이 정도만 적용 예시를 적는 걸로 마치고 다음 리팩토링 적용을 진행해보자. 후기예시로 적은 것들 외에도 많은 리팩토링이 이루어질 수 있는 내용이었다. 기본적이지만, 적용할 거리가 많았다. 조금더 고급진 리팩토링 기법은 다음 장부터 나타나지 않을까 싶다.","link":"/posts/etc/refactoring-practice/"},{"title":"RxJS Operator 01 - Creation, Filtering","text":"RxJS에 대한 개념과 동작 방식 등을 간단하게 알아봤고, RxJS의 핵심이라고 볼 수 있는, Operator 사용에 대해서 몇 가지만 간단하게 뽑아서 정리해보려고 한다. 이번 글에서는 Creation Operator, Filtering Operator를 확인 보려고 한다. Creation Operator는 &quot;rxjs/operators&quot;에서 가져오지 않고, &quot;rxjs&quot;에서 가져와서, 그냥 생성 함수라고 설명하는 경우도 있는 것 같다. Creation Operatorfrom옵저버블로 변환이 가능한 객체를 옵저버블로 바꿔주는 오퍼레이터이다. 가능한 객체 타입 종류는 다음과 같다. Observable Array Promise Iterable String 123456789101112import { from } from &quot;rxjs&quot;;from([1, 2, 3, 4]).subscribe({ next: console.log, complete: () =&gt; console.log(&quot;complete&quot;),});// 1// 2// 3// 4// complete 아래는 이터러블을 사용한 예시이다. 123456789101112131415161718192021222324252627import { from } from 'rxjs';import { take } from 'rxjs/operators'; function* generateDoubles(seed: number) { let i = seed; while (true) { yield i; i = 2 * i; }}const iterator = generateDoubles(3);const observable = from(iterator).pipe(take(10));observable.subscribe(console.log); // Logs:// 3// 6// 12// 24// 48// 96// 192// 384// 768// 1536 Promise의 경우에 resolve 되는 값을 next처럼 동작하게 하고, reject 되는 값을 error 쪽에서 동작하게 만든다. 비동기로 동작하게 된다. 함수 스케줄러를 두 번째 인자 값으로 넣어서 비동기로 동작하게 만들 수도 있다. 123456789101112131415import { from, asyncScheduler } from 'rxjs';console.log('start');const array = [10, 20, 30];const result = from(array, asyncScheduler);result.subscribe(console.log);console.log('end');// Logs:// start// end// 10// 20// 30 fromEventEventEmitter 객체 또는 브라우저 이벤트를 Observable로 바꿔야 하는 경우에 사용한다. 12345678import { fromEvent } from &quot;rxjs&quot;;fromEvent(document.querySelector(&quot;btn&quot;)!, &quot;click&quot;).subscribe({ next: (event) =&gt; { // ... 이벤트 처리 }, // ... observer 내용}); of나열된 인자 값들을 순서대로 발행하도록 옵저버블을 만들어준다. 간단한 경우에는 new Observable을 사용하는 것 보다 훨씬 간단하게 사용할 수 있다. 1234567891011121314151617import { of } from &quot;rxjs&quot;;const complete = () =&gt; console.log(&quot;complete&quot;);of(1, 2, &quot;a&quot;, &quot;b&quot;, 3, 4, [&quot;string&quot;, 10]).subscribe({ next: console.log, complete});// 1// 2// a// b// 3// 4// [ 'string', 10 ]// complete range일정 범위 안에 있는 숫자 값을 next 값으로 발행하는 Observable을 만든다. 반복문을 실행하는 것과 같은 형태이다. 123456789101112131415161718import { range } from &quot;rxjs&quot;;range(1, 5).subscribe(console.log);// 1// 2// 3// 4// 5range(2, 5).subscribe(console.log);// 2// 3// 4// 5// 6range(5, 1).subscribe(console.log);// 5 헷갈릴 수 있는 부분은 range(start, end) 형태가 아니라, range(start, count) 형태인 점이다. 즉, 시작 지점과 몇 번 동작하게 되는지를 인자 값으로 넣는 것이다. 다른 Creation Operator 처럼 스케줄러를 마지막 인자값으로 넣어줄 수 있다. 즉, range(start, count, scheduler?) 형태를 인자로 받는다. intervalms 단위의 값을 인자 값으로 넣으면, 그 텀마다 (첫 시작도 기다린 후 첫 값을 내보냄) 음이 아닌 정수를 차례대로 반환하는 함수이다. 1234import { interval } from &quot;rxjs&quot;;interval(1000).subscribe(console.log);// 1초 뒤에 0, 그 이후로 1초마다 1씩 늘어난 값을 내보낸다. 이 Creation Operator 역시, 스케줄러를 두 번째 인자 값으로 넣을 수 있다. timer지정한 시간이 지난 다음 값을 한 개 내보내는 함수이다. 두 번째 값으로 그 다음 값에 대한 주기를 줄 수 있다. 123456789import { timer } from &quot;rxjs&quot;;timer(1000).subscribe(console.log);// 0timer(1000, 500).subscribe(console.log);// 0// 1// 2 ... 첫 번째 값은 number | Date를 입력 받을 수 있다. 어느 정도 시간 만큼 기다릴 지 설정하는 것이고, 두 번째는 첫 값 발행 후 어느 정도 시간 간격을 둘지이다. 만약 인자를 하나만 넣으면 값은 한 개만 나오게 된다. throwError값을 발행 하다가, 특정 에러를 발생시키고 종료 해야 하는 상황에 사용할 수 있는 함수이다. 다른 생성 함수 처럼, scheduler를 두 번째 인자 값으로 넣을 수 있다. 123456789import { throwError } from &quot;rxjs&quot;;throwError(new Error(&quot;error&quot;)).subscribe({ next: console.log, error: (err) =&gt; console.error(`error: ${err.message}`), complete: () =&gt; console.log(&quot;complete&quot;),});// error: error error 상황을 만들어내는 것을 알 수 있다. Filtering OperatorObservable이 값을 발행할 때, 필터링을 해주는 작업을 하는 연산자를 Filtering Operator라고 한다. filter배열의 filter 함수처럼, 조건을 통과 하면 값을 발행하도록 만든다. 123456789import { range } from &quot;rxjs&quot;;import { filter } from &quot;rxjs/operators&quot;;range(1, 5) .pipe(filter((x) =&gt; x % 2 === 0)) .subscribe(console.log);// 2// 4 필터링의 인자값으로 사용되는 함수를 predicate 함수라고 한다. last마지막 값 한 개만 내보내는 Filtering Operator이다. next로 내보내지는 값을 모아두다가, complete이 호출되기 바로 전의 값을 내보낸다. 다만, complete이 없는 Observable에서는, 값을 내보내지 않는 상태가 된다. last 함수도 인자로 predicate를 받을 수 있다. 해당 조건을 만족하는 값을 내부적으로 최신 값으로 유지하다가, 마지막에 최신 값을 내보내는 방식으로 동작한다. 12345678910import { range } from &quot;rxjs&quot;;import { last } from &quot;rxjs/operators&quot;;range(1, 10).pipe(last()).subscribe(console.log);// 10range(1, 10) .pipe(last((x) =&gt; x &lt;= 3)) .subscribe(console.log);// 3 take정해진 개수만큼구독하고 구독을 해제하게 해준다. 별도로 unsubscribe를 동작하지 않아도 되기 때문에, 코드가 간결해지고, 동작 파악이 비교적 쉽다. 123456789import { interval } from &quot;rxjs&quot;;import { take } from &quot;rxjs/operators&quot;;interval(1000).pipe(take(5)).subscribe(console.log);// 1// 2// 3// 4// 5 interval은 무한히 실행될 수 있는 연산자인데, take(5) 파이핑을 통해서 5개 까지만 한정하도록 하는 모습이다. takeUntiltake은 개수 제한을 두는 형태로 동작하지만, takeUntil은 조건 제한을 두는 형태이다. 특정 조건이 만족하면, unsubscribe 한다. 주의할 점은 인자 값으로 받는 타입이 Observable이다. 1234567import { fromEvent, interval } from 'rxjs';import { takeUntil } from 'rxjs/operators';const source = interval(1000);const clicks = fromEvent(document, 'click');const result = source.pipe(takeUntil(clicks));result.subscribe(console.log); 위 코드는 interval로 값이 내보내지는 것에서, 페이지를 클릭하게 되면, 구독을 멈추게 된다. takeWhile이 연산자는 filter 연산자처럼, predicate를 인자로 받는다. 만족하는 동안은 구독을 하고 있다가, 만족하지 않게 되면, unsubscribe 하는 구조이다. 12345678import { interval } from &quot;rxjs&quot;;import { takeWhile } from &quot;rxjs/operators&quot;;interval(1000) .pipe( takeWhile((x) =&gt; x &lt;= 10) ) .subscribe(console.log); 1부터 차례대로 값을 내보내다가 10을 초과하게 되면 구독을 해제한다. skip원하는 만큼 내보내지는 값을 생략하고 그 다음 값부터 내보내지도록 한다. 12345678interval(250).pipe(skip(3)).subscribe(console.log);// 3// 4// 5// 6// 7// ... 0, 1, 2 값은 생략하고 그 다음 값부터 내보낸다. skipUntiltakeUntil에서처럼, 인자 값으로 Observable을 받고, 인자로 받은 Observable이 구독 시작되는 조건일 때부터 값을 내보낸다. 12345678910111213import { interval, fromEvent } from 'rxjs';import { skipUntil } from 'rxjs/operators';const intervalObservable = interval(1000);const click = fromEvent(document, 'click');const emitAfterClick = intervalObservable.pipe( skipUntil(click));const subscribe = emitAfterClick.subscribe(value =&gt; console.log(value));// clicked at 4.6s. output: 5...6...7...8........ or// clicked at 7.3s. output: 8...9...10..11....... 클릭이 일어난 시점 이후부터 값을 내보내기 시작한다. skipWilepredicate로 들어가는 인자 함수가 만족하면 값을 건너 뛴다. 1234567891011import { interval } from &quot;rxjs&quot;;import { skipWhile } from &quot;rxjs/operators&quot;;interval(300) .pipe(skipWhile((x) =&gt; x &lt; 4)) .subscribe(console.log);// 4// 5// 6// 7// ... 4 보다 작은 수는 생략하고, 그 다음 값부터 내보내는 결과를 확인할 수 있다. Reference https://rxjs-dev.firebaseapp.com/guide/operators RxJS 프로그래밍 (책)","link":"/posts/etc/rxjs-operator-01/"},{"title":"RxJS 빠르게 배우기 01 - Overview","text":"새로운 토이프로젝트를 하기 앞서, 프로젝트에 도입하기 위한 RxJS를 한 번 볼까 한다. RxJS를 사용하면 비동기 처리에 대해 정말 직관적이고 간단한 코드를 쓸 수 있다는 풍문을 들었기 때문에, 리액티브 프로그래밍에 대한 느낌도 잡아보고자… 이 시리즈는 주로 RxJS 공식 문서의 Overview 부분을 번역하면서, 참조로 ReactiveX에서 설명해주는 개념적인 부분을 공부한 내용이다. 모든 내용을 담지는 않았다. OverviewObservable Sequences를 사용해 비동기와 이벤트 베이스의 프로그램을 함성하기 위한 라이브러리라고 RxJX 공식 문서에서는 설명하고 있다. Observable 이라고 하는 것은 RxJS의 핵심 타입이고 그 외 Observer, Schedulers, Subjects 라고 하는 타입들을 제공하고 있다. RxJS에서 비동기 이벤트를 다루기 위한 기본적인 개념들은 다음과 같다. Observable: 호출 가능한 미래 값들과 이벤트의 모임 Observer: Observable이 전달해주는 값을 어떻게 처리할지에 대한 콜백 Subscription: Observable의 수행, 기본적으로 실행의 취소에 유용함 Operators: map, filter, concat, reduce와 같은 연산으로 대상을 처리하는 함수형 프로그래밍 스타일을 가능하게 하는 순수 함수들 Subject: EventEmitter와 동일하다. 그리고 값이나 이벤트를 여러 옵저버들에게 동시 전달해주는 유일한 방법이다. Schedulers: 중앙 집중된 동시성 컨트롤을 위한 dispatcher이다. 간단한 예시12// 일반적인 Event Listener 방식document.addEventListener(&quot;click&quot;, () =&gt; console.log(&quot;Clicked&quot;)); 123import { fromEvent } from &quot;rxjs&quot;;fromEvent(document, &quot;click&quot;).subscribe(() =&gt; console.log(&quot;Clicked&quot;)); 순수성RxJS를 강력하게 만드는 것은 값을 순수 함수들을 이용해서 만들어내는 것이다. 이게 뭔 뜻이냐면, 에러들에대한 취약점이 더 적다는 것을 의미한다. 123// state를 지저분하게 할 수 있는 코드let count = 0;document.addEventListener('click', () =&gt; console.log(`${++count}` times)); 1234567// RxJS를 사용해서 state를 독립import { fromEvent } from &quot;rxjs&quot;;import { scan } from &quot;rxjs/operators&quot;;fromEvent(document, &quot;click&quot;) .pipe(scan((count) =&gt; count + 1, 0)) .subscribe((count) =&gt; console.log(`Clicked ${count} times`)); scan 오퍼레이터는 배열에서 reduce와 유사하게 동작한다. 초기 값을 넣고, 다음 값이 어떤 처리를 거쳐야 하는지 콜백함수를 넣으면 되는 것 같음. FlowRxJS는 이벤트들의 흐름을 Obserbables를 통해서 컨트롤할 수 있게 도와주는 연산자들을 가지고 있는데, 아래 예시는 1초마다 클릭되도록 하는 코드를 비교한 것이다. 1234567891011// 일반 자바스크립트let count = 0;let rate = 1000;let lastClick = Date().now() - rate;document.addEventListener(&quot;click&quot;, () =&gt; { if (Date.now() - lastClick &gt;= 1000) { console.log(`Clicked ${++count} times`); lastClick = Date.now(); }}); 1234567891011// RxJSimport { fromEvent } from &quot;rxjs&quot;;import { throttleTime, scan } from &quot;rxjs/operators&quot;;fromEvent(document, &quot;click&quot;) .pipe( throttleTime(1000), scan((count) =&gt; count + 1, 0) ) .subscribe((count) =&gt; console.log(`Clicked ${count} times`)); Values값들을 Observables를 통해서 값들을 변형할 수 있다. 아래 코드는 현재 마우스의 x 축 값을 매 클릭마다 더하는 상황을 비교한 것이다. 1234567891011let count = 0;const rate = 1000;let lastClick = Date.now() - rate;document.addEventListener(&quot;click&quot;, (event) =&gt; { if (Date.now() - lastClick &gt;= rate) { count += event.clientX; console.log(count); lastclick = Date.now(); }}); 12345678import { fromEvent } from &quot;rxjs&quot;;import { throttleTime, map, scan } from &quot;rxjs/operators&quot;;fromEvent(document, &quot;click&quot;).pipe(throttleTime(1000)), map((event) =&gt; event.clientX), scan((count, clientX) =&gt; count + clientX, 0).subscribe((count) =&gt; console.log(count) ); 그 외 값을 만들어내는 연산자들에는 pluck, pairwise, sample 등이 있다. 후기함수형 프로그래밍을 잘 알지는 못 하지만 느낌이 난다고 해야 하나, 그렇다. Overview 단원 답게 얄팍했지만 예시 코드를 보면 꽤 괜찮아 보인다. 모르는 용어 사용에 직관적으로 익숙해질 필요가 있는 것 같다. 알고 있던 영어 단어와 실제로 사용되는 느낌에서 이질감을 느낀다. Reference https://rxjs-dev.firebaseapp.com/guide/overview http://reactivex.io/ https://www.learnrxjs.io/","link":"/posts/etc/rxjs-quicklearn-01/"},{"title":"RxJS 빠르게 배우기 02 - Observables","text":"지난 글에서 RxJS로 코드를 짰을 때 어떤 느낌인지 살짝 확인 했다. 이제는 핵심적인 개념들과 실제로 어떻게 구현 되어 있는지 등을 하나씩 확인해보자. 옵저버 패턴과 옵저버블 (Observable)Rx의 개념은 옵저버 패턴을 확장한 것이라고 알려져잇다. 그렇다면 옵저버 패턴은 뭘까? 쉽게 설명하자면 관찰의 역할을 하는 옵저버 객체들을 서브젝트라는 객체에 등록한 다음, 서브젝트 객체의 상태 변경이 일어나면 notify()를 호출해 알리게 되는 구조이다. 위키 문서 이미지 자바스크립트에서는 가장 대표적으로 EventListener를 예시로 들 수 있다. 옵저버블 타입은 ReactiveX에서 기존 옵저버 패턴에서 데이터가 없다는 것을 알리는 complete 메서드, 에러가 발생했음을 알리는 error 메서드가 추가된 것이라고 한다. RxJS는 옵저버 패턴을 적용한 Observable을 중심으로 동작한다. Observable 객체는 객체 안에서 여러 값이나 이벤트를 취급하고, 옵저버의 함수를 호출해 필요한 값이나 이벤트를 보내는 역할을 한다. Observable이 사용되는 예시를 간단하게 확인해보자. 아래 코드는 subscribe 될 때 1, 2, 3 값을 즉시 (동기적으로) 밀어넣는 Observable이다. 그리고 값 4는 1초 뒤에 subscribe 호출로 값이 들어가고, 그 다음 완료된다. 123456789101112import { Observable } from &quot;rxjs&quot;;const observable = new Observable((subscriber) =&gt; { subscriber.next(1); subscriber.next(2); subscriber.next(3); setTimeout(() =&gt; { subscriber.next(4); subscriber.complete(); }, 1000);}); Observable을 호출하고 값들을 보기 위해서는 subscribe를 해야 한다. 1234567891011121314151617181920212223242526import { Observable } from &quot;rxjs&quot;;const observable = new Observable((subscriber) =&gt; { subscriber.next(1); subscriber.next(2); subscriber.next(3); setTimeout(() =&gt; { subscriber.next(4); subscriber.complete(); }, 1000);});console.log(&quot;Just before subscribe&quot;);observable.subscribe({ next(x) { console.log(`got value ${x}`); }, error(err) { console.error(`something wrong occurred: ${err}`); }, complete() { console.log(&quot;done&quot;); },});console.log(&quot;Just after subscribe&quot;); 위 코드는 콘솔에 아래와 같은 결과를 보여준다. 1234567Just before subscribegot value 1got value 2got value 3Just after subscribegot value 4done Pull vs PushPull, Push라고 하는 것은 데이터를 만들어내는 쪽 (Producer)과 데이터를 소비하는 쪽 (Consumer) 사이에 어떻게 커뮤니케이션 하는가를 설명해주는 두 프로토콜이다. 먼저 Pull 프로토콜을 사용하는 경우에는 Consumer가 데이터를 언제 받을지를 결정하는 시스템이다. Producer는 데이터가 언제 Comsumer에게 전달될지를 알 수 없는 상태로 있는다. 모든 자바스크립트 함수는 기본적으로 Pull 시스템을 따르고 있다. 함수는 Producer라고 볼 수 있고, 해당 함수를 호출하는 코드는 호출한 결과로 하나의 리턴 값을 받는 “pulling“을 한 것이라고 볼 수 있다. ES2015는 generator 함수들과 iterators를 소개하고 있다. 이것들은 다른 타입의 Pull 시스템을 따르고 있다. iterator.next()를 호출하고 있는 코드는 Consumer라고 할 수 있고, 그 Consumer는 다양한 값들을 이터레이터(Producer)로부터 받을 수 있게 된다. Pull 방식이 익숙한 방식임을 알게 되었고, 그렇다면 Push는 그 반대겠지? Push 시스템에서는 Producer가 Consumer에게 언제 데이터를 보내줘야 할지를 결정하게 된다. 반대로 Consumer는 데이터를 언제 받게 되는지를 알지 못하는 상태로 있는다. Promise는 가장 일반적인 Push 시스템이라고 볼 수 있다. 자바스크립트를 알고 있다면 무슨 느낌인지 알 수 있기 때문에 디테일한 설명은 생략한다. Observable과 function의 차이는 뭘까? Observable은 다수의 값을 시간 흐름에 따라서 리턴할 수 있다는 것이다. 쉽게 말하자면 함수는 아래와 같은 동작을 지원하지 않는다. 1234function foo() { return 42; return 100; // 이 값은 절대 리턴되지 않는다.} 반면 Observable는 아래와 같은 것이 가능하다. 123456789import { Observable } from &quot;rxjs&quot;;const foo = new Observable((subscriber) =&gt; { subscriber.next(42); subscriber.next(100); subscriber.next(200);});foo.subscribe(console.log); 아래와 같이 나타난다. 12342100200 subscribe는 next 값이 소진될 때까지 map이 도는 것과 유사하게 동작하는 것 같다. 이 부분은 그냥 여러 값을 리턴하는 것처럼 동작할 수 있군 하면서 넘어가면 될 것 같다. 정리해보자면, 여러 값을 내보낼 수 있는가? 또는 하나의 값만 내보낼 수 있는가?의 기준과 Pull 방식을 따르는가? Push 방식을 따르는가?의 기준으로 설명할 수 있다. 해당 기준으로 나누면 아래와 같다고 볼 수 있다. Push Pull 단일 Promise Function 복수 Observable Iterable 옵저버블에 대한 자세한 내용Obeservable이 맡는 관심사는 다음과 같다. Observable 생성 Obeservable 구독 Obeservable 실행 Obeservable 처리 (구독 취소) 생성Observable 생성자는 subscribe 함수 하나를 인자로 받는다. 아래 예시는 hi 문자열을 매 초마다 subscriber에게 전달하는 코드이다. 1234567import { Observable } from 'rxjs';const observable = new Observable(function subscribe(subscriber) { const id = setInterval(() =&gt; { subscriber.next('hi') }, 1000);}); 일반적으로는 new Observable을 사용해 만들 수 있지만, of, from, interval 등 생성 함수들을 이용해 보통 만들어진다. 구독과 실행구독은 데이터를 전달할 콜백을 제공해 함수를 호출하는 것이라고 볼 수 있고, 실행은 Observable에서 발행하는 값을 사용하는 것 이라고 설명할 수 있다. 구독은 subscribe 함수를 이용한다. 1234567891011121314const observable = new Observable(function subscribe(subscriber) { try { subscriber.next(1); subscriber.next(2); // 실행되는 값들 subscriber.complete(); subscriber.next(3) // 전달되지 않는다. } catch (e) { subscriber.error(e); }})observable.subscribe(console.log); // 구독 new Observable(function subscribe(subscriber) { ... })는 구독이 발생하는 시점부터 실행되는 내용이다. Observable의 실행에서 전달할 수 있는 값의 타입은 세 가지이다. Next 알림: 특정 값을 전달해준다. Error 알림: 자바스크립트의 에러 또는 예외를 전달한다. Complete 알림: 값을 보내지 않는다. Next 타입은 핵심적인 데이터라고 볼 수 있고, 나머지 두 가지는 실행 중 한 번만 발생하게 되는 것들이다. Next는 무한으로 보낼 수 있지만, 나머지 두 개가 발생하고 나서는 값을 전달할 수 없게 된다. subscribe 메서드를 통해서 next() 메서드로 전달받는 값을 처리하는 콜백을 넣을 수 있다. complete 메서드가 실행되면 구독은 끊어지고, 이후 값이 전달되지 않게 된다. 구독 취소구독 취소는 Observable의 구독을 해제하는 것이다. subscribe 메서드를 호출하면 리턴 값으로 Subscription 객체를 리턴한다. 1const subscription = observable.subscribe(console.log); 이 객체는 진행중인 실행을 나타내고, 최소한의 실행 취소를 위한 API를 가지고 있다. 123456import { from } from 'rxjs';const observable = from([10, 20, 30]);const subscription = observable.subscribe(console.log);subscription.unsubscribe(); 만약 create() - (아마 과거에 Observable.create() 함수를 말하는 것 같다. 현재 문서에서는 new Observable(function subscribe(subscriber) {...})) 형태로 모두 바뀌었다. - 블로그 주인)를 사용한 경우에는 커스텀한 unsubscribe 함수를 리턴해줘야 한다. 12345678910const observable = new Observable(function subscribe(subscriber) { const intervalId = setInterval(() =&gt; { subscriber.next('Hi'); }, 1000) // 구독 해제 하는 방법을 함수형태로 리턴해준다. return function unsubscribe() { clearInterval(intervalId); }}) Reference RxJS 프로그래밍 (책) https://rxjs-dev.firebaseapp.com/guide/observable","link":"/posts/etc/rxjs-quicklearn-02/"},{"title":"React환경에서 Redux로 비동기 처리 하기","text":"새롭게 공동 창업을 시작한 이후로 개인적으로 공부하고 글 쓸 시간이 확 줄어서 계획했던 글을 쓰지를 못 하고 있다 (거창하게 JS에서 공부하기 어려웠던 부분들을 정리해보겠다고 목차만 써놨는데 공부만 하고 정리하지 못 하고 있다). 연휴를 맞아서 개인적인 시간이 남기도 하고 새로 빌딩 중인 앱에 리덕스를 붙여야 하는 상황이라 개별적으로 평소에 리덕스의 비동기를 위한 미들웨어를 공부해보려고 한다. 사실 비동기 처리를 위해서 미들웨어를 사용해야 한다는 것에 대해서 잘 이해를 못 했다. 자바스크립트는 기본적으로 비동기 처리가 간단하게 되는데 왜 비동기를 위한 미들웨어가 필요할까? 라는 생각이 들고 지금 글을 쓰는 과정에서도 그 부분을 해결하기 위한 관점이 가장 클 것 같다. 우선 프로젝트는 CRA를 통해 구성했다. 그리고 필요한 패키지들을 설치했다. 1234npx create-react-app . --typescriptyarn add react-redux redux axios styled-componentsyarn add @types/{styled-components,react-redux} -D # redux는 타입스크립트로 구성되어 있어서 설치할 필요가 없다고 함 Redux에 대해서리덕스는 너무나 유명한 상태 관리 시스템이다. 상태 관리는 React 프로젝트에서 너무나 필요한 기능이지만, 리덕스라는 개념은 정확하게는 리액트에서만 통용되는 것은 아니고, 그냥 말 그대로 상태를 관리하도록 돕는 일종의 시스템이다. 리덕스를 소개하는 페이지에서는 리덕스의 중요 원칙을 세 가지로 정리했다. Single source of truth애플리케이션의 모든 상태는 하나의 스토어 안에 하나의 객체 트리 구조로 저장된다는 의미인데. 디버깅이 쉽고, 여러 곳에서 상태를 관리하지 않아도 된다는 것을 의미한다. State is read-only애플리케이션의 상태는 항상 읽기 전용이고, 상태를 객체를 직접 조작함으로써 변경할 수 없다는 것을 의미한다. 상태를 변화시키기 위해서는 어떤 일이 발생할지 기술된 Action을 전달하는 것이 유일한 방법이 되어야 한다는 것이다. Changes are made with pure functionsAction을 통해 생기는 변화는 순수 함수를 통해서 이루어져야 한다는 의미이다. 이때 변화를 만드는 함수를 리듀서라고 하는데 이 리듀서는 이전 state를 받고 이전 state와는 독립적인 새로운 state를 반환해야 한다. 이 세 가지가 리덕스의 원칙이고, 이 원칙 마지막에 리덕스를 모두 알게 된 것이라고 말한다(?). 사용하다 보면 대충 알게 되지만 엄밀히 틀린 말은 아니라고 생각 되었다. 우리는 상태(state)가 있고, 이를 특정 action을 dispatcher에게 전달 함으로써 다음 상태로 변경시키는 과정을 리듀서로 적용해서 실제로 스토어를 변화시키는 것을 리덕스가 하는 전체 과정이라고 볼 수 있다. 풀어서 쓰다 보니 오히려 비문이 되긴 했지만, 간단한 원리. 123451.[STATE]2.dispatch(action) -&gt; reducer(action, STATE) =&gt; NEWSTATE -&gt; STATE = NEWSTATE ActionAction은 애플리케이션에서 스토어로 보내는 데이터 묶음이다. 위 도식에서 볼 수 있듯, dispatch({type: GET_POST}) 처럼 dispatch를 통해 전달할 수 있다. 타입 말고 다른 데이터 들도 넘길 수 있다. 타입은 반드시 필요하고, 나머지는 마음대로지만, 넘겨야 하는 데이터가 적을 수록 좋다고 한다. Action CreatorAction Creator는 이름 그대로 액션을 만들어내는 주체이다. 만들어내는 행위는 당연 함수가 하겠지. Action Creator의 존재는 사실 함수형 프로그래밍의 원칙을 위해 만들어진 개념이 아닌가 싶었다. 대충 예시는 다음과 같다. 12345678function getPost(id: number) { return { type: GET_POST, id };}dispatch(getPost(id)); Reducer리듀서는 액션에 따라서 스토어의 상태를 변화시키는 함수이다. 이전 상태와 action을 받아서 새로운 state를 내보내기만 하면 역할은 완료이다. 여러 action들이 있기 때문에 보통 리듀서는 switch 문으로 작성되게 된다. Reducer function이 부분은 사실 분리 하는 사람도 있고 아닌 사람도 있겠지만, 함수의 Depth가 깊어지는 것을 막고 싶기 때문에 추가한다. 쉽게 말해서 리듀서가 어떻게 동작할지를 따로 분리시키는 것이다. 12345678910111213141516...export default function reducer(state = initalState, action = {}) { switch (action.type) { case CREATE_POST: // 이부분에 해당하는 걸 함수로 나눠준다. applyCreatePost(state, action) break; ... default: return state; }}...const applyCreatePost = (state, action) =&gt; { ...} Store스토어에서는 애플리케이션의 상태를 저장하고, getState()를 통해서 상태에 접근하게 하고, dispatch(aciton)을 통해 상태를 수정할 수 있게 하고, subscribe(listener)를 통해 리스너를 등록한다. createStore()를 통해서 스토어를 만들 수 있다. createStore(app, initialState) 같은 형태로 사용된다. react-redux리액트의 경우에 스토어를 모든 컴포넌트에서 불러올 수 있도록 하기 위해서 Provider를 제공해주는 react-redux 패키지가 존재한다. 아래와 같이 hoc 패턴으로 스토어를 제공한다. 12345678910...import {Provider} from 'react-redux';const App: React.FC = () =&gt; { return ( &lt;Provider store={store}&gt; &lt;PostScreen /&gt; &lt;/Provider&gt; );}; 그리고 컴포넌트의 props에 store와 dispatch 함수를 넣기 위해서 connect 함수를 제공한다. Redux ducks 패턴리덕스 앱을 구축하는 과정에서, 리덕스의 여러가지 부분을 파일로 분할하게 되면 여러 파일을 수정해야 하는 경우가 생기기 때문에 그러한 경우를 방지하기 위해 단순하게 하나의 파일에 reducer가 만들어지도록 하는 패턴이다. 규칙은 다음과 같다. 하나의 모듈은 항상 reducer라는 이름의 함수를 export default 해야 한다. 항상 모듈의 actionCreator를 함수 형태로 export 해야 한다. 항상 npm-module-or-app/reducer/ACTION_TYPE 형태의 actionType을 가져와야 한다. 어쩌면 actionType을 UPPER_SNAKE_CASE로 export 할 수 있다. 만약 외부 reducer가 해당 action이 발생하는지를 기다리거나, 재사용할 수 있는 라이브러리로 퍼블리싱 할 경우에 간단하게 비동기를 진행하기 전까지의 원칙과 사용 방법 등을 간단하게 정리했다. 그러면 리덕스를 붙이기 전 리액트 앱을 만들고, 리덕스를 붙이는 방식으로 바꾼 다음 비동기를 작성해보고 어떤 문제가 있는지 확인하고, 이 문제를 해결하기 위해서 가장 유명하다는 react-thunk 또는 react-saga를 도입해보자 리덕스를 붙이기 전여기서 당시 코드를 확인할 수 있다. screens/PostScreen을 Container, Presenter 구조로 나눠서 api로 데이터를 받아오는 로직을 PostScreen.tsx에 담고, CSS와 관련된 컴포넌트들을 Presenter.tsx에 담도록 했다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import React from &quot;react&quot;;import styled from &quot;styled-components&quot;;interface IProps { postList: Array&lt;{ title: string; body: string; id: number }&gt;;}const Presenter: React.FC&lt;IProps&gt; = props =&gt; { const { postList } = props; return ( &lt;Wrapper&gt; &lt;Container&gt; {postList?.length &amp;&amp; postList.map((post, index) =&gt; ( &lt;PostDiv key={index}&gt; &lt;div&gt;{post.title}&lt;/div&gt; &lt;div&gt;{post.body}&lt;/div&gt; &lt;/PostDiv&gt; ))} &lt;/Container&gt; &lt;/Wrapper&gt; );};export default Presenter;const Wrapper = styled.div` width: 100%; display: flex; justify-content: center;`;const Container = styled.div` width: 70%; display: flex; flex-direction: column; justify-content: center;`;const PostDiv = styled.div` width: 90%; height: 10rem; border: 1px solid black; border-radius: 5px; margin: 1rem 0; padding: 0.25rem;`; 1234567891011121314151617181920212223// src/screen/PostScreen.tsximport React, { useState, useEffect } from &quot;react&quot;;import Presenter from &quot;./Presenter&quot;;import Axios from &quot;axios&quot;;import { ENDPOINTS } from &quot;../../../constants&quot;;const PostScreen: React.FC = props =&gt; { const [postList, setPostList] = useState([]); const getPost = async () =&gt; { const { data: postList } = await Axios.get(ENDPOINTS.GET_POSTS, { baseURL: ENDPOINTS.BASE_URL }); setPostList(postList); }; useEffect(() =&gt; { getPost(); }); return &lt;Presenter postList={postList} /&gt;;};export default PostScreen; 리덕스 구성하기이 링크에서 이 챕터까지 진행한 코드를 확인할 수 있다. 위와 같은 상황에서 글 쓰기와 불러오기를 useEffect에 들어가는 getPost와 같은 형태가 아니라 모듀 리덕스를 통해 동작하도록 바꿔보자.src/redux/configureStore.ts를 만들고 아래 폴더에 src/redux/modules/를 만들어서 아래 리덕스 모듈들을 ducks 형태로 만들어서 전체적인 리덕스를 구성해보려고 한다.src/redux/modules/post.ts를 만들고 ducks 형태로 리덕스 모듈을 구성하자. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// src/redux/modules.post.ts// importsimport axios from &quot;axios&quot;;import { ENDPOINTS } from &quot;../../../constants&quot;;// Action Typesexport const GET_POST = &quot;GET_POST&quot;;export const CREATE_POST = &quot;CREATE_POST&quot;;// Action Creatorsexport function getPost() { return { type: GET_POST };}export function createPost(title: string, content: string) { return { type: CREATE_POST, title, content };}// Initial Stateinterface IPost { title: string; body: string;}export const initialState: { postList: Array&lt;IPost&gt;; newPost?: IPost } = { postList: []};// Reducerfunction reducer(state = initialState, action: any) { switch (action.type) { case GET_POST: return applyGetPost(); case CREATE_POST: const { title, body } = action as IPost &amp; { type: string }; return applyCreatePost(state, title, body); default: return state; }}// Reducer Functionsconst applyGetPost = async () =&gt; { const postList = await axios.get(ENDPOINTS.GET_POSTS, { baseURL: ENDPOINTS.BASE_URL }); return { postList };};const applyCreatePost = async (state: any, title: string, body: string) =&gt; { const newPost = await axios.post(ENDPOINTS.GET_POSTS, { baseURL: ENDPOINTS.BASE_URL, body: { title, body } }); return { ...state, newPost };};// Default Reducer exportexport default reducer; useEffect에서 했던 것을 비동기 망한 예시를 위해 모두 reducer로 작성했다. 작성하면서 비동기로 하면 왜 안될 것 같은지를 깨닫게 되었지만 일단 진행해보자. 아래는 configureStore파일이고, 가장 상위 App에 store를 보내는 것으로 수정했다. 123456// src/redux/configureStore.tsimport { combineReducers } from &quot;redux&quot;;import post from &quot;./modules/post&quot;;export default combineReducers({ post }); 123456789101112// src/App.tsx...const store = createStore(rootReducer, applyMiddleware(logger)); // 두 번째 인수는 초기 상태를 지정할 수 있음const App: React.FC = () =&gt; { return ( &lt;Provider store={store}&gt; &lt;PostScreen /&gt; &lt;/Provider&gt; );}; 그러고 나서 실제 스크린에서 사용할 수 있게 props로 dispatch 하는 함수와 store의 state를 넘겨주었다. 12345678910111213141516171819import PostScreen from &quot;./PostScreen&quot;;import { connect } from &quot;react-redux&quot;;import { getPost } from &quot;../../redux/modules/post&quot;;const mapStateToProps = (state: any, ownProps: any) =&gt; { return { ...ownProps, ...state };};const mapDispatchToProps = (dispatch: any, ownProps: any) =&gt; { return { ...ownProps, getPost: async () =&gt; dispatch(getPost()) };};export default connect(mapStateToProps, mapDispatchToProps)(PostScreen); 컨테이너에 해당하는 PostScreen.tsx는 아래와 같이 수정했다. useEffect를 없애고 클릭하면 스토어가 업데이트 되도록 변경한 모습이다. 1234567891011121314151617181920212223242526import React from &quot;react&quot;;import Presenter from &quot;./Presenter&quot;;interface IProps { getPost: Function; postList: Array&lt;any&gt;;}const PostScreen: React.FC = props =&gt; { const { postList, getPost } = props as IProps; // const getPost = async () =&gt; { // const { data: postList } = await Axios.get(ENDPOINTS.GET_POSTS, { // baseURL: ENDPOINTS.BASE_URL // }); // setPostList(postList); // }; const onClick = async () =&gt; { await getPost(); }; return &lt;Presenter onClick={onClick} postList={postList} /&gt;;};export default PostScreen; 결과적으로 가져오기 버튼을 클릭하면 스토어의 모습이 다음과 같이 변한다. 스토어의 객체가 Promise가 된 것이다. 이것을 그냥 해결해 보려고 recursive 하게 async await으로 변경도 해봤지만 바뀌지 않았다. 아무튼 이러한 문제가 있다는 것을 알게 된 시도였다. Redux thunk이 링크에서 thunk로 문제를 해결한 다음 코드를 확인할 수 있다. 일단 미들웨어를 설치해보자 1yarn add redux-thunk 그러면 이제 saga, thunk를 적용해보도록 하자. 먼저 redux 공식 문서에서 소개하고 있는 redux-thunk를 먼저 도입해보자. Action Creator 부분도 fetching 중인지 나타내는 상태까지 관리하도록 변경했다 (공식 문서에 있는 예시에 나온 그대로) 변경된 내용은 아래와 같다. 먼저 configureStore.ts에 store를 만들어서 export default 변경해주었다. 하는 방식으로 변경했다. 미들웨어 설정도 붙이게 되면 App.tsx에다가 코드를 쓰는 것보다는 여기서 코드를 완성해서 내보내는 게 더 맞는 것 같다고 생각했다. 123456789import { createStore, applyMiddleware, combineReducers } from &quot;redux&quot;;import logger from &quot;redux-logger&quot;;import thunk from &quot;redux-thunk&quot;;import post from &quot;./modules/post&quot;;const rootReducer = combineReducers({ post });export default createStore(rootReducer, applyMiddleware(thunk, logger)); // 두 번째 인수는 초기 상태를 지정할 수 있음 그리고 나서 reducer를 많이 변경 했는데, FETCH_POST_START와 FETCH_POST_END를 만들었고, getPost는 그냥 네트워킹 로직을 처리하는 함수가 되었다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849...// Action Creatorsexport function getPost() { return async (dispatch: Function) =&gt; { dispatch(startPost()); const { data: postList } = await axios.get(ENDPOINTS.GET_POSTS, { baseURL: ENDPOINTS.BASE_URL }); return dispatch(endPost(postList)); };}export function startPost() { return { type: FETCH_POST_START };}export function endPost(postList: Array&lt;any&gt;) { return { type: FETCH_POST_END, payload: { postList } };}...function reducer(state = initialState, action: any) { switch (action.type) { case FETCH_POST_END: const { postList } = action.payload; return applyFetchPostEnd(state, postList); case FETCH_POST_START: return applyFetchPostStart(state); default: return state; }}// Reducer Functionsconst applyFetchPostStart = (state: any) =&gt; { return { ...state, isFetching: true };};const applyFetchPostEnd = (state: any, postList: Array&lt;any&gt;) =&gt; { return { ...state, postList, isFetching: false };}; 작성하는 과정에서 thunk가 어떻게 구성되어 있는지 확인하지는 않았는데 시행 착오를 겪고 나니 대충 감이 왔다. 의사코드로 나타내면 아마도 정말 간단하게도 이런 구조가 아닐까 싶다. 1234567const thunk = (dispatch, getState, action) =&gt; { if (typeof action === &quot;function&quot;) { return action(dispatch, getState); } return action;}; 물론 미들웨어가 다음 단계로 넘어가는 구조가 어떻게 되어 있는지는 확인하지 못 해서 정교한 구조는 아니겠지만, actionCreator로 dispatch(actionCreator())가 되었을 때를 판단하게 되는 것 같다. 궁금해서 까봤는데 코드가 아주 비슷하긴 하다. 실제로 14줄 짜리 코드라 더 놀라운데 궁금한 사람은 이 링크에서 확인해보자. Redux-saga여기서 마지막으로 해결한 상태의 코드를 확인해볼 수 있다. 일단 설치먼저 해두고 문서를 또 읽어보기로 했다. 1yarn add redux-saga 사가 문서를 읽어보면 사이드 이펙트를 쉽게 관리해준다고 표현되어있다. 특히 여러 글에서 실패하는 경우 마치 트랜젝션이 가능한 것처럼 표현하는 글을 한 두 번 본 기억이 있다. 사가는 리덕스의 함수형 프로그래밍적인 특성을 그대로 유지하고 (순수 함수, 상태 변경을 action으로만 등), 비동기적인 로직을 사가에서 맡아서 처리하도록 한다. 여기서 맡아서 처리하도록 한다를 Generator를 사용하도록 만들어져있다. 기본적으로 Generator는 실행 맥락을 함수 중간 중간 떠넘길 수 있도록 구성할 수 있기 때문에 이러한 투트랙 느낌의 비동기 처리가 가능한 것 같다. 아주 Advanced한 경우는 아직 확인해보지 못 했지만, 위와 같은 간단한 예시를 구성하는 것 자체는 크게 어렵지 않았다. 우선 thunk 미들웨어를 saga 미들웨어로 교체했다. (기존에 src/redux/modules/post 아래를 thunkReducer.ts, sagaReducer.ts로 나눴다.) 1234567891011121314151617// src/redux/configureStore.tsimport { createStore, applyMiddleware, combineReducers } from &quot;redux&quot;;import logger from &quot;redux-logger&quot;;import createSagaMiddleware from &quot;redux-saga&quot;;import post, { rootSaga } from &quot;./modules/post/sagaReducer&quot;;const sagaMiddleware = createSagaMiddleware();const rootReducer = combineReducers({ post });export default createStore( rootReducer, applyMiddleware(sagaMiddleware, logger)); // 두 번째 인수는 초기 상태를 지정할 수 있음sagaMiddleware.run(rootSaga); sagaReducer.ts는 다음과 같이 변경되었다. getPost Action Creator를 붙여서 타입과 페이로드만 넘기는 형태로 바뀌었다. (기존 thunk 에서는 api를 동작시키는 함수를 호출해야 했음, 그리고 에러 처리를 위해 FAIL이 추가됨) 그러면서 saga에게는 어떤 Action Type인지에 따라서 비동기 처리를 할 것인지 말 것인지를 listen 하고 있는 형태인 것 같다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import { call, put, takeEvery, all } from &quot;redux-saga/effects&quot;;import axios from &quot;axios&quot;;import { ENDPOINTS } from &quot;../../../constants&quot;;// Action Typesexport const FETCH_POST_FAIL = &quot;FETCH_POST_FAIL&quot;;export const FETCH_POST_START = &quot;FETCH_POST_START&quot;;export const FETCH_POST_END = &quot;FETCH_POST_END&quot;;export const FETCHING_POST = &quot;FETCHING_POST&quot;;// Sagaconst fetchPost = () =&gt; axios.get(ENDPOINTS.GET_POSTS, { baseURL: ENDPOINTS.BASE_URL });function* fetchPostList() { try { const { data: postList } = yield call(fetchPost); yield put({ type: FETCH_POST_END, payload: { postList } }); } catch (e) { yield put({ type: FETCH_POST_FAIL, payload: e }); }}function* postSaga() { yield takeEvery(FETCHING_POST, fetchPostList);}export function* rootSaga() { yield all([postSaga()]);}// Action Creatorsexport function getPost() { return { type: FETCHING_POST };}export function startPost() { return { type: FETCH_POST_START };}export function endPost(postList: Array&lt;any&gt;) { return { type: FETCH_POST_END, payload: { postList } };}// Initial Stateinterface IPost { title: string; body: string;}export const initialState: { postList: Array&lt;IPost&gt;; newPost?: IPost } = { postList: []};// Reducerfunction reducer(state = initialState, action: any) { switch (action.type) { case FETCH_POST_END: const { postList } = action.payload; return applyFetchPostEnd(state, postList); case FETCH_POST_START: return applyFetchPostStart(state); case FETCH_POST_FAIL: return applyFetchPostFail(state, action.payload); default: return state; }}// Reducer Functionsconst applyFetchPostStart = (state: any) =&gt; { return { ...state, isFetching: true };};const applyFetchPostEnd = (state: any, postList: Array&lt;any&gt;) =&gt; { return { ...state, postList, isFetching: false };};const applyFetchPostFail = (state: any, payload: any) =&gt; { const error = { title: payload.message, body: JSON.stringify(payload.stack) }; state.postList.push(error); return { ...state };};// Default Reducer exportexport default reducer; 현재 프로젝트는 src/screens/PostScreen/index.tsx에서 import { getPost } from &quot;../../redux/modules/post/thunkReducer&quot;; 이 부분만 sagaReducer로 바꿔주면 사가로 동작하고 thunkReducer로 두면 thunk로 동작하게 구성되어져 있다. 최종적인 프로젝트는 이 링크에서 확인할 수 있다. 위 링크에 있는 코드가 최종 버전은 맞지만, README.md는 다듬기 전의 글 모습이다. 글과 README.md의 디테일한 내용이 아마 차이가 있을 수 있다. 후기결과적으로 프로덕트에는 saga를 공부해서 붙이기로 결정했다. 간단하게만 경험했지만, saga는 thunk의 +a를 많이 제공하고 있다. 단순히 thunk가 하는 일을 takeEvery();와 call(API) 정도로 모두 커버 가능 하면서도, 강력한 기능에 대한 예시들을 공식 문서나 advanced 자료에서 쉽게 찾아볼 수 있다는 점, 디테일한 Helper function 등 선택하는 것에 큰 매력을 주기도 했고, actionCreator를 단순하게 유지할 수 있다는 것과 동시에 재사용이 thunk보다는 쉽다는 점 (그냥 리슨하고 있는 제네레이터 함수일 뿐이기 때문에, 종속적이지 않게 짤 수 있다고 생각됨) 등… 두서 없이 생각해내다 보니까 계속 나오는데 강력한 기능을 경험한 다음 saga에 대한 글을 한 번 더 쓸 수 있으면 좋겠다. Reference https://lunit.gitbook.io/redux-in-korean/advanced/asyncactions https://mskims.github.io/redux-saga-in-korean/introduction/BeginnerTutorial.html https://github.com/reactkr/learn-react-in-korean/blob/master/translated/deal-with-async-process-by-redux-saga.md https://redux-advanced.vlpt.us/2/05.html https://ko.javascript.info/generators https://ko.javascript.info/async-iterators-generators","link":"/posts/etc/redux-async-demo/"},{"title":"RxJS 빠르게 배우기 03 - Observer &amp; Subscription &amp; Operator","text":"지난 글에서는 Observable과 구독 관련 동작 방식에 대해서 글을 작성했다. 이번 글에서는 Observer, Subscription, Operator에 대해서 간단하게 알아보자. Observer옵저버는 Observable에 의해 전달된 값을 소비하는 주체이다. 일종의 콜백 묶음으로 볼 수 있는데, Observable이 전달해주는 next, error, complete을 어떻게 처리하는지에 대한 내용이 있는 콜백들이다. 1234567const observer = { next: x =&gt; console.log(`Next value: ${x}`), error: err =&gt; console.error(`Error: ${err}`), complete: () =&gt; console.log(`Complete`)}observable.subscribe(observer); 모든 경우를 처리해줄 필요 없이, 일부분만 설정해줘도 정상 동작한다. 다만 알람이 오게 될 때 해당 부분을 처리해주는 콜백이 없으면 무시하게 된다. 12345observable.subscribe( x =&gt; console.log('Observer got a next value: ' + x), err =&gt; console.error('Observer got an error: ' + err), () =&gt; console.log('Observer got a complete notification')); 꼭 객체 형태로 전달해줄 필요도 없다. 위 예시처럼, next, error, complete 순서대로 인자값을 전달받게 된다. SubscriptionSubscription(구독)은 처리 가능한 자원을 나타내는 객체라고 설명하고 있다. 쉽게 보면 Observable을 실행하는 것이라고 볼 수 있다. Subscription은 unsubscribe라고 하는 중요한 메서드가 있는데, subscription에 묶여있는 자원을 해제하는 것이다. 설명을 줄줄 하면 더 헷갈리는 것 같은데, 쉽게 말해서 Observable을 구독하면 Subscription 객체를 리턴한다. 그 객체는 구독을 취소할 수 있는 unsubscribe 메서드를 가지고 있는 객체라고 볼 수 있다. 1234567import { interval } from &quot;rxjs&quot;;const observable = interval(1000);const subscription = observable.subscribe(console.log);subscription.unsubscribe(); // 구독을 바로 취소한다. Subscription은 여러 개를 함께 둘 수 있다. 그리고 한 번에 복수 개의 Subscription을 구독 취소할 수 있게 된다. 한 개로 묶는 방법은 add 메서드를 사용하면 된다. 반대로 함께 묶인 것을 제거하려면, remove(otherSubscription) 형태로 메서드를 사용하면 된다. 123456789101112131415161718192021import { interval } from &quot;rxjs&quot;;const observable1 = interval(400);const observable2 = interval(300);const subscription = observable1.subscribe((v) =&gt; console.log(`first: ${v}`));const childSubscription = observable2.subscribe((v) =&gt; console.log(`second: ${v}`));subscription.add(childSubscription); // Subscription을 하나로 묶어서 관리setTimeout(() =&gt; { subscription.unsubscribe(); // 두 개 모두 한 번에 구독 취소 처리가 된다.}, 1200);// second: 0// first: 0// second: 1// first: 1// second: 2 OperatorRxJS는 보통 Operator(오퍼레이터)가 가장 유용하다. 복잡한 비동기 코드를 쉽고 명확한 방법으로 합성하게 해준다. 오퍼레이터는 함수이다. 두 가지 종류가 있는데, 하나는 Pipeable Operator이고 하나는 Creation Operator이다. Pipeable OperatorPipeable은 operatorInstance.pipe(operator())와 같은 문법을 사용하는 Observables을 통과할 수 있는 연산자 종류이다. 대표적으로는 filter(...), mergeMap(...) 등이 있다. 호출 되었을 때, 기존에 존재하는 Observable instance를 바꾸지는 않고, 새로운 Observable을 만들어낸다. 즉, 순수 함수의 특성을 가지고 있다. Creation OperatorCreation Operator는 새로운 Observable을 만들기 위해 단일한 함수 호출이 가능한 함수이다. 예를 들자면, of(1, 2, 3)를 사용하면, 1, 2, 3 값을 내보내는 observable을 만들 수 있다. 예를 들자면 interval 함수는 숫자를 매개 변수로 받아서 Observable을 내보내는 함수이다. 123456import { interval } from &quot;rxjs&quot;;const observable = interval(1000 /* number of milliseconds */);observable.subscribe(console.log);// 0부터 1초마다 1씩 큰 수가 로그로 찍힌다. 위처럼 간단하게 특정하게 정해진 방식대로 독자적으로 Observable을 생성해주는 함수들을 Creation Operator라고 한다. Operator를 사용한 예를 간단히 들어보자 map 오퍼레이터는 JS 배열의 메서드에서의 map 메서드와 유사한 역할을 한다. 12345678910111213141516import { of } from &quot;rxjs&quot;;import { map } from &quot;rxjs/operators&quot;;map&lt;number, number&gt;((x) =&gt; x * x)(of(1, 2, 3)).subscribe(console.log);// 1// 4// 9of(1, 2, 3) .pipe(map((x) =&gt; x * x)) .subscribe(console.log);// 1// 4// 9 first 오퍼레이터는 첫 번째 값을 새로운 Observable로 만들 수 있다. 1234567import { first } from &quot;rxjs&quot;;first()(of(1, 2, 3)).subscribe(console.log);// 1of(1, 2, 3).pipe(first()).subscribe(console.log);// 1 위 예시에서 pipe 메서드를 사용한 것을 공식 문서에서는 Piping(파이핑) 이라고 말 한다. 실제로 파이핑 하지 않고 작성한 코드는 쉽게읽히지 않게 되는 경우가 있다. 예를 들자면 op4()(op3()(op2()(op1()(obs)))) 이러한 느낌의 코드가 나올 수 있다. 이러한 이유로 옵저버블에서는 pipe 메서드를 제공하고 있다. 방금 예시의 읽기 힘든 코드는 아래와 같이 작성될 수 있게 된다. 123456obs.pipe( op1(), op2(), op3(), op3(),) 공식 문서에 정말 많은 Operator를 설명해두었지만, 이번 글에서는 자세하게 다루지 않고, 디테일한 내용들은 다른 글에서 분리하여 정리해보려고 한다. 이 링크에서 오퍼레이터의 종류와 여러 오퍼레이터를 나열해두었다. Reference RxJS 프로그래밍 (책) https://rxjs-dev.firebaseapp.com/guide/subscription","link":"/posts/etc/rxjs-quicklearn-03/"},{"title":"함수형 프로그래밍에 대한 얕은 지식","text":"최근에 타입스크립트와 함수형 프로그래밍을 주제로된 책의 베타 리더로 활동하면서 관련된 책 한 권을 읽었다. 책은 그야말로 JS, TS, Functional Programming에 대한 내용들이었는데 그 중에 함수형 프로그래밍은 이번 기회에 처음 접하게 되어서, 책을 통해 새로 배우는 함수형 프로그래밍에 대한 내용들은 간단하게 정리를 해두려고 한다. 배경지식: 프로그래밍 패러다임프로그래밍 패러다임은 프로그래머에게 프로그래밍의 관점을 갖게 해주고, 결정하는 역할을 한다. 선언형 프로그래밍과 명령형 프로그래밍선언형 프로그래밍은 프로그램이 어떤 방법으로 해야 하는지를 나타내기보다 무엇인지를 설명하는 경우 “선언형”이라고 한다. 가장 대표적인 예시로는 웹페이지의 레이아웃이 있다. 제목, 본문, 메뉴 등 “무엇”을 나타내려고 하는지를 설명하는 것이기 때문이다. 명령형 프로그래밍은 반대로 “어떻게”를 명시하는 프로그램이다. 프로그래머가 알고리즘을 명시하고 목표를 명시하지 않게 프로그래밍을 한다면 이는 명령형 프로그래밍을 한 것이라고 볼 수 있다. 절차 지향, 객체 지향, 함수형 프로그래밍 절차 지향 프로그래밍: 순서대로 명령을 받아 문제를 해결하는 방식이다. 가장 직관적이고 가장 오래된 방식이다. 객체 지향 프로그래밍: 프로그램을 작게 나눠 각각의 단순한 기능을 모아서 문제를 해결하는 방식이다. 프로그램의 기능이 다양할 수록 코드는 복잡해진다. 절차 지향 프로그래밍에서는 이를 해결하기 위한 적절한 방법이 없었고 이러한 문제를 해결하기 위해 객체 지향 프로그래밍이라는 방법론이 제안 되었다. 단순한 문제를 해결하는 여러 기능으로 나누어 문제를 해결하는 것이다. 함수형 프로그래밍: 프로그램을 하나의 큰 함수로 보고, 그것을 작은 함수들의 합성 함수로 구현해 문제를 해결하는 방식이다. 객체 지향 프로그래밍보다 더 문제를 세분화 하는 것이라고 볼 수 있는데, 함수형 프로그래밍과 용어들함수형 프로그래밍은 순수 함수와 선언형 프로그래밍 토대 위에 함수 조합과 모나드 조합으로 코드를 설계하고 구현하는 기법이다. 용어들에 대해서 간단하게 정리해보자. 일급 객체, 일급 함수일급 객체는 아래 조건을 만족하는 객체를 뜻한다. 변수나 데이터 구조안에 담을 수 있다. 인자값으로 전달 할 수 있다. 반환값으로 사용할 수 있다. 할당에 사용된 이름과 관계 없이 고유한 구별이 가능하다. 동적으로 프로퍼티 할당이 가능하다. 자바스크립트의 객체는 일급 객체를 만족하고, 자바스크립트의 함수는 객체이므로 자바스크립트의 함수는 일급 함수이다. 어떤 경우엔 일급 함수를 위한 추가적인 조건도 요구하는데 그 추가적인 조건은 아래와 같다. 런타임 생성이 가능하다. 익명 생성이 가능하다. 마찬가지로 자바스크립트 함수에게 모두 해당되는 내용이므로 문제 없이 자바스크립트는 일급 함수 특성을 갖는다고 말할 수 있다. 고차 함수와 부분 함수어떤 함수가 또 다른 함수를 반환할 때 그 함수는 고차 함수 (high order function)이라고 한다. 1234const add = (x:number): (number =&gt; number) =&gt; (y: number): number =&gt; x + y;add(1)(2) // 3 위에서 구현된 add 함수는 2차 함수에 해당한다. 또한 호출 연산자를 2번 연속으로 사용하고 있는데 이를 함수형 프로그래밍에서는 커리(curry)라고 한다. 그런데 만약 add의 차수인 2보다 호출 연산자를 조금 사용하면, 즉 1번만 사용한 경우를 보자. 12const add5 = add(5);add5(10); // 15 위 예시에서 사용된 add5 함수는 부분 함수 - partial function (부분 적용 함수 - partially applied function)이라고 한다. 순수 함수함수형 프로그래밍에서 함수는 순수 함수(pure function) 조건을 만족할 수 있어야 한다. 순수 함수는 부수 효과(side effect)가 없는 함수를 뜻한다. 부수 효과는 함수가 가진 목적 외 다른 효과를 갖는 것을 의미한다. 부수 효과를 가진 함수는 불순 함수(impure function)이라고 한다. 순수 함수는 다음 조건을 만족해야 한다. 함수 내부에서 전역 변수, 정적 변수를 사용하지 않는다. 함수가 예외를 발생시키지 않는다. 비동기 방식으로 동작하는 코드가 없다. 함수 body에서 만들어진 결과를 즉시 반환한다. 함수 body에 매개 변수를 변경하지 않는다. 함수 body에 입출력이 없어야 한다. 함수 조합함수 조합 (function composition)은 작은 기능을 구현한 함수를 여러 번 조합해 더 의미 있는 함수를 만들어 내는 프로그램 설계 기법이다. 함수 조합을 할 수 있는 언어는 compose 또는 pipe라는 이름의 함수를 제공하거나 만들 수 있다. compose 함수compose는 합성 함수를 의미한다. compose(f, g, h)는 수학적으로 f ∘ g ∘ h를 나타낸 것이다. 합성 함수의 기호를 떠올려보면 아래와 같이 작동하길 바라는 것이 수학적으로 잘 표현한 경우이다. 12345678const f = &lt;T&gt;(val: T): string =&gt; `f(${val})`;const g = &lt;T&gt;(val: T): string =&gt; `g(${val})`;const h = &lt;T&gt;(val: T): string =&gt; `h(${val})`;const composedFunc = compose(f, g, h);composedFunc(&quot;some value&quot;);// f(g(h(some value))) 타입스크립트에서 위와 같은 compose를 만들어보면 아래와 같을 것이다. 함수가 적용되는 순서에 유의해서 reverse 하는 과정을 넣어 줘야 한다. 123456const compose = &lt;T, R&gt;(...functionList: readonly Function[]): Function =&gt; (x: T): R =&gt; { const deepCopiedList = [...functionList]; return deepCopiedList.reverse().reduce((val, func) =&gt; func(val), x); }; pipe 함수compose와 pipe의 차이는 함수 적용되는 순서이다. compose는 의미상 먼저 인자 값으로 들어간 함수를 더 나중에 적용하게 된다. 하지만 pipe는 인자 값으로 들어간 순서대로 함수를 실행하길 원한다. 이러한 점을 고려해보면 위 compose 함수에서 reverse만 제외해주면 된다. 1234567const pipe = &lt;T, R&gt;(...functionList: readonly Function[]): Function =&gt; (x: T): R =&gt; functionList.reduce((val, func) =&gt; func(val), x);const pipedFunc = pipe(f, g, h);pipedFunc(&quot;some value&quot;); // h(g(f(some value))) 포인트가 없는 함수포인트가 없는 함수(pointless function)라는 것은 함수 조합을 고려해 설계한 함수를 뜻한다. 예를 들어서 위 pipe 함수의 인자로 들어갈 함수들의 배열 (functionList)을 고려한 인자로 사용되는 함수를 아래와 같이 만들 수 있다. 12345678const map = (f) =&gt; (arr) =&gt; arr.map(f);const squareMap = map((val) =&gt; val * val);//const squareMap2 = arr =&gt; map(val =&gt; val * val)(arr);const squarePipedFunc = pipe(squareMap, squareMap);squarePipedFunc([5, 6]); // [(5 * 5) * (5 * 5), (6 * 6) (6 * 6) ] 위 코드에서 squareMap는 포인트가 없는 함수에 해당하고 squareMap2는 포인트가 있는 함수인 것이다. 완전히 이해하기는 어렵지만, 느낌적으로는 함수를 완성 시키는 게 아니고, 인자값으로 쓸 수 있는 정도로만 정의하고 함수의 인자로 전달하는 느낌이다. arr.filter(val =&gt; Boolean(val))형태가 아니라, arr.filter(Boolean) 같은 코드 느낌이 난다. 모나드모나드는 카테고리 이론에서 사용되는 용어이다. 프로그래밍에서 모나드는 코드 설계 패턴으로서 몇 개의 인터페이스를 구현한 클래스이다. 모나드 클래스는 다음 4가지의 조건을 만족시켜야 한다. Functor: map이라는 인스턴스 메서드를 가지는 클래스 Apply: Functor 이면서 ap라는 인스턴스 메서드를 가지는 클래스 Applicative: Apply면서 of라는 클래스 메서드를 갖는 클래스 Chain: Applicative면서 chain이라는 메서드를 가지는 클래스 타입 클래스와 고차 타입12345678910class Monad&lt;T&gt; { constructor(public value: T) {} static of&lt;U&gt;(value: U): Monad&lt;U&gt; { return new Monad&lt;U&gt;(value); } map&lt;U&gt;(fn: (x: T) =&gt; U): Monad&lt;U&gt; { return new Monad&lt;U&gt;(fn(this.value)); }} 위와 같은 Monad&lt;T&gt; 클래스를 타입 클래스라고 한다. 타입 클래스는 함수를 만들 때 특별한 타입으로 제약할 필요가 없다. 예를 들어서 아래 함수는 인자 값으로 들어오는 함수의 인자 값(b)가 map이라는 메소드를 가지고 있어야 한다. 따라서 타입스크립트로 작성할 때 map 함수를 가지고 있는 매개 변수임을 정의 해줘야 오류로부터 안전해진다. 1234const callMap = &lt;T, U&gt;(fn: (T) =&gt; U) =&gt; &lt;T extends { map(fn) }&gt;(b: T) =&gt; b.map(fn); 위와 같은 작업을 타입 클래스로 대체 하면 아래와 같다. 1const callMonad = (fn) =&gt; (b) =&gt; Monad.of(b).map(fn).value; 위와 같이 타입에 따른 오류를 없애고, 코드 재사용성이 뛰어난 함수를 만들 수 있다. 12callMonad((a: number) =&gt; a + 1)(1); // 2callMonad((a: number[]) =&gt; a.map((val) =&gt; val + 1))([1, 2, 3]); // [2, 3, 4] 모나드 클래스에서 타입 T를 잠시 Monad&lt;T&gt; 타입으로 바꾼 다음 T 타입이 필요해질 때 그 값을 주는 것을 확인할 수 있는데 이 때 Monad&lt;T&gt;를 고차 타입이라고 한다. 이 고차 타입에 대한 아이디어는 카테고리 이론에서 얻었다. 모나드 룰어떤 클래스의 이름이 M, 인스턴스를 m이라고 했을 때, 모나드는 Applicative와 Chain 기능을 가지고 있고, 아래 두 법칙을 만족하게 구현한 클래스라고 볼 수 있다. 왼쪽 법칙(left identity): M.of(a).chain(f) == f(a) 오른쪽 법칙(right identity): m.chain(M.of) == m 마치며함수형 프로그래밍에 대해서 처음 접하는 개념이 많아서 깔끔하게 정리가 안됐다. 아마 점차적으로 공부 하면서 더 나은 정리를 해볼 수 있지 않을까 싶다. Reference https://ko.wikipedia.org/wiki/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D_%ED%8C%A8%EB%9F%AC%EB%8B%A4%EC%9E%84 https://velog.io/@kyusung/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EC%9A%94%EC%95%BD https://freshrimpsushi.tistory.com/1361 https://ko.wikipedia.org/wiki/%EC%84%A0%EC%96%B8%ED%98%95_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D https://ko.wikipedia.org/wiki/%EB%AA%85%EB%A0%B9%ED%98%95_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D https://bestalign.github.io/2015/10/18/first-class-object/","link":"/posts/etc/shallow-knowledge-of-funtional-programming/"},{"title":"대규모 시스템 디자인 Part 1 강의","text":"이번에 제가 시스템 설계와 관련된 강의를 만들었습니다. 대규모 시스템 디자인은 대규모 시스템을 만들며 공통적으로 만나게 되는 문제들을 해결하는 방법을 설명하고, 유명한 시스템을 공부하면서 실제로 어떻게 적용되고 있는지 배웁니다. 강의 바로 가기","link":"/posts/etc/system-design-part-1/"},{"title":"초보를 위한 테라폼","text":"서비스의 복잡도가 증가하면서 인프라를 어떻게 더 쉽게 관리할 수 있을까에 대한 고민은 지속해서 증가했다. 정적인 상태와 안전한 코드로서 인프라를 관리하고 인프라에서 해야 하는 작업을 확장성있는 방식으로 구성하기 위해 개발자들은 인프라를 코드로써 표현하려고 했다. 이를 IaC (Infra as Code)라고 한다. 현재는 Terraform이 주류를 잡고 있는 듯 하다. 이 글은 IaC에 대한 이야기부터 테라폼의 얕은 이야기를 다룰 예정이다. 이번 글은 테라폼이 무엇일지 대략적인 감을 잡기 위한 글이다. tfstate, 협업하는 방법, Best Practice같은 얘기는 이번 글에서 다루지 않는다. IaC (Infrastructure as Code)말 그대로 인프라를 코드로 관리하는 방법이다. 이런 구성의 목적은 무엇일까? 재사용성: 손으로 하던 작업을 코드로 변경하게 되면 대표적으로 바뀌는 점은 반복 가능하다는 점이다. 그리고 코드를 어떻게 짜는지에 따라 다르겠지만, 작은 단위마다 재사용이 가능하다. 예를 들어서 웹 서버를 클러스터 형태로 배포하고 앞에 로드 밸런서를 달아둔다고 해보자. 이 구성은 여러 서비스를 새로 배포할 때마다 몇 가지 설정을 제외하고 모두 같은 과정을 거치게 된다. 만약 코드가 있다면 이 과정을 반복하는 지루한 일을 할 필요 없다. 안정성: 코드를 재사용할 수 있다는 것은 안전한 흐름을 재사용하는 것과 같다. 안전하다는 것은 테스트를 통해 확인할 수 있고, 테라폼 역시 테스트 코드를 만들 수 있다. 어찌 됐든 안전한 작은 흐름을 모아 크고 안전한 흐름을 구성할 수 있게 해준다. 가시성: 가시성이라고 표현할만한지 사실 잘 모르겠지만 코드로 인프라를 구성하는 것은 인프라의 구성을 파악하기 훨씬 쉬워진다고 생각한다. 또한 Orphan이 된 인프라를 손쉽게 제거할 수도 있고, 사람이 보지 못해서 생기는 문제들을 줄여준다. 이외에도 여러 장점이 있겠지만 제일 중요한 포인트는 재사용성인 것 같다. 재사용이 가능하다는 것은 여러 스테이지로 현재 서비스 구성을 복사할 수도 있다는 것을 의미하고, 완전히 동일한 인프라 구성에서 테스트 스테이지를 구성할 수도 있다는 것을 뜻한다. IaC의 장점은 이렇게 쉽게 환경을 만들고 부술 수 있는 능력에 있다고 생각한다. IaC 구분IaC는 크게 다섯 가지로 구분한다. 애드훅 스크립트, 구성 관리 도구, 서버 템플릿 도구, 오케스트레이션 도구, 프로비전 도구로 나눌 수 있다. 애드훅 스크립트인프라 자동화의 가장 기본적인 방법이고 가장 쉽다. 서버에서 수행할 작업을 스크립트로 구성하고 직접 실행시키는 방법이다. Shell Script 또는 익숙한 코드로 필요한 내용을 작성하고 이를 동작시키는 방식이다. 소규모 인프라에서 일회성 작업이 필요한 경우 아주 적합한 방법이다. 하지만 복잡한 작업을 해야 한다면 결국 새로운 코드 베이스를 만들어 관리하는 형태가 되기 때문에 바람직하지 않다. 구성 관리 도구타겟 서버가 가지고 있어야 하는 소프트웨어를 관리하도록 설계되어있다. 애드훅 스크립트와 비슷한데 인프라 관련된 규칙을 가진 도구이기 때문에 무슨 작업을 하는지 비교적 쉽게 코드로 파악할 수 있다. 애드훅 스크립트와 비교했을 때 가장 큰 차이점은 이 종류에 해당하는 도구들의 대부분이 멱등성을 가지고 있다는 점이다. 쉘 스크립트로 인프라에 대한 멱등성 있는 코드를 작성하는 것은 귀찮은 일이고, 코드도 복잡해진다. 이러한 도구의 가장 대표적인 도구는 앤서블이다. 서버 템플릿 도구말 그대로 서버 구성을 스냅샷화 해서 사용하는 도구이다. 운영체제, 소프트웨어, 파일 및 기타 필요한 모든 상황에 대해 스냅샷을 만들 수 있는 도구이다. 이런 스냅샷은 보통 “이미지”라고 불린다. 서버 템플릿 도구로 만들어진 이미지를 구성 관리 도구를 사용해 서버에 다운받는 방식으로 사용할 수도 있다. 이러한 도구의 가장 대표적인 도구는 도커와 패커가 있다. 서버 템플릿은 Immutable Infrastructure를 구성하기 위한 핵심적인 요소이다. 한 번 만들어진 스냅샷은 변하지 않고, 새로운 버전을 만들고 싶으면 아예 새로운 스냅샷을 빌드해 배포해야 한다. Snow Flake를 확실히 막는 방법이지만, 아주 작은 변경 사항마다 새 버전을 빌드하고 기존 버전의 서버를 내리는 방식은 조금 무거운 작업일 수 있다. 오케스트레이션 도구오케스트레이션은 VM및 컨테이너를 어떻게 관리할지에 관해 관심을 두는 도구이다. 몇 개의 컨테이너가 유지되어야 하는지, 어떤 컨테이너에서 장애가 발생했는지, 어디로 요청을 포워딩해야 하는지 등 여러 서비스 사이의 컨트롤 타워 역할을 한다. 가장 대표적인 도구로는 쿠버네티스가 있다. 쿠버네티스도 IaC라고 할 수 있는 건가? 쿠버네티스는 원하는 상태를 오브젝트로 만들어 피드백 루프를 돌면서 지속해서 바람직한 상태에 있는지 확인하면서 오케스트레이션을 수행한다. 개발자는 이러한 “상태”를 yaml 구성 파일을 가지고 정의한다. 따라서 일종의 IaC라고 볼 수 있다. 프로비전 도구구성 관리 도구, 서버 템플릿 도구, 오케스트레이션 도구는 크게 서비스에 대해 주된 관심을 두는 도구들이다. 서버 내부가 어떻게 되어야 하는지 또는 서비스가 어떻게 실행되고 있어야 하는지를 관리하는 도구들이다. 반면 프로비전 도구는 이름처럼 서버 자체를 생성하고 관리한다. 인스턴스의 개수, 인프라 사이의 연결 등 인프라에 대한 모든 것을 프로비저닝 하는 데 사용된다. 테라폼은 대표적인 프로비전 도구다. 테라폼테라폼은 IaC를 위한 선언형 언어이다. 선언형 언어라는 것은 인프라의 상태를 정의하는 코드라는 뜻이다. “프로그래밍 언어”로서 기능을 한다고 생각하는데, 이유는 반복문과 조건절, 함수와 비슷한 모듈의 개념이 있기 때문이다. 테라폼 맛을 한번 보자. 123456789101112resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;ami-something&quot; instance_type = &quot;t3.micro&quot;}resource &quot;google_dns_record_set&quot; &quot;a&quot; { name = &quot;demo.google-example.com&quot; managed_zone = &quot;example_zone&quot; type = &quot;A&quot; ttl = 300 rrdatas = [aws_instance.example.public_ip]} AWS에서 인스턴스를 만들고 GCP에서 AWS 서버에 접속할 IP를 DNS 레코드에 넣는 작업이다. 모르는 사람이 봐도 대충 이렇겠구나 싶은 느낌이 오는 정도의 가독성을 가지고 있다. 핵심적인 테라폼 흐름테라폼의 핵심적인 흐름은 다음 구성을 가지고 있다. Write: 테라폼 코드를 적는 단계. 어떤 리소스들이 어떤 방식으로 구성되어있는지 선언하는 단계이다. Plan: 실제 인프라에 적용하기 전에 어떻게 변화가 생기는지 확인하는 단계이다. Apply: 인프라에 실제로 의도한 대로 변화를 가하는 단계이다. Write 단계에서는 테라폼의 문법대로 tf 확장자의 파일을 구성하는 방식이다. Plan은 이렇게 구성한 테라폼 파일들의 실행 계획을 확인하는 단계이다. terraform plan이라는 명령어를 사용한다. Apply는 위 계획대로 인프라에 적용하는 단계이다. terraform apply 명령어로 수행한다. Plan 단계에서 문제없이 실행될 것처럼 보여도 실제 Apply 단계에서 문제가 발생할 수도 있다. 예를 들어 변경하려고 하는 S3 버킷의 이름이 이미 있는 이름이라든지, 여러 이유가 있을 수 있다. 테라폼 코드를 작성하거나 팀이 이미 작성한 테라폼 코드를 가져온 다음 테라폼 코드가 실행될 수 있도록 초기화해주는 단계가 있다. terraform init 명령어인데, 이 과정에서 테라폼을 실행하기 위해 필요한 플러그인을 설치하기도 하지만 신경 써야 하는 점은 테라폼 백앤드를 설정하는 과정이다. 기본은 로컬에서 동작하지만, 이는 팀과 함께 작업하기에는 부적합하다. init 명령어가 하는 역할과 협업 과정에서 백앤드 초기화하기 위한 좋은 방법을 이 링크에서 확인해보자. Syntax 구성 요소테라폼의 문법적 요소를 구성하는 키워드와 규칙이 크게 두 가지 있다. 하나는 Argument, 다른 하나는 Block이다. Argument특정한 식별자에 할당된 값을 의미한다. 1image_id = &quot;1234&quot; 등호를 기준으로 좌측이 식별자, 우측이 표현 식이다. 프로그래밍 언어처럼 그냥 리터럴한 값이 올 수도 있지만, 프로그램에 의해 결정되는 표현 식이 오기도 한다. Argument는 특정 컨텍스트 안에서 유효성이 결정된다. 즉, 어떤 컨텍스트 안에 속해있는지에 따라 타입, 식별자 이름 등이 유효한지 아닌지 판단된다. Block1234resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;ami-id&quot; ...} 위 { ... } 부분이 블록이다. 블록은 타입을 가지고 있는데 위 예시의 타입은 resource라는 타입이다. resource 타입에서는 두 가지의 라벨을 기대하고 있다. 하나는 aws_instance와 같이 정해진 리소스 이름이고 다른 하나는 example과 같이 임의로 정하는 식별자 이름이다. 지금 당장 resource 타입에 대해 설명하고자 하는 것은 아니니 블록에는 라벨이 붙을 수도 있고 아닐 수도 있다는 점만 알고 있자. 블록 내부와 라벨은 어떤 타입의 블록 컨텍스트인지에 따라 달라진다. Module테라폼은 tf 확장자를 가진 파일들로 만들어진다. 이 파일들이 모여 모듈이 구성되는데 모듈의 기준은 디렉토리이다. 같은 디렉토리 안에 모여있는 tf 파일들이 모두 모듈로 활용된다. 어떤 디렉토리의[ 하위에 위치하더라도 그 둘은 다른 모듈이다. 테라폼이 실행되는 모듈은 하나뿐이다. 이 모듈을 “루트 모듈“(Root Module)이라고 한다. 그리고 이 모듈에서 사용하는 다른 디렉토리에 위치한 모듈들을 “자식 모듈“(Child Module)이라고 부른다. 모듈은 테라폼의 함수와 같은 역할을 해준다. 작은 단위로 나누고 테스트하고 확장성 있게 만들어서 여러 스테이지에서 반복되는 코드를 쓸 필요 없도록 해준다. 구체적으로 자식 모듈을 가져와 사용하는 예시는 이후 후술한다. ProviderProvider는 인프라를 제공하는 클라우드 혹은 오픈 스택과 같은 인프라 차원의 API 종류를 정하는데 사용하는 블록 타입이다. 제공하는 벤더에 따라 테라폼이 실행할 때 다운로드 해 오는 코드가 달라진다. Provider는 실행하고자 하는 루트 모듈에만 있으면 된다. 자식 모듈은 실행하는 루트 모듈의 프로바이더 정보를 사용한다. 1234provider &quot;aws&quot; { profile = &quot;terraform&quot; region = &quot;ap-northeast-2&quot;} 위 코드는 AWS를 사용하겠다는 것을 의미한다. provider 블록은 정해진 벤더사의 이름을 넣어야 하는 라벨을 요구한다. 그리고 블록 안에서는 필수적인 Argument들이 있을 수 있다. 테라폼은 정말 많은 프로바이더를 가지고 있다. 이 링크에서 확인해볼 수 있다. Resource테라폼은 인프라를 다루는 언어인 만큼 인프라 리소스를 정의할 수 있는 블록이 필요하다. 이 역할을 resource 타입이 해준다. 1234resource &quot;aws_instance&quot; &quot;web&quot; { ami = &quot;ami-id&quot; instance_type = &quot;t3.small&quot;a} 위에서 예시로도 사용됐는데, 두 가지의 라벨을 요구하는 블록 타입이다. 하나는 리소스 종류를 정해야 하고, 그다음 나오는 라벨은 로컬 네임이다. 로컬 네임은 테라폼 모듈 안에서 해당하는 리소스를 참조하기 위해 사용된다. 다른 모듈에서 같은 이름이 나오는 것은 상관없다. 리소스가 무엇이냐에 따라 안에 Argument들도 달라진다. AWS를 생각해보면 정말 수많은 리소스 종류가 있기 때문에 어떻게 작성해야 하는지를 일일히 외울 수는 없다. 공식문서에서 프로바이더마다 제공하는 리소스와 어떻게 테라폼 리소스 코드를 써야 하는지 알려주는 문서를 확인하면서 코드를 써야 한다. AWS의 경우 이 링크를 통해 알 수 있다. Data테라폼 외부의 데이터를 사용하기 위한 타입이다. 여기서 외부라는 말은 분리된 다른 테라폼의 설정이라든지, 인프라 내에서 변경되는 값들을 의미한다. 12345678data &quot;aws_ami&quot; &quot;example&quot; { most_recent = true owner = [&quot;self&quot;] tags = { Name = &quot;app-server&quot; Tested = &quot;true&quot; }} 위 데이터 블록은 테라폼에게 aws_ami를 읽어와서 example이라는 이름으로 필요한 정보를 읽을 수 있게 해준다. 예를 들어 테라폼에 DB를 생성한 다음 만들어지는 주소를 웹서버에게 넘겨주기 위해 사용할 수 있다. 데이터 블록 안의 Argument들은 데이터 블록의 쿼리를 만들어주는 역할을 한다. 1234567data &quot;aws_vpc&quot; &quot;default&quot; { default = true}data &quot;aws_subnet_ids&quot; &quot;default&quot; { vpc_id = data.aws_vpc.default.id} 위 예시 코드는 기본 VPC를 가져와서 data.aws_vpc.default에서 참조할 수 있게 하는 데이터 블록을 만들고 그 블록을 사용해 해당 VPC 안에 있는 서브넷들을 가져오는 코드이다. 서브넷들은 마찬가지로 data.aws_subnet_ids.default로 참조할 수 있다. Variable확장적인 인프라 모듈을 위해서는 적절한 변수들이 활용되어야 한다. 100개의 인스턴스 이름을 바꾸는 작업에 단순 작업이 필요하다면 확장이 어려운 것으로 볼 수 있다. 임의의 변수를 설정하는 것은 variable이라는 변수 타입으로 만들 수 있다. 1234567891011variable &quot;server_port&quot; { description = &quot;The port for HTTP requests. type = number default = 8080}variable &quot;alb_name&quot; { description = &quot;The name of the ALB&quot; type = string default = &quot;terraform-asg&quot;} 이 변수는 모듈에 대한 input이다. 루트 모듈이 자식 모듈을 불러올 때 변수를 지정할 수도 있고, 루트 모듈을 실행할 때 환경 변수 및 실행할 인자 값 등으로 넣어줄 수 있다. 만약 위 예시처럼 기본값이 정해져 있다면 따로 넣어줄 필요 없다. 위 예시에서는 server_port, alb_name이라는 이름을 모듈 안에서 var.&lt;name&gt; 형태로 사용할 수 있다. 1234resource &quot;alb_something&quot; &quot;example&quot; { port = var.server_port name = &quot;${var.alb_name}-cluster-lb&quot;} Output함수처럼 사용할 수 있으려면 반환 값도 필요하다. 모듈의 결과를 내보낼 수 있는 블록이 있는데, 이 블록이 output이다. 1234output &quot;asg_name&quot; { value = aws_autoscaling_group.example.name description = &quot;The name of the ASG&quot;} aws_autoscaling_group.example.name은 모듈 안에 있는 ASG 리소스 중 example이라는 식별자가 붙은 것의 이름을 의미한다. 다른 모듈에서 이 변수를 사용한다면 다음과 같이 사용하게 된다. 12module.[module_name].[output_name]module.webclusters.asg_name 반복문선언적 언어는 절차적인 유형의 작업을 처리하기 어렵다. 반복, 조건문 등을 지원하지 않는 경우가 많이 있다. 테라폼에서는 루프를 지원해서 이런 상황에서 사용이 가능하다. 테라폼은 다음과 같은 루프가 있다. for_each: 리소스 내에서 리소스 또는 인라인 블록을 반복한다. for: 리소스와 맵 타입을 반복한다. count: 리소스를 지정한 수만큼 반복한다. 12345678910variable &quot;aws_iam_user&quot; { description = &quot;IAM users with these names&quot; type = list(string) default = [&quot;neo&quot;, &quot;trinity&quot;, &quot;morpheus&quot;]}resource &quot;aws_iam_user&quot; &quot;example&quot; { count = length(var.aws_iam_user) name = var.aws_iam_user[count.index]} length는 내장함수이다. 여러 내장 함수가 있으므로 이 링크에서 학습하는 것을 추천한다. count는 index를 통해 순회한다. 위 예시는 aws_iam_user 리소스를 콜렉션의 길이만큼 반복한다. count를 사용할 때는 다음과 같은 제약이 생긴다. 리소스 안에서 특정 인라인 블록을 반복할 수는 없다. 인라인 블록은 태그나, 로컬 변수 등 블록 안에서 만들어지는 블록을 의미한다. 배열의 인덱스대로 처리한다. 만약 위 예시에서 trinity를 지운다면, 기존 IAM 중에서 morpheus를 지우고 trinity의 이름을 morpheus로 변경한다. for, for_each에 대한 설명은 이 글에서 하지 않았다. 구체적인 타입에 대한 설명과 몇 가지 내장 함수들을 설명해야 하는데, 글의 취지와 적합하지 않은 듯 하여 뺐다. 자세한 설명은 이 링크를 읽자. 조건문조건문은 아쉽지만 삼항 연산자로만 표현해야 한다. 1condition ? true : false 위와 같이 조건이 맞으면 앞의 값을 사용하고 틀리면 뒤의 값을 사용한다. 리소스를 생성할지 말지를 결정하는 것은 count를 함께 사용해 구성할 수 있다. 1234resource &quot;something&quot; &quot;something&quot; { count = var.some_boolean ? 1 : 0 ...} 만약 var.some_boolean 값이 true라면 이 리소스는 생성될 것이고 그렇지 않으면 생성되지 않는다. Usecase지금까지의 내용을 통해 테라폼이 어떤 느낌의 언어인지 파악할 수 있다. 선언형 언어라는 점, 프로그래밍 언어적 특성들을 어떤 방식으로 제공하고 있는지 등. 테라폼의 모든 내용을 이 글에서 배우기는 어렵고, 더 자세한 내용은 테라폼의 공식 문서가 굉장히 잘 정리해주고 있으니 참고해서 학습할 수 있다. 테라폼을 사용한 조금 구체적인 활용 예시는 다음과 같다. 테라폼을 사용하면 인프라에 대한 테스트 코드를 작성할 수 있게 된다. 그렇게 되면 안정적인 인프라 구성을 할 수 있게 된다. Stage를 분리하려고 할 때 동일한 상태를 간단하게 복사할 수 있다. 테스트 환경을 제거하는 방법도 destory 명령으로 간단히 해결할 수 있다. 테라폼으로 인프라를 관리함으로써 제거해야 하는 리소스를 빼먹고 남겨두는 상황을 최소화할 수 있다. 적어도 테라폼으로 삭제하지 못하게 되면 어떤 리소스가 왜 삭제되지 않았는지 파악할 수도 있다. 이 글로도 사실 테라폼이 어떤 건지 쉽게 감을 잡기 어려울 수 있다. 개인적인 생각으로는 직접 Example을 따라 해본다면 쉽게 이해할 수 있고, 프로그래밍 관점에서 테라폼을 바라볼 때 이 글이 도움이 될 수 있을 것 같다. Reference https://www.terraform.io/cli/commands/init https://www.terraform.io/intro/core-workflow#working-as-a-team https://learning.oreilly.com/library/view/terraform-up/9781492046899/","link":"/posts/etc/terraform-for-newbie/"},{"title":"주니어 개발자의 스타트업 3개월 회고","text":"스타트업 3개월사실 긴 기간은 아니라서 회고 할 만한 점이 있는가 싶긴 하다. 그래서 글은 짧아질 것 같긴 한데, 첫 개발직 회사나 다름 없었기 때문에, 짧은 기간에 필요한 내용들을 잘 배운 것 같기도 하다. 결과적으로 나는 이번 경험을 통해서 나에게 맞는 기업 가치관, 개발 하는 회사에 대한 요구 조건 등이 형성 된 것 같기도 하다. 이번 회고는 앞으로 나는 어떤 회사를 찾아 나가고 싶다는 점을 정리하고 싶다. 커뮤니케이션과 코드 리뷰이번에 있었던 회사는 커뮤니케이션을 하지 않았다. 아니, 하려고 노력 한 것 같은데 쉽게 바뀌지는 않았던 것 같다. 회사 분위기는 정말 차분하고 조용했다. 심지어 점심 시간 마저도 밥 먹으면서 서로 거의 한 마디도 하지 않았다. 입사 전에는, 개발과 관련된 얘기를 할 사람이 주변에 없어서 회사를 다니면, 이러한 사적인 대화는 제외 하더라도, 개발과 관련된 얘기를 하고 할 수 있는 환경이 있을 것이라는 기대가 있었는데 그런 점은 거의 없었고 앉아서 트렐로에 주어진 카드만 해결하는 노동을 했다 (정말 이 점은 별로였음). 졸업 이후 취업할 때는 꼭 여러 커뮤니케이션 채널이 있고, 개발자들 사이에 긍정적인 영향력을 주는 개발 문화가 정착된 회사를 꼭 가고 싶다. 예를 들어서 스터디가 활성화 되어 있다든지, 내부 세션을 자주 열고 있다든지 사내 토이프로젝트? 오픈소스 프로젝트 등이 별도로 존재한다든지 등… 노력이라 함은 식사 중에 세 마디 이상 하기 (얼마나 말을 안 했으면…), 그리고 코드 리뷰 시도가 있었다. 코드 리뷰는 깃헙을 이용했는데, 코드 리뷰 경험 자체가 없었기 때문에 회사에서 사용한 방식이 베스트인가는 판단할 방법이 없지만 아래와 같은 점을 주로 봐주셨던 것 같다. 중복된 코드가 이미 존재하는가? 아키텍처에 맞는 위치에 코드가 존재하는가? 해당 코드가 리소스 효율적으로 사용하는가? (이 부분은 사실… 거의 잘 안나왔다. 아마 회사에 시니어급의 개발자가 없었기 때문이 아닐까?) 나는 아래와 같은 점이 더 있으면 좋겠다고 생각했다. 읽기 좋은 코드인가? 적절한 네이밍이었나? 적절한 알고리즘이었나? 개발 스택과 포지션이번 회사에서는 풀스택 개발자로 입사했다. Node.js, MongoDB 백앤드와, React, JQuery를 사용했는데 JQuery를 사용한 부분은 레거시로 남아 있는 부분이고, React는 바꿔가고 있는 부분이었다. 입사 전에도 프론트보다 백앤드가 훨씬 재밌었지만, 프론트도 재미로 공부했었다. 그런데 회사를 다녀보니 확실하게 나는 프론트를 하고 싶지 않다(주로 맡고 싶지가 않다 정도? 사실 프로젝트 할 때는 괜찮다…). 앞으로 포지션을 선택할 때 프론트 부분은 신중하게 생각해볼 필요가 있을 것 같다. 또 지금은 백앤드 개발을 하고 싶다 보다는, DevOps를 하고 싶다. 최근에 자동화에 큰 관심이 생겨서 개인, 동아리 프로젝트에 천천히 적용해 보고 있다. 막히는 부분들을 뚫기 어려워서 뭔가 레퍼런스가 있는 환경에서 배운다면 좋겠다고 생각하는데, DevOps를 적용한 회사에서 일 할 수 있다면 (이왕이면 DevOps를 할 수 있다면) 정말 좋을 것 같다. 회사를 다니면서 가장 기대한 점은 테스트 코드를 쓰는 개발 방식이다. 그런데 이번 회사에서는 경험하지 못 했다. 회사의 개발 우선 순위이번 회사에서 느낀 가장 큰 점은 개발팀의 퍼포먼스, 개발 효율성을 위한 기술 도입을 비지니스에 의해 지나치게 막히지 않았으면 좋겠다는 것이다. 예를 들어서 Redux를 도입할 만한 상황인데 비지니스적인 요소로 개발 우선 순위가 무한으로 밀려나거나, 레거시를 대체하는데 큰 노력을 하지 않는다든지… 이러한 요소들은 개발 효율성, 퍼포먼스를 떨어뜨리고, 점점 악화시키는 요소라고 느껴졌다. 개발 문제를 해결하기 위한 기술 도입을 한 경험을 공유한 아티클을 읽으면서 정말 부러움을 많이 느꼈다. 말로만 도전을 두려워하지 않는 게 아니고 여러 시도를 장려하고 도전하는 회사를 선택해야겠다고 생각했다. 그런데 이러한 점을 어떻게 확인할 수 있을지는 모르겠다! 좋았던 점?회사 사람들 모두 좋은 사람들을 만났던 것 같다. 너무 잘 배웠다. 그리고 트렐로, 노션 등을 활용하는 방법, 깃헙으로 협업하는 방법 등을 조금 더 디테일하고 정형화 된 방식으로 배웠던 것 같다. 백앤드 API서버 아키텍처를 배웠다. 사실 그 전에는 MVC 패턴으로 작성했는데, 유사하지만(MVC 패턴으로 아예 설명하는 글도 보긴 했다.) 3 Layer로 나뉘는 아키텍처를 알게 되었고, 지금은 Monotholic한 앱을 만들 때는 같은 방식을 사용한다. (MSA를 시도하려고 노력하는데, 어떤 아키텍처가 좋을지가 항상 고민이다. Typescript를 시도하면서 types, interface를 또 어떻게 관리하는게 좋을지도…) 앞으로 나는 뭘 할 건가?당연히 학교를 졸업해야겠지. 학교를 다니면서 공학에 집중해보고 싶기도 하고, MSA를 열심히 배워보고 싶기도 하고, DevOps를 열심히 공부해보고 싶기도 하다. 교환학생을 가보고 싶기도 하고, 졸업하기 전에 창업도 해보고 싶다. 어느 정도는 동시에 할 수 없지만, 대부분은 동시에 가능하다고 생각한다 (열심히 살면!). 열심히 사는 건 좋으니까 시도해볼만 하다.","link":"/posts/logs/20191027/"},{"title":"콜로나 맵(코로나 원격의료 정보앱) 만들기 기록","text":"이번에 팀에서 코로나 원격의료와 관련한 정보 제공 앱을 만들게 되었다. 질병과 관련해서 형세가 급하게 흘러가다 보니, 원격 의료와 의약품 배달이 한시적으로 법적 제한이 풀리게 되었고, 원격 의료 및 의약품 배달을 실시하는 것에 대해서는 의료인들 자율적으로 따를 수 있게 했다. 해당 경험이 일반인들, 의료인들 모두 전무하다 보니, 프로세스에 대해서도 정부 지침 정도만 있고, 일반인들은 그 조차도 잘 모르는 게 현실이었다. 우리 팀은 PWA로 빠르게 해당 정보를 제공해주는 앱을 만들기로 결정했다 기술 스택이 앱이 우리 팀의 메인 앱이 될 것이라고 생각하지는 않아서, 익숙하지는 않지만 빠르게 만들 수 있는 스택을 골랐다. 앱 심사가 통과된 경험도 없어서 그 부분에서 얼마나 시간을 써야 할지 모르기 때문에, 우리는 PWA를 통해 사용자에게 앱을 직접 설치할 수 있게 했다. 웹으로 접근하면 이 앱이 무슨 앱인지, PWA를 홈 화면에 추가할 때는 어떻게 해야 하는지, 우리 팀은 이렇게 구성 되어있다. 라는 내용의 3장짜리 페이지를 만들었다. PWA는 React로 구성을 했다. serviceWorker를 조작하기 쉽게 해놓기도 했고 초기에 선택했던 expo web은 여전히 베타 버전이라고 하고 (분명, 35 버전에는 정식 버전이 릴리즈 될 거랬는데 지금 36이다.) 여러가지 제약이 있어서, 개발 중간에 아주 난처한 상황이 생길까봐 애초부터 리액트를 선택한 것이 있다. 프론트 개발자 친구가 TS에 익숙하지 않아서 JS를 사용하게 되었다. 이번 프로젝트에서 내가 프론트에 참여한 것은 극히 일부고, 배포와 관련된 작업을 진행했다. 애플리케이션 자체의 특별한 기술은 없었고, 배포할 때는 Github Action을 사용해서 S3에 배포되도록 했고, CloudFront를 사용해서 https 통신이 가능하게 했다. 설정하는데 오랜 시간이 걸리지 않기도 했고, 프론트 개발자가 master 브랜치에 머지하게 되면 자동으로 배포되는 환경이어서 배포를 직접 하지 않아도 됐다. 다만 아쉬운 점은 개발 단계에서 빠르게 여러번 배포 해보고 확인하는 과정이 있었는데 CloudFront에서는 캐시가 24시간 지속이 되서, 변경 사항이 제대로 반영되지 않았다. 직접 Invalidating을 해줘야 캐시를 없애고 새로 보여줬다. 이 부분이 프리티어가 이긴 한데 금액이 있다. 확인해보지는 않았지만 앱 자체자 작아서 프리티어 안에 해결할 수 있다고 생각해서 이 방법을 선택했다. 그렇지만 문서에서는 이 방법 보다는 객체에 versioning을 할 것을 추천했다. 이 부분은 아마도 웹팩으로 가능할 것 같기도 한데, 다음 단계에선 꼭 시도할 수 있었으면 좋겠다. 이 링크에서 관련된 내용을 확인할 수 있다. 백앤드는 서버리스 프레임워크를 선택했다. 선택한 가장 큰 이유는 해보고 싶어서 선택했다 (ㅎㅎ…). 그리고 일단, 이 앱에 사용자가 늘었을 때, 운영 환경을 신경쓰지 않고 싶어서라는 이유도 있긴 하다. 서버리스는 항상 관심 갖고 있던 것이기도 하고, 공부는 몇 번 했지만, 실제 프로젝트에 도입해 본 적은 없었다. 서버리스 프레임워크를 사용할 때 아키텍쳐에 대해 무지했기 때문에 그런 부분에서 조금 아쉬움이 많이 남았다. 개발하면서 가장 큰 화두는 아키텍쳐와 CORS 제한이었다. 우리 앱은 실제로 https://corona.deliverypharmacy.co.kr 이라는 앤드포인트와 https://callona.deliverypharmacy.co.kr이라는 앤드포인트 두 개가 사용되고 있다. API Gateway가 모든 서비스에 열려있지 않길 바라기도 해서 CORS 제한을 두려고 했는데, 생각해보니까 여러 앤드포인트를 CORS Origin header로 설정해둘 수 없었다. 그 부분은 뭐… 쉽게 해결이 가능하긴 했다. 그런데 궁금한게 사실 CORS 설정은 람다에서 보내주는 해더를 설정하는 것도 하고, 서버리스 프레임워크에서도 cors 옵션을 넣어줬다. 그렇다면 서버리스 프레임워크에 넣은 건 뭘 하는 데 쓰이는 건가?… 데이터베이스는 DynamoDB를 사용했다. 람다와 가장 잘 맞는 데잍터베이스라는 말을 많이 들어서 사용하긴 했는데 사실 잘 모르겠다. 일단 다른 걸 사용해보지를 않아서 그렇고, API처럼 사용할 수가 있어서 연결 관련된 레이턴시를 줄일 수 있기 때문인가 싶다. 우선 완전 처음은 아니지만 메인 데이터베이스로 사용해본 결과 정말 힘들었다. 이 링크에서 데이터베이스 사용 미숙으로 얼마나 맘에 안드는 코드를 작성 했고, 앞으로 이 작업을 할 생각에 힘이 빠진다는 내용이 있다. 백앤드는 쪽에서는 몇 가지 해결해야지 싶은 것을 남긴 것들이 있는데, 첫 번째로 서버리스 프레임워크를 RDS와 한 번 사용해보는 것이다. RDS를 VPC 내부에서 접근할 수 있게 만들고 싶기 때문에, 람다도 VPC 내부에 배포를 하고, S3 등에 접근하게 하기 위해서는 NAT Gateway같은 방법을 사용해야 한다. 프로젝트를 진행할 생각을 이미 하고 있어서. VPC 안에 람다를 배포하는 데모를 만든 적이 있다. 그리고 서버리스 프레임워크 가이드 중에 Express application에 serverless-http라는 모듈을 래핑해 배포하는 것을 봤는데, 이것도 사용해봐야겠다고 생각했다. layer도 사용해야 할 것 같고, 해보고 싶은게 많긴 하다. 아무튼 우리는 이렇게 개발을 했다. 원격 근무우리 팀은 만들어진지 얼마 되지 않았고, 그전까지 원격 근무를 해본 적이 없다. 인원도 얼마 없고, 모두가 모여서 일하는게 일상이었는데 코로나가 점점 심해지면서 원격근무를 진행했다. 사실 나는 원격 근무 허용이 좋다. 이러한 상황이 아니더라도 매일을 같은 공간에서 일하고 싶지는 않았다. 원격 근무를 걱정하는 많은 사람들이 도덕적 해이에 대해서 항상 얘기를 한다. 로켓펀치의 조민희 대표님은 성악설을 믿는 사람은 원격 근무가 불가능하다고 말했다고 한다. 무슨 말인지는 알겠는데 뭐 사실 대표하는 자리에 있는 사람일 수록 조직원들의 도덕적 해이가 걱정 안 할 수가 없다는 점은 누구든 공감하지 않을까? 성선설 성악설을 떠나서 일단 팀원에 대한 신뢰가 필요한 거라고 생각한다. 대표자가 팀원에게 신뢰를 보여주는 좋은 경험을 할 수도 있을 것 같다. (우리팀 대표님은 팀원들이 그렇게까지 신뢰가 가지는 않았나보다ㅎ 반대를 많이 하시고 걱정도 많이 한다. 그래도 뭐 결과적으로는 잘 해주고 있는 것 같다고 하시긴 했다.) 결과적으로 원격 근무는 좋다. 일단 현재 지내는 곳, 일하는 곳 모두 근처에 확진자가 많이 있기도 하고, 지하철을 타고 출근하기 때문에 좀 겁도 났는데 지금은 집에서 개발한다. 나의 경우 원격 근무를 진행해본 결과, 근무지와 큰 상관 없이 일을 하는 것 같다. 회사에서 근무 했을 때와, 집에서 근무 했을 때 waka time에 찍히는 시간도 비슷하고 (오히려 집에 있으니, 일상과 업무에 대한 경계가 모호해지면서 업무 시간이 훨씬 길어지기도 한다.), 퍼포먼스도 그다지 차이가 나지 않는다. 잠깐 쉬어야지 하는 것도 훨씬 편하게 쉬고, 스트레칭도 편하게 하고, 통근 시간이 없고 등 좋은 점은 참 많았다. 그럼에도 불구하고 안 좋은 점도 있긴 하다. 혼자 일하고 있자니 솔직히 좀 심심하긴 하다. 그 외엔? 없음 ㅋㅋ","link":"/posts/logs/20200307/"},{"title":"초기 스타트업 회고","text":"개발을 시작하고 1년이 되어서 아주 처음 시작하는 스타트업에 참여하게 되었다. 일반적인 초기 스타트업 경우와 다르게, 초기 자본이 어느 정도 있었기 때문에 월급을 적게 받았어야 한다든지, 뭐 정말로 열악한 환경에서 개발을 했다든지 이런 내용은 크게 없었다 (필자가 환경에 대해 둔감한 편이기도 해서 사실 환경은 크게 문제 삼지 않는다. 좋으면 좋은거지). 이번 글은 약 8월 동안 참여했던 스타트업 후기이다. 입사사실 초기 스타트업은 내가 창업하지 않는 이상 별로 내키지 않는 구석이 많았다. 연봉 협상이 쉽지 않다는 점, 과하게 아무 것도 없는 조직, 경험 부족 팀원, 적은 트래픽, 원하는 개발 이외 업무가 많다는 점 등 하기 싫은 이유는 너무 많았다. 그 당시 교환 학생을 갈 자금을 모아두기 위해 일을 시작한 상태였고, 교환 학생 가기 위해 필요한 영어 시험을 치룬 직후 쯤 되는 시기였다. 돈은 약 1년간 적당하게 모아뒀고, 6개월 정도 더 모으면 좋을텐데, 어떤 회사를 가면 내가 충분히 성장할 수 있을까? 이런 고민을 하던 중에 개발을 하시던 학과 선배님과 우연히 만나뵐 수 있는 기회가 생겼다. 선배님은 급성장 하던 초기 스타트업에서 1년간 일 했던 경험이 본인에게 있어서 따라 성장하게 되는 행운이었다고 말씀하셨다. 누구는 10년이 지나서 겪을 경험을 본인은 단계별로 아주 빠르게 겪을 수 있엇다고 하셨다. 이 말씀을 전해주시면서 성장 가능성이 있어보이는 초기 스타트업을 가보는 것도 지금 시기에 좋은 선택이 될 수 있을 것 같다고 하셨고 공감이 많이 되었다. 정말 기가 막힌 타이밍에 뭔가 생각을 바꾸는 조언을 듣게 된 뒤 학교 커뮤니티에서 유망한 분야의 스타트업 창업을 준비하는 사람을 만나게 되었고, 위에 원하지 않는 구석 중 많은 부분을 해결 할 수 있기도 한 상황이면서, 성장을 경험할 수 있을 거라고 판단해서 개발팀으로 뛰어들었다. 팀원대표님은 초기 개발자를 데려오기 위해서 큰 거짓말을 하셨는데, 쿠팡 개발자 급 CTO가 곧 합류한다고 거짓말 하셨다. 나는 그 말을 인용해서 친구를 데려왔다. 거짓말임을 알았을 때 불만이 생기긴 했지만… 어쩔 수 없는 노릇이었다. 아무튼 팀원은 나와 그리고 함께 개발을 시작했던 친구 한 명, 이렇게 둘을 데려왔다. 둘이서 많은 고생을 한 것 같다. 본인은 백앤드와 개발 환경 구성, 배포 위주로 업무를 맡고 친구는 React Native 개발 위주로 업무를 맡았다. 그렇지만 그런 명확한 구분은 없었다. 친구는 서버와 배포에 대한 지식이 많지는 않아서 내가 맡은 부분을 많이 도와줄 수는 없었지만, 나는 클라이언트 개발도 맡아서 진행했다. 딱히 불만스럽지는 않았다. 클라이언트 코드도 가끔 짜다 보면 재밌기 때문에! 개발 외적으로 팀원들 자체는 정말 좋았다. 개발자가 친구이기도 하고 대표님도 정말 좋은 사람이었고 함께 일하는 다른 팀원들이 정말 좋은 분들이었다. 인력난은 초기 스타트업이 겪는 가장 큰 문제인 것 같다. 잘 하는 사람을 데려오기 힘들고, 일은 너무 많다. 개발일이 힘들고 자시고는 사실 큰 문제가 아닌 상황이었다. 안정적이고 편안한 직장을 원한 것도 아니고 도전적이고 급진적인 성장을 기대하면서 왔기 때문에, 서비스가 빠르게 나오고 사용자가 빠르게 증가하는 걸 보고 싶었다. 그러나 서비스 타겟이 워낙 정치적이고 법적으로 예민한 구석에 있는 스타트업이라 현 상황에 발빠르게 대응하는 게 가장 중요한 문제였다. 여기서 발빠른 대응은 토스처럼 재난 지원금이 발표되었을 때 바로 서비스에 도입하는 그런 개념이 아니라. 우리는 아예 서비스가 없는 상태니까, 어떤 서비를 할지를 결정하는 그런 대응이었다. 쉽게 말해서 여러번 뒤엎어야 하는 상황인 것이었다. 거의 5개월간은 베타서비스를 만들고 뒤엎는 것만 했다. 프로젝트 레포지토리가 14 개 정도 만들어졌다. 정말 재미 없는 기간이었고, 퇴사 욕구가 뿜어져 나오던 기간이었다. 서비스를 빨리 해야 사용자도 생길텐데… 원하는 건 이런 게 아닌데 하면서 고통을 받았다. 진짜 최악이 뭐냐면 서비스를 뒤엎는게 새로운 서비스를 고민하는 게 아니고 A와 B 모델을 가지고 처음엔 A, 그 다음엔 B, 다시 A’ 형태로, 다시 B’ 형태 이런 식으로 몇 번이나 고민했던 문제를 다시 선택하고 그 다음엔 비슷한데 전 모델로 돌아가는 식이라서 지루함이 미쳐버린 시기였다. 그리고 매번 급박하게 만들어야 했기 때문에 날밤 새는 경우가 많았다. 좋았던 점은 딱 하나인데, 초기 모델이니 내가 원하는 스택을 사용해서 구현할 수 있었다. 그전 회사에서 경험하던 지독한 레거시 문제가 없다는 점? 물론 레거시는 쌓이게 되겠지만 그 당시에는 기분 좋은 일이었다. 문화초기 회사라도 문화? 라고 볼 수 있는 몇 가지가 있다. 이런 문화는 대표님과 핵심 팀원들이 주로 만들어 가는 편이다. 문화는 복지 혜택이 될 수도 있고, 일하는 방식이 될 수도 있는 것 같다. 우리팀 개발 문화는 정말 자유로운 편이었다. 선비같은 나와 친구의 성격이라 대표님이 항상 걱정하던 도덕적 헤이가 발생하는 일이 없었다. 사실 인원이 적으니 맡은 바 업무를 안 하면 바로 티가 난다. 코로나 사태가 겹친 시국이라 자율 출퇴근, 재택 근무 활용 등 괜찮은 문화가 있었는데, 개발팀을 제외하고 다른 팀원들은 이런 상황이 조금 답답하신 걸로 보였다. 사실 커뮤니케이션 문제가 있다고 하셨는데 말씀하신 대부분은 슬랙으로 해결 가능한 경우였고, 화상 통화를 통해서 해결할 수 있었다. 물론 모여서 얘기 해야 하는 상황이 없었다고 말하는 건 아니고, 개발팀도 그런 상황은 적극적으로 모여서 얘기 했다. 우리는 자율 출퇴근을 정말 **’자율’**적으로 잘 사용한 케이스라고 생각한다. 나는 사람에게 본인 퍼포먼스를 발휘할 수 있는 최적의 환경이 모두 이른 아침 사무실이라고 생각하지 않는다. 사무실에 앉아 있다고 일을 잘 하는 거라고 생각하지 않기 때문에 이런 문화가 궁극적으로는 최적화된 규율과 함께 적용되어야 한다고 생각한다. 다만, 회사는 동아리가 아니기 때문에 과도한 자유는 초기일수록 자제할 필요가 있다고 생각한다. 성장스타트업 성장은 투자로 어느 정도 윤곽을 볼 수 있는 것 같다. 물론 투자를 잘 받는다고 내실이 튼튼한 건 아니구나라는 점을 깨닫기도 했다. 회사의 아이템은 내가 보기에도 블루오션 상태이고 충분히 시장도 보장 받는 서비스가 될 것 같다고 생각 들었다. 그런 아이템만 보고 초기 투자가 들어오는 걸 보고 있자니 회사 가치 10억 20억 이렇게 투자를 받는 건 정말 계획과 적당한 거짓말로도 가능하구나 싶은 생각이 들었다. 우리는 약 6개월 동안은 방황하는 것이나 마찬가지였고 나오기 약 2달 동안 본격적인 활동을 하고 있었다. 이게 무슨 말이냐면, 나는 발판까지만 보고 온 셈이다. 다시 말해서 초기 스타트업을 들어와서 얻고 싶었던 경험을 한 번도 못 하다가 하기 직전에 나왔다는 뜻이다. 나온 이유는 개인적인 문제로 인해… 정말 아쉬움이 컸다. 개발팀도 곧 커질 예정이고 사람들도 점점 들어올 것 같았는데, 내가 원하는 경험을 하기 직전에 나와야 한다는 게 조금 많이 아쉬웠다. 보상초기 스타트업에서 느낀 중에 한 가지는 초기 맴버들에게 어떤 보상을 주는 게 맞는가? 어떻게 회사에 기여한 바를 보상받을 수 있게 하는가? 이런 질문들이 정말 중요하고, 이 질문에 대해 팀원들에게 확신을 줄 수 있어야 한다는 점이다. 초기 스타트업은 노력에 비해서 보상이 적은 것으로 느끼기 쉬운 환경인 것 같다. 그렇다면 어떻게 이런 보상을 주는 게 맞을까? 보통 스타트업은 일정 비율의 인센티브, 또는 스톡, 스톡 옵션을 제공해줄 수 있는데, 나의 경우는 스톡에 대한 내용과 인센티브를 경험했다. 스톡은 결과적으로 말하자면 되지 않았다. (됐으면 안 나오고 했겠지) 스톡을 주는 건 주주가 된다는 소리고, 지분을 많이 갖게 되면 그 만큼 회사에 묶이는 조건이 많아지는 구조인 것 같다. 지나치게 오랜 기간 동안 회사에 묶여 있으라고 써있는 계약서는 사실 나에게는 보장 받지 못하는 것과 같았다. 나는 이제 곧 내 서비스를 만들어보고 싶은데, 30대가 되어서나 보장받는 스톡은 매력이 떨어졌다. 중간에 팔고 나갈 수도 있다 이런 내용이 있을 수 있지만 사주는 걸 아무도 보장하지 않는다. 사실 많은 공동 창업자들이 이런 딜레마가 있을 것 같다고 생각이 들었다. 차라리 지분을 조금 덜 받고, 보다 빠른 시일 내에 확실하게 올 수 있는 보상이라면 나는 받아들일 수 있었다고 생각한다. 또 거절한 당시 생각엔 이런 계약 조건을 보면서도 끄덕일 수 있는 회사에 대해 헌신적인 마인드를 가지고 있는 사람이 이런 계약을 할 수 있는 거구나 싶기도 했다. 내가 이런 지분을 가지고 있다면 회사에게 좋지 않겠다고 생각하기도 했다. 내가 서비스를 만들고, 창업을 하게 된다면 공동 창업자에게 확실한 보상을 약속해 줄 수 있는 방법을 최대한 많이 고민 해봐야겠다. 인센티브는 프로젝트의 목표 달성일을 두고 달성 여부에 따라서 지급되는 개념이라고 봤다. 우리가 합리적인 기간 안에 기획된 바를 마치겠다고 했을 때, 그보다 더 많은 노력을 기울여 더 빠른 기간 안에 끝낼 수 있도록 동기 부여를 주는 건데, 우리는 기간이 초과 되었지만 기존 약속일에서 줄인 만큼 퍼센트를 인센티브로 받게 되었다. (80 퍼센트를 줄였다면 원래 인센티브의 80 퍼센트를 받는 형태) 인센티브는 사실 이런 방법으로 사용되어야 하는지 잘 모르겠다. 만약 인센티브를 바라보고 열심히 했는데, 목표 달성에 실패해 인센티브를 받지 못하는 경우에는 의욕 감퇴가 심하게 왔을 것 같다. 맺으며재밌는 경험이었다. 일단 대표님을 보면서 나도 내가 하고 싶은 사업을 얼른 해보고 싶다는 생각을 많이 하게 되었다. 준비가 완료된 시점은 없을 것 같다. 스타트업은 부족과 결핍 상태로 시작하고 부족과 결핍 상태와 함께 가는 것 같다. 개인적인 사정으로 회사를 그만 두게 되었지만, 만약 지금 일을 구해야 한다면 이런 회사에서 일을 하면 좋을 것 같다. 그러면 8개월 전에 원했던 경험을 할 수 있지 않을까 싶은 생각이 든다.","link":"/posts/logs/20200807/"},{"title":"2020년, 개발 2년 차 회고","text":"개발에 흥미를 갖고 시작한 지 대충 2년이 찼다. 만족스러운 개발자 직업을 가진 적이 없음으로 주니어 개발자라고 하기는 아직도 거북한 감각이 있다. 이번 한 해는 다사다난했으므로 성장하기 좋은 한 해가 아니었을까 싶다. 올해 경험한 내용을 정리하고 내년에는 더 탄력받을 수 있으면 좋겠다. 초기 창업 팀 작년 말부터 올해 8월까지는 초기 창업 팀에서 열심히 창업 과정을 겪었다. 바닥부터 시작하는 창업은 많은 걸 경험해볼 수 있었다. 과거에 한 개발자 선배님께서, 현재 정말 잘되고 있는 스타트업의 초기 멤버로 참여해, DAU가 늘어나는 과정을 짧은 시간 내에 경험했던 것이 개발자로서 쉽게 경험할 수 없는 것이라고 말씀하신 것에 홀려 초기 창업을 생각한 적이 있다. 운이 좋게도(?), 좋은 실적을 낼 수 있다고 생각되는 아이디어를 가진 초기 창업 대표님을 만나서 창업을 시작했다. 하지만 말씀하신 그 경험은 사실 타이밍에 대한 아주 큰 행운이 따라 주면 겪을 수 있는 케이스이다. 물론 시작 전에도 그렇게 생각했지만, 아쉽게도 그 과정을 초기 창업 팀에 있을 때 겪지는 못했다. 사실 목적으로 정한 바는 위와 같았지만, 더 많은 것을 경험하고, 여러 방면으로 새로운 시야와 방향으로 동기부여를 많이 얻을 기회가 되었다고 생각했다. 창업에 대한 아주 막연한 관심만 두고 있던 필자는 이 과정 이후 초기 창업에 어떤 단계가 있고, 개발 외에도 해야 할 것이 수만 가지이며 어떤 점들을 특히 신경 써줄 필요가 있을지, 팀원들과 어떻게 소통해야 좋을지에 대한 엄청난 고민, 팀을 리드할 때 어떻게 해야 팀에게 동기부여를 줄 수 있을지에 대한 고민 등등을 했다. 초기 창업에서 개발자가 겪는 문제초기 창업이라는 상황이 가져오는 몇 가지 고통이 있었던 것 같다. 보통 인력 부족으로 생기는 문제이다. 금전적 여유가 없는 단계에서 생기는 문제 예비 창업 패키지, 초기 창업 패키지를 바라보면서 달리는 상황에서는 금전적인 압박이 있을 수밖에 없다. 대표님께서 개인 돈도 꽤 투자하신 편이라, 일단 정말 바닥부터 하는 스타트업들과 다르게 여유가 아예 없었던 건 아니지만, 당연히 어마어마한 금액은 아니므로, 바로 다음 단계를 위한 시드 머니가 필요한 상황이다. 이런 상황에서 개발자 구인은 정말 힘들다. 초기 창업 멤버로 잘하는 사람을 데려오기는 정말 어렵고, 잘하지 못 하는 사람이더라도 포텐셜을 믿고 가기엔 금전적인 한계가 있다 (애초에 잘한다 잘 하지 못 한다는 것에 대한 평가조차 어렵다). 기본적으로 많은 사람과 함께 일할 수 없지만, 개발 일정은 정말 촉박하다. 바로 다음 달 안에 MVP를 뽑아야 하고 며칠 안에 완성해야 하고 이런 상황이 부지기수이다. 갈려 나가는 느낌은 정말 잊을 수가 없다. 하루 열 시간 이상 근무, 잠도 못 자는 경우도 많았다. 불완전한 사업 모델로 인해 생기는 문제 필자의 경우, 가장 동기부여를 깎아 먹는 문제는 여기에서 발생했다. 또한 이 문제로 인해 파생되는 여러 문제가 있다. 초기 창업 아이템은 정말 추상적이거나, 불완전하다. 물론 서비스라는 것은 항상 더 나은 방향으로 발전해야 하므로 불완전이라고 하는 것이 언제나 존재하지만, 초기 창업에서 불완전은 뜻이 아주 다르다. 피벗 수준의 기획 변경 과정이 수도 없이 발생하고, 정말 아예 새로운 프로젝트가 되어서 갈아엎고 처음부터 하게 되는 경우도 생긴다. 필자와 함께한 창업팀의 아이템은 상당히 규제에 민감한 사업이어서 특히 여러 번 프로젝트가 엎어졌고, 거의 코드 십만 줄 이상은 버린 것 같다. 그 과정에서 오는 현타가 정말 심했던 것 같다. 원하는 개발을 하지 못하는 문제 이 문제는 사실 초기 창업뿐만 아니라 어디서든 마찬가지이다. 창업에서 비즈니스 앞에 개발이 있을 수는 없기 때문에, 비즈니스 요구사항을 위해 선택할 수 있는 개발 선택지가 제한되는 경우가 있다. 하지만 초기 창업에서는 선택지가 과하게 좁혀지는 경우가 많다. 예를 들어서 본인은 테스트코드를 작성하고 주기적인 리팩토링 하는 것이 결과적으로 좋다는 걸 알고 있고, 그렇게 하고 싶었다. 하지만 MVP 모델인 것도 알고 있고, 쉽게 엎어지는 과정을 반복하기도 하고, 빡빡한 개발 일정을 보고 있으면서 테스트코드를 촘촘히 짜며 개발할 수는 없었다. 그리고 문제를 해결하기 위해 새로운 기술을 도입하는 것에 대해서도 아주 보수적으로 바라봐야 한다. 러닝 커브가 일정 수준 이상인 경우 적용할 수 없다. 초기 창업에서 개발자가 경험할 수 있는 것들힘든 경험도 있었지만, 그래도 하길 잘 했다고 생각이 들게 만드는 몇 가지 포인트도 있었다. 개발 외 다양한 업무를 맛볼 수 있음. 초기 창업의 팀원은 말 그대로 제네럴리스트여야 한다. 본인의 전문 분야가 아니더라도 해결해야 할 문제에 초점을 맞춰야 할 때가 많고, 필자의 경우에는 대부분 개발에 집중할 수 있는 환경을 최대한 보장 받았음에도 불구하고, 기획과 기업의 운영 관련해서도 일부 참여하기도 했다. 많은 개발자는 이런 것을 그리 좋아하지 않는다. 하지만 본인은 좋은 경험이었다고 생각했다. (회의가 많고 여러 이유로 사람들을 만나는 역할도 해야 했다는 점은 좋지 않았음) 공동 창업자로서의 대우 개발자에게 한정된 얘기는 아니지만, 창업에 함께 뛰어들면 위험 부담과 기여에 따라 공동 창업자로서 대우를 받게 된다. 감사하게도 대표님이 공동 창업자로서 필자를 잘 챙겨주셔서, 어느 정도 감사한 마음과 함께 동기부여가 잘 되던 기간도 있었다. 애초에 지분 공유 관련된 얘기라든지, 경영과 관련된 얘기를 이번 기회가 아니었다면 어디서 경험했을 수 있겠냐는 생각도 들기 때문에 이런 경험 자체가 신선하고 좋았다. 열정적인 사람들과 일 하는 맛 초기에는 소수의 인원이 프로젝트에 참여하고, 프로덕트에 기여하는 정도가 큰 편이기 때문에, 프로덕트에 대한 주인 의식도 높고, 다들 애정을 갖고 하는 기간이 있다. 이 기간에는 창업자들이 함께 열심히 일하는 과정에 껴서 하루하루를 즐겁게 일할 수 있었다. (사실 즐겁게는 자기최면일 수도 있다. 본인은 즐겁긴 했는데, 업무량 증가로 인해 즐겁게 보다 고통이 커지는 순간이 오기는 했다) 그래도 스타트업 판이라는 게, 초기 삼산텍처럼 느긋할 수가 없이 정신없는 구조이고 이런 구조 속에서 재미를 느낄 수 있는 사람이면 또 나름의 매력을 얻는 포인트가 되지 않을까… 학기필자는 현재 대학생으로, 가을 학기를 수료하면서 6학기를 수료한 상태가 되었다. 스타트업은 코로나 상황으로 인해 갑작스러운 개인적인 계획의 변경이 생기면서 오래 일하지 못하고 나오게 되었다. 남은 학점을 많이 채우고 조기 졸업을 할 수 있게 하기 위해서 22학점을 수강했다. 성적도 나름 잘 받았고 프로젝트에 생각보다 집중을 많이 해서 원하던 공부를 열심히 할 수 있던 건 아니었다. 소프트웨어공학이번 학기의 정말 80%를 차지했다고 볼 수 있는 수업이었다. 한 학기 동안 프로젝트를 하는 수업인데, 간단하게 만들 계획이었으나 백앤드가 요구 사항에 들어가고, 인공지능 AI 스피커와 결합한 프로젝트를 하려고 팀원들과 얘기하다 보니까 어느새 앱이 비대해져서 프로젝트를 쉽게 마무리 지을 수 없게 되었다. 결국, 한 달 조금 넘는 기간 동안 프로젝트를 진행했고 이 프로젝트 동안 그래도 나름 새로운 것들을 공부하려고 큰 노력을 했다. 프로젝트는 Ivy Lee Method를 구현한 애플리케이션이었다. 이 링크에서 프로젝트의 자세한 소스코드와 문서를 확인할 수 있다. 약 한 달 동안 많은 코드를 짰다 이 프로젝트에서 Go와 DynamoDB를 한번 써봐야겠다고 생각하고 (DynamoDB는 지난번에 맛보기처럼 사용한 적이 있는데, 익숙하지 않아 많이 당황하고 메인 데이터베이스로 쓸 수 있는 거 맞나 싶은 생각도 했었다. 처음 DynamoDB를 맛보고 남겼던 글도 있다) 그 이후로 여러 레퍼런스를 찾아보고, 특히 이번 학기에 이 독특한 구조의 데이터베이스는 어떻게 설계해야 할까에 대한 고민을 많이 하고 공부한 글도 남겼다. 그래도 만들면서 “복잡한 관계를 설정하기는 여전히 어렵군” 하면서 만들었던 기억이 있다. 필자는 꼭 학생으로서 받을 수 있는 많은 지원을 최대한 받으며 직접 창업에 도전해보고 싶다는 생각을 오래전부터 하고 있었기 때문에, 이번 학기에 아이디어 MVP를 한 번 빌드해 봐야겠다고 생각했는데, 하고 싶던 아이디어가 노코드 앱처럼 일반적인 케이스가 아니라서 직접 만들어야 했다. 계획은 학기와 그 MVP 만들기를 병행할 계획이었으나, 현실은 이 수업에 묶여서 아예 생각하지 않았던 앱을 만들어냈다. 20202019년에 작성했던 회고도 한 번 읽어봤다. 지금도 항상 비슷하지만, 필자가 봐 왔던 어떤 n년 차 개발자를 기준으로, 내가 n년 차가 되면 저분보다 더 잘 할 수 있을까에 대한 기대와 걱정이 항상 있었다. 처음 개발을 공부한 지 1년 되는 해에는 “당연히 가능하지, 지금 이 속도면 어떻게 불가능하겠어” 같은 느낌이 정말 컸지만, 최근에는 성장 속도가 이전에 비해 느려진 것 같다는 생각도 하게 된다. 아마도 개인적으로 경험해볼 수 있는 시나리오의 한계가 있어서 그렇겠다고 생각은 하고 있고, 그래서 가을 학기가 시작할 때쯤엔 오히려 공학 이론이나 알고리즘과 같은 기본기에 충실해져야겠다고 생각해서, 수업에 집중했던 것 같다. 결과적으로 내가 튼튼한 개발자가 되었는가? 그것에 대해서는 잘 모르겠다, 단시간에 해결되지는 않는 것 같고, 더 집중적으로 공부해야 할 것 같다. 20212020년 말에 교수님의 도움으로 창업 할 수 있는 사무실을 받게 되었다. 그리고 소프트웨어공학 수업에서 팀으로 만난 친구와 개발하는 친구, 디자인하는 친구 이렇게 4명이 같이 창업을 하기로 했다. 기대도 되고 걱정도 많고, 괜히 일을 벌여놓으려니까 감당 할 수 있나 싶기도 한데, 어차피 졸업 전에는 꼭 해보고 싶었던 거니까 올해는 데이터베이스, 알고리즘, 창업, 이 세 가지에 몰빵 해보는 거로 결정했다.","link":"/posts/logs/20201230/"},{"title":"4개월 간 서비스 개발 후기 (사업화 실패하는 데 성공)","text":"지난 21년 1월 2일부터 시작해서 4월 30일까지 열심히 노력해온 서비스의 사업화가 실패로 돌아갔다. 실패로 돌아갔다기 보다, 결과를 보지 못하고 정리하게 되었다. 4월 30일은 예비창업패키지가 선정되는 날이었다. 결과적으로 심사하시는 분들을 설득하는 것에 실패했고, 팀원들과 지속할 수 없다고 판단하여, 클로즈 베타 중인 서비스를 종료하고, 코어 컨셉만으로 동작하는 서비스 상태로 둘 예정이다. 이 기록은 4개월 동안 어떤 것을 배웠는지 회고하는 글이다. 이 글이, 그니까, 누군가에게 배웠던 걸 가르치고 싶어서 쓰는 내용이 절대 아니다. 이 글은 순전히 개인적인 경험과 의견이다. 우리 서비스는 망고테이블라는 서비스로, 만다라트 차트로 프로젝트를 계층적으로 분할하고, 서브 프로젝트를 통합적으로 관리하는 시각화 대시보드를 제공해주는 서비스를 기획했다. 기존 칸반 시스템으 포괄하면서, 서브 프로젝트를 포괄할 수 있다면, 충분히 이 치열한 시장에서 고개를 내밀 수 있을 것 같다는 생각을 했다. 우리의 망고테이블 로고… 나는 창업이 좋은 것 같아!나 개인적은 특성으로서도 한 두가지 느낀바가 있었다. 나는 창업을 천 억을 모을 수 있는 나와 가장 잘 맞는 방법이라고 생각했다. 서비스를 만드는 것을 좋아하고, 내가 좋아하는 서비스를 만들 때 정말 행복감을 느끼는 편이다. 이번 프로젝틀르 진행하면서도 물론 압도적인 작업량과 미래의 작업량을 생각하면 어지러울 정도로 힘든 적도 많았지만, 결과물이 눈에 나타나기 시작하는 점에서는 정말 재밌었다. 창업을 지금 시작한 계기는 졸업 전에 꼭 한번 만들어 보겠노라고 생각하던 서비스이기 때문이고, 두 번째는 학생으로서 받을 수 있는 많은 창업 관련 지원을 받을 수 있을 것이라 생각했다. 졸업이 1년 남은 시점에서 시작한 것은 좀 아쉽긴 했지만 (만약, 예비창업패키지만 되었다면? 1년 이상은 할 것 같다는 생각을 했기 때문에, 확실한 성패를 1년 안에 확인하지 못 할 수도 있겠다는 걱정을 했다.) 결과적으로 정말 깔끔하게 망했기 때문에, 시간 낭비가 딱히 없이 좋은 경험으로 남았다. 그러나 창업가가 나와 잘 맞을까? 나는 엔지니어 역할을 하는 것이 더 즐겁다. 나는 대표로서 재원을 끌어모으기 위한 활동을 할 때, 물론 미래를 그려보면서 느꼈던 즐거움도 있긴 하지만… 개발을 할 수 있다면 개발을 하는 상태가 더 좋았던 것 같다. 한 가지 가능성 있는 예상은, 서비스의 PMF가 잘 맞지 않았기 때문이 아니었을까? 내가 봐도, 언제쯤 트렐로보다 쓸만하다고 생각되는 서비스가 될 수 있을까에 대한 고민이 항상 많았다. 만약 내가 만들던 서비스의 PMF가 잘 맞아, 좋은 반응을 가져올만 했다면, 이런 대표로서 비개발적인 작업 역시 즐겁게 할 수 있었으려나? 팀원창업을 시작하는 사람들의 가장 답답한 부분은, 좋은 팀원을 어디에서 구할까에 대한 고민이다. 나는 시작이 가장 중요하다고 생각했고, 주변에 개발하는 친구 한 명, 디자인 하던 친구 한 명을 꼬셔서 시작했다. 다들 돈을 바란다기 보다는 서비스를 만들어보는 경험을 하고 싶었던 학생이었다는 점이 팀원으로 들어오는 데 큰 이유가 된 것 같다. 창업을 시작할 때는 4인으로 시작했으나, 4월부터는 3인 체제로 바뀌었다. 아주 대단치 않은 조직이었으나, 그 조직 안에서도 공동 창업 멤버로서 부적합하다고 판단되어 한 팀원을 내보내는 선택을 했다. 원인은 내가 판단했던 능력보다, 경험치의 부재가 더 큰 친구였기 때문인데, 초기 팀의 방향에서 눈에 띄는 도움을 줄 수 없었던 친구였다. 이 과정에서 느꼈던 점은, 명백한 JD를 수행할 수 없다면, 특히 초기에 팀원으로 데려오는 것에 대해 신중을 기해야 한다는 점이다. 같이 하게 된 친구도 친밀한 관계의 친구였는데, 지난 약 6개월 동안 목표에 집중할 수 있는 모습을 보여주어서 같이 해도 충분히 금방 역할을 해낼 것이라고 생각했기 때문에 함께 했는데, 쉽지 않은 동행을 했던 것 같다. 남은 두 팀원들은 그 뒤로도 정말 열심히 뛰어주었다. 돈 한 푼도 못 주는 창업이었지만 기량을 펼쳐주고 발전되는 모습을 보는 것도 신기했다. 좋은 사람들을 만나서 짧지 않은 시간을 함께한 것만으로도 원하던 것 중 하나를 이루긴 했다고 생각이 든다. 정리하자면, 팀원은 내가 지내왔던, 경험한 곳에서 지인들이 주로 오게 되는 것 같고, 사람마다 다르겠지만, 전혀 모르는 사람하고 하루 아침에 만나서 공동 창업자로 뚝딱 변모하는 경우는 거의 없을 것 같다. (아이러니하게도, 지난 직장에서 나는 그런 케이스이긴 했다.) 또, 팀원을 모시기 위해서는 그 사람의 역할에 대해 충분히 고민해볼 필요가 있다. 나의 능력의 한계를 커버해줄 수 있는 능력이 있는 사람인지, 또 창업에 대한 열정이 넘치는 사람인지, 같은 비전을 공유할 수 있는 사람인지, 창업가가 지시하지 않아도 많은 의견을 개진해주고, 스스로가 서비스를 발전시켜줄 수 있는 사람인지 등…. 창업을 하면서 “대표는 원래 외롭게 결국 혼자 이끌어야 해” 라는 말을 들었다. 위에서 언급했던 것들을 해줄 수 있는 공동 창업 팀원을 만난다면 최소한 혼자 하는 것 같다는 느낌은 최소화 할 수 있을 것 같다. 서비스초기 스타트업에 조인한 개발자는 으레 다음과 같은 생각을 하는 것 같다. “내가 서비스를 만들어 내는데, 내 기여도는 적어도 절반 아니야?” 필자는 사실 초기엔 서비스를 만들어내는 개발자나, 디자이너와 같은 실무자들의 지분이 상당히 높다고 생각했던 개발자이다. 절대 틀린 말은 아니지만, 과장 해석할 필요가 없다. 서비스를 만드는 건 서비스를 성공시킨다와 동일하지 않다. 서비스를 성공시키는 것은 비개발적 영역일 확률이 높은 것 같다. 절반은 서비스 개발, 절반은 그 외 사업의 영역이라고 본다. 오히려 지분으로만 냉정히 보자면, 대표가 왜 90퍼센트나 가져가야 해라는 생각을 싹 없애도 좋다. 망한 사업의 대표들도 열심히 했겠지만, 성공한 사업의 대표는 90퍼센트 이상을 했다고 생각한다. “나는 내가 아이디어, 기획도 하고 개발도 하는데, 그럼 내가 서비스를 만들고 있는 거 아니야?” 맞지! 그런데 사업은 서비스를 만드는 게 아니고, 돈을 벌어오는 것이라고 생각한다. 그것도 본인이 하고 있으면 본인이 대표 해야지. 나는 대표를 지내면서, 돈을 벌기 위한 준비 과정을 맛봤다고 생각하고, 그 과정 조차 쉽지는 않구나라는 걸 느꼈다. 마켓 핏이 잘 맞지 않는 사업은 끌고 가지 않는 게 무조건 맞는 것 같다. 시장을 개척한다는 발상은 너무나 훌룡하고 존경스러우나, 너무 어렵고 가능성이 적은 일인 것 같다. 가장 크지만, 우리가 신경 써야 할 일부분이 줄어든다는 것이 PMF가 잘 맞는 아이템을 선정하는 것이라고 생각한다. 우리 서비스는 어땠을까? 우리 서비스는 사실 상상으로는 굉장히 사용할만 하다. 이것 또한 나의 상상이고 부딪혀 봐야 아는 거지만, 잘 만들기만 하면 된다. 잘 만들려면 시간과 인력이 필요한 부분이 있었고, 인력을 충원하기 위한 재원을 마련하지 못하면서 사이드 프로젝트로 돌리고, 초장기적으로 한 번 끌고 가보려고 한다. 이 서비스를 포기한 상태는 아니다. 어쨌든 재밌겠다라고 생각했던 서비스라 그런지, 재밌게 개발했고 앞으로 사이드 프로젝트로 돌린 후 경험하고 싶은 것들이 꽤 많다. 초기 과정은 짧게, 근데 이제 완성도를 곁들인우리에게 초기 과정은, MVP를 만들어내는 것이었다. MVP는 가볍게 만들었지만, 우리가 원하는 바를 충실히 수행하는 상태는 아니었다. 무엇이든 예창패를 위해 지표를 만들 생각이었고 클로즈 베타를 빠르게 냈다. 속도는 나쁘지 않았던 것 같다. 우리가 Product Hunt에 게시하고, 클로즈 베타를 위한 유저풀을 모은 뒤, 메일을 통해 클로즈 베타가 오픈되었음을 알렸는데, 사람들이 우선 사용하기 위해 접근하지 않았다. “서비스가 별로네요. 쓰기 힘들듯 합니다.” 이런 느낌도 아니라, 메일을 통한 접근 정도가 정말 작았다. 그래서 다시 한다면, Product Hunt에는 클로즈 베타가 준비된 상태로 서비스를 올리고, 프로덕트 헌트를 통해 접근한 유저에게 서비스를 열어주는 방식으로 하고 싶다. 이 방식이 훨씬 많은 피드백을 얻을 수 있다고 생각한다. 너무 초조해 하지 말고, 천천히 칼을 갈 수 있는 마인드 셋이 있었다면, 사용한 유저 수도 늘고, 더 좋은 사용자 경험을 줄 수 있었을 것 같다. 그럼 이제?역량을 기르고 주변에 더 좋은 사람들을 만드는 과정을 지내야 할 것 같다. 언젠간 다시 창업을 도전 할 것 같다. 그런데 지금은 아주 훌륭한 팀원들고 함께 서비스를 만들고 싶다. 학점은 얼마 안남았지만, 아직 졸업까지 한 학기가 더 남아있고 공부할 시간이 충분히 있으니까, 열심히 기본기에 대한 공부랑 그동안 미뤄왔던 개인 프로젝트들을 정리해서 블로그도 다시 시작해야겠다. (지금 쓰겠노라고 리스트업 해놓은 글만 해도 좀 많다.)","link":"/posts/logs/20210501/"},{"title":"2019년, 개발 1년 차 회고","text":"2019년에 이룬 것들이번 해는 정말 많은 일을 했다. 2018년도에 학생회장을 하면서 지냈던 시간들 보다 어쩌면 더 바빴던 것 같고, 한 일도 많은 것 같다. 차이점은 내가 좋아하는 일을 하기도 했고, 진로도 결정되는 계기가 되었고, 이 블로그를 만들 수 있게 되었다. 2019년은 특별한 해이다. 2019년 1월 1일부터 뭘 했는지 기억한다. 2019년 1월 1일부터 2019년 12월 31일까지의 회고를 한 번 해보자. 피로그래밍나는 피로그래밍을 통해 처음 개발을 시작했다. 다시 말해서 2019년도 1월 1일부터 피로그래밍을 통해 개발을 시작했고 이제 딱 개발 경력이 1년이 되는 해이다. 그 전에 학과에서 하던 공부로는 내가 컴퓨터 공학과 맞는 건가 싶은 느낌만 가지고 있었는데, 피로그래밍을 통해서 여러 가지를 확인 할 수 있었다. 우선 컴퓨터 공학은 나와 잘 맞는다. 말도 안되는 스케줄을 잠도 안 자면서 소화하는 기간을 지나고 나니까 내가 재밌어 하는 일이 뭔지 확인할 수 있었다. 뿐만 아니라 개발 공부에 대한 방법에 대한 확신이나 신념이 서기도 했다. 특별히 나의 경우에는 처음 공부하는 부분의 이론적, 문법적, 개념적 부분을 최대한 빠르게 끝내버리고 프로젝트를 해보고 여러 경우를 맞이 하는 것이 가장 잘 맞는 공부 방법임을 느꼈다. 물론 지금도 책을 보면서 천천히 이론적 공부만 하는 것을 싫어하는 것은 아니고, 무언가를 제대로 공부할 때는 느리게 하지 않는 것을 선호하게 된 것 같다. 올 해 나와 같은 시기에 시작한 개발자 중에서 나보다 빠르게 성장한 개발자가 없다는 것을 확신하기 때문에, 이런 개발 공부 방법론을 이번 겨울 기수 피로그래밍 강의에 가서 널리 전파해줄 계획이다. 이번 기수는 생각해 볼 수록 감회가 남다른 기수이다. 정확하게 1년 전에 2기수 윗 선배님이 오셔서 강의하는 것을 보면서 내가 1년 뒤에는 저렇게 될 수 있을까? 라는 생각을 했던 게 기억나는데, 이런 궁금증을 가진 2020 겨울 방학 기수 후배님이 계시지 않을까 생각이 들었다. 1월 초에 가는 강의에서 꼭 당연히 가능하다고 얘기해 줄 생각이다. 구체적으로 피로그래밍에서 Django를 배우기 시작했다. 지금 생각해도 장고는 처음 개발을 공부하는 사람에게는 최고의 선택이 아닐까 싶었다. 아무 것도 모르는 사람이 가장 빠른 시간 내에 프로젝트를 진행할 수 있는 프레임워크라고 생각한다. 피로그래밍에서 밤을 새면서 한 프로젝트들이 제일 재밌었다. Git으로 협업 하는 방식을 직접 모든 프로젝트를 뒤집어 엎어 가면서 체득하였고, 밤 새면서 고통 받는 사람이 아님을 확인했고, 주변에 나와 잘 맞는 사람과 있을 때 큰 시너지와 자극을 얻는 것을 새삼 다시 느꼈다. 피로그래밍은 내가 개발 공부를 하루 열 시간 이상 하는 탄력을 제대로 심어주었고 나는 운이 좋게도 피로그래밍이 끝나자 마자 그 탄력을 이어서 자율적으로 원 없이 공부할 수 있는 환경에 놓여지게 되었다. 위한과 인턴위한 개발팀에서 클라우드 인프라를 지원 받으면서 인프라 공부를 할 수 있었다. 개발 동아리로서는 정말 최고의 동아리가 아닐 수 없었다. 다만 개발하는 친구들이 없어져서 아쉬웠다. 그 당시에 위한 개발팀에 개발하는 사람이 없어서 사실상 거의 나 말고는 없다고 볼 수 있었다. 위한에서는 방학이 끝나고 인턴을 했다. 정확하게는 위한과 관련된 회사에서 원 없이 개발을 했다. 개발 회사가 아닌 곳에서 짜잘한 개발 업무를 맡아서 혼자서 공부했다. 위한이 얻은 건 없다고 볼 수 있지만 나는 너무 많은 것을 얻어갔다. 그 시기에 REST API가 무엇인지 제대로 알게 되었고 많은 프로젝트를 했다. 그 중 하나가 연하대라는 대학생 미팅앱을 만드는 프로젝트였다. 완성하지는 못했다. 다만 DRF를 써보고 React Native, React, Redux 등 프론트앤드 프레임워크를 많이 공부할 수 있었고, 앱 환경에서의 인증 방식을 여러번 고민하고 JWT 방식의 인증도 시도해 봤다. GraphQL에 대해서도 접하게 되었고 회사에서 진행하는 외주 같은 프로그램에 직접 적용해 프로덕트를 만들었다. REST와 또 다른 매력을 느낀 계기가 되었지만, 이게 왜 효율적인지 그 때는 깨닫지 못 했다. 코드의 양은 그대로 같은데? 라고 생각하면서 생산성이 더 좋다는 아티클이 잘못 쓴 것이라고 생각했다. 아무튼 개발팀 덕분에 돈 걱정 없이 EC2, RDS, S3 정도는 많이 쓴 것 같다. 이 과정이 끝나고 나니까 내가 좀 잘 해진 느낌이 들었다. 솔직히, 이 때 성장 속도도 말이 안된다고 지금도 생각한다. 하루 평균 10시간씩 6개월 정도 개발하는 것을 마친 후에 나는 진짜 개발자가 있는 곳에 가서 지금까지 내가 해온 공부들이 맞는 것인지 확인해보고 싶어졌다. 여러 면접들위한에서의 학점 인정 인턴이 끝나고, 나는 개발자가 보고 싶었다. 나보다 월등하게 잘하는 사람들 사이에서 내가 잘 공부해 온 것인지 확인해보고 싶었다. 다른 개발 회사에 들어가기 위한 여정이 거의 한 달 정도 된 것 같은데 그 동안 약 7, 8 곳에서 면접을 봤다. 이 과정이 생각보다 내가 앞으로 무엇을 공부할지, 회사에서 요구하는 능력은 어느 정도일지를 느끼는 계기가 되었다. 어디서나 묻는 HTTP와 관련된 내용들, Database와 관련된 트랜잭션, 인덱싱, 설계 방법, 알고리즘 관련 내용들, 문제 풀기, Javascript의 비동기, 이벤트 루프, 호이스팅과 ES6 등 단편적인 지식들이지만 정말 중요하고 많은 내용들을 공부했다. 그리고 면접 가서 떨지 않는 법, 완전 모르지 않게 보이는 방법, 완전 모르는 거는 깔끔하게 인정하고 자연스럽게 지식을 줍줍 하고 가는 방법 등을 얻어간 것 같다. 특히 면접 보면서 얻은 경험 중에 가장 좋았던 건 공통적인 공학 내용 외(당연 이것도 좋지만) 평소 혼자 개발하던 경우엔 느끼지 못 했던 도커, GraphQL 사용 이유에 대해서 띵하는 깨달음을 얻은 경험이었다. 프론트앤드와 백앤드를 혼자서 하니까 GraphQL이 왜 좋은 건지 이해하지 온전히 이해하지 못 했고, 개발 환경이 혼자니까 Docker가 왜 필요한지 온전하게 느끼지 못 했던 것이라고 생각한다. 아무튼 이런 저런 경험을 하고 나서 면접에 대해 예리해진 상태로 팀원 30명 대, 개발팀 15명 정도 되는 스타트업에 들어갈 수 있었다. 조용한 스타트업운이 없게도 들어가게 된 스타트업은 나와 정말 안 맞는 곳이었다. 일단 집이 멀고, 만드는 서비스에도 크게 애착이 가지 않았다(이유는 모르겠다. 그 전에도 써 본 적이 있는 서비스였지만 그냥 개발하는 내내 기계같았다.). 팀원 분들 개인들은 다들 너무 좋았고 나에게 잘 대해 주셨지만, 팀이 너무 조용해서 식사 시간 조차 한 마디도 안하고 다시 올라온 적도 있는 그런 팀이었다. 개발자간의 개발 관련 얘기나, 협업 하면서 토론을 하는 그런 기대를 하면서 왔지만 수습기간 내내 한 번도 그런 재미를 못 봤다. 테스트 코드를 쓰는 것이나, 인프라에 대한 간접적인 경험도 기대했지만(솔직히 인프라는 몰라도 테스트 코드는 했어야 하지 않았나… 싶었다.) 그런 건 없었고, 문제가 많은 레거시 코드를 안은 체로 비지니스에 치여 계속 레거시를 쌓는 구조로 개발이 굴러가던 회사였다. 어떤 걸 바꿔야 하는 지는 모두가 공감하지만, 적극적으로 바꾸지는 못 하는 그런 상황. 내가 주니어이기도 하고 비지니스에 대해서는 접촉면이 없어서 쉽게 판단 내리기 어려웠지만 상황 자체는 실망감이 너무 컸다. 그래서 앞으로 어떤 회사를 가야 할지에 대한 여러 가지 기준점을 세우는 정도로 이 회사에서의 경험은 마쳤다. 수습 기간이 끝날 때 쯤 퇴사하겠다고 하고 퇴사했다. 학업을 마치고 싶다는 이유를 댔지만… 사실 학업은 큰 이유가 아니었다. 이 회사 이후로 회사 선택 기준이 많이 바뀌게 되었다. 그에 대한 내용은 다른 글에 작성되어 있다. 피로그래밍 운영진피로그래밍에는 남다른 애착이 있기 때문에 운영진까지 했다. 운영진은 재밌는 경험이긴 했다. 사실 피로그래밍은 개발 공부하는 장점이 완전 크지만 그만큼이나 엄청 열심히 사는 사람들이 모인다는 장점이 또 있다. 주변 사람들에게 이렇게 자극 받는 환경은 흔하지 않다고 생각했다. 여기에 모이고, 온전하게 완수 하는 사람들은 개발이 적성에 맞지 않더라도 어디가서든 열심히 살 사람들이라고 생각했다. 이런 사람들을 많이 만나고 싶기도 해서 운영진을 했는데, 운영진을 한 친구들하고 많이 친해지고 11기 몇 사람들과도 친해지고, 좋았다. 여전히 개발하는 친구들 모임은 여기에서 만난 사람들 지분이 상당히 높다. 11기 중에 한양대학교 사람들은 위한 개발팀으로 꼬셔왔다. 해커톤: Amathon아마톤은 정말 나에게 큰 반환점을 준 해커톤이다. 아마톤에서는 나에게 있어서는 꽤나 실험적이었는데, 서버리스와 Graphene였다. 특히 서버리스는 나에게 신세계로 다가왔는데 클라우드 의존적인 기술 스택을 사용하는 것이 생각보다 벽이 높지 않고, 공부하고 바로 사용할 수 있구나라는 띵한 경험을 했다. 이후 AWSKRUG라는 커뮤니티를 알게 되었고, 여기서 진행하는 클라우드 관련 밋업을 나가서 여러번 참석 했다. 커뮤니티 참석은 재밌었다. 현업자들이 클라우드 기술을 어떻게 도입했는지에 대한 내용이나, 클라우드 관련 핸즈온 등 새로운 걸 접하기에 퀄리티 높은 정보들이 많았다. 아무튼 해커톤에서 서버리스의 매력을 느끼고 이후 클라우드 의존성이 높은 프로젝트를 한 두 번은 진행해보고 싶었다. 지금까지도 클라우드를 적극적으로 공부하고 있다. 이런 퀄리티의 해커톤이 있다면 또 다시 나가보고 싶다. 기타 여러가지와 내년도개발적인 측면으로 큰 사건들은 위 정도로 정리해도 되지 않을까 싶다. 그 외에 CI/CD, TDD, Docker, Typescript 등에 큰 관심을 갖게 되었고 지금은 CircleCI, github actions, Mocha, should, jest, Docker, docker-compose, Typescript를 모두 사용해서 백앤드 개발을 하고 있다. 회사를 나오고 나서 교환 학생이 가고 싶어서 한 달간 영어 공부를 대충 하고 시험을 보고, 학교 졸업할 계획을 세우는데, 중간에 학과 선배님의 조언을 듣고 생각이 바뀌었다. 본인이 엄청 초기부터 시작한 스타트업이 상당히 커졌는데, 그 과정을 모두 함께 참여하다 보니까 남들이 10년 동안 해야 얻을 수 있는 경험치를 본인은 정말 짧은 시간에 쌓을 수 있었다는 조언과 개발자로서 빠른 성장을 기대하고 있다면 “성공할 것 같은” 초기 스타트업을 선택해서 한 번 뛰어 보라고 하셨다. 생각보다 울림이 있어서 머릿 속에 딱 저장 한 상태일 때 학교 커뮤니티에서 마침 초기 스타트업 개발자를 나쁘지 않은 조건으로 뽑는 것을 보고 지원했고, 지금은 그 회사에서 일을 시작했다. 기획 과정과 디자인이 나오는 과정을 모두 참여하고 있는데 재밌고 이전 스타트업에서 느꼈던 단절감이 전혀 없다. 또 공동 창업자로서 제안을 받아서, 당분간은 다시 하기 힘든 경험을 할 수 있겠지? 라는 생각 때문에 우선은 구두로 합류하겠다 한 상태이다. 계약이 어느 정도로 변경되는지 확인해야겠지만 별 변동이 없다면? 월급을 기존 제안보다 조금 더 인상하고 공동 창업자로 들어가길 바란다. 내년은 창업자로서 회사에 기여하고 있길 바라기도 하고, 또 정신없이 성장하고 있길 바라고 있다. 내년은 올해보다 더 재밌을 것 같다.","link":"/posts/logs/20191231/"},{"title":"2021년, 개발 3년 차 회고","text":"개발을 시작했다고 볼만한 시점부터 2022년 01월 01일 기준으로 3년이 채워졌다. 시작도 2019년 01월 01일부터 했기 때문에 3년을 딱 맞게 채웠다. 조금 늦은 감이 있지만 생일 기념으로, 그리고 매년 하는 일인 만큼 지난해 무슨 일들이 가장 유의미했는지 정리해봤다. 다른 개발자의 회고 내용에 기대하듯, 이 주니어 개발자는 무엇을 하며 1년을 보냈을까? 를 기대하며 이 글을 읽는다면 조금 실망하실 수 있습니다. 개발자보다는 개인적인 회고에 가깝습니다. 그러나 저는 개발자입니다. 졸업 전 창업 해보기 예전부터 만들어보고 싶었던 서비스가 있었다. 프로젝트 관리 SaaS를 만들어보고 싶었다. 간단히 설명하자면 지금 Jira가 충분히 만족스럽지 않고, 태스크 사이의 관계들을 더 잘 파악하도록 바뀌었으면 좋겠다는 생각을 자주 해왔다. 나는 그 방법이 트리 구조에 있으리라 생각했고, 뭐 비슷한 방법으로 문제를 풀어보려고 노력했다. 결과적으로는 실패를 경험했고, 그 과정은 이전에 “4개월간 서비스 개발 후기 (사업화 실패하는 데 성공)“ 이라는 글에 배운 점과 느낀 점을 나름 디테일하게 서술했다. 과정에서 많은 것들을 배우고 얻었지만, 사람들을 많이 얻은 것 같아서 좋다. 팀원들을 포함해서 내가 만들고자 하는 것을 응원해주고 같이 고민해준 사람들도 있고… 일단 교수님하고 친해져서 좋았다. 교수님은 서비스 자체에 대해서는 잘 모르셨지만, 공간을 빌려주시고 최대한 도움을 주시려고 많이 노력해주셨다. 수업 중에 나서지도 않는 학생이어서 이름도 잘 기억 못하실 법한데 이렇게 도움을 주셔서 정말 감사했다. 구체적인 내용들은 이미 다른 회고에서 작성했으니 이 정도로 마무리… 취준 사실 취준 과정이 엄청 고통스럽거나 힘들지는 않았다. 취업할 자신도 있었고 취업 준비 과정이 딱히 그전 생활과 다르지도 않았다. 물론 CS에 집중해서 공부했다든지, 포트폴리오 정리하고 자기소개서를 쓴다든지 하는 것들은 있었지만, 그전에도 개발 공부하고 자유시간 갖고 반복하던 삶을 살았기 때문에 특별히 다르다는 느낌은 없었다. CS 스터디, 멘토링스터디를 2개 열어서 진행했는데, 하나는 CS 전반적인 내용에 대한 스터디였고 하나는 시스템 프로그래밍을 주제로 한 멘토링이었다. CS 스터디과거에 취업 준비를 한다고 생각하면 Github에 올라가 있는 CS 토막 상식 같은 링크를 보곤 했다. 물론 훌륭한 레포지토리들이라 보면서 끄덕끄덕 기억을 살려내곤 했는데, 뭔가 CS에 대해 잘 알게 되었다는 느낌을 받지는 못했다. 그래서 학교에서 사용했던 교과서를 다시 보면서 CS를 정리하기로 했다. 교과서 스터디는 워낙 양이 많아서 취준을 하던 다른 개발자 친구, 이미 개발하는 친구와 함께 스터디를 진행했다. Notion에 정리하기로 하고, 학부에서 배운 수준의 교과서 레벨을 커버한다는 느낌으로 OS와 네트워크까지 잘 정리했다. DB, 자료구조까지 정리했으면 좋았을 것 같은데, 친구들이 바빠지고 취업도 되고 하면서 흐지부지 정리됐다. DB는 따로 학습하긴 했으나 정리되지는 않았고 조금 깊숙한 얘기들에 관해 공부해볼 계획이다. 일단 그래도 OS와 네트워크에 대해서는 배운 내용들이 잘 정리된 느낌이었고, 실제로 면접에 큰 도움이 되었다. 구멍들이 느껴지긴 하지만, 차차 해결해가면 되겠지. 멘토링학교에 연이 깊은 듯 얕은 듯 한 개발 동아리가 있다. 이 개발 동아리에서는 매 학기 멘토를 모집한다. 멘토는 주제를 선정하고 멘티들이 이 주제에 투표하는 구조이다. 평소에 리눅스 시스템을 조금 잘 써보고 싶다는 생각을 항상 하고 있어서 파일 시스템을 다루거나 소켓을 다루는 등의 프로그래밍에 대해 알아본 적이 있었다. 이런 걸 학습하는 주제가 Unix 시스템 프로그래밍이라는 것을 알게 되었고, 그 당시 Go에 흥미가 많아서 Go를 가지고 시스템 프로그래밍하는 내용으로 수업을 준비했다. 해당 동아리는 비전공자 또는 초보인 분들이 많은 특성이 있어서 관심을 끌기에는 조금 어려운 주제가 아닐까 하는 생각이 들었다. 다행히도 조금 고인물 분들이 나타나 스터디를 재밌게 끌어주셨다. 일단 교재는 Go System Programming이라는 책을 사용했다. 시스템 프로그래밍의 이론적인 이야기라든지, 유닉스 시스템이 파일시스템을 구성하고 있는지 등 자세한 설명이 부족한 책이었다. 다만 Go를 가지고 시스템 프로그래밍을 어떻게 할 수 있는지 등 조금 실전적인 이야기가 많이 담긴 책이었다. 조금 이론적인 이야기도 궁금해서 Advanced Programming in the Unix Environment 책을 같이 참조하면서 공부했다. 꼼꼼히 읽어보면 좋을 것 같은데, 스터디 진행이 꽤 빨라서 꼼꼼하게 읽어보지는 못했다. 스터디 진행은 멘토가 Go언어 자체에 대한, 또는 Go를 통해 어떻게 시스템 프로그래밍을 할 수 있는지 설명해주는 시간과 멘토가 정해준 주제에 대해 멘티들이 특정 주제에 대한 발표를 준비해오고 이를 세션 형식으로 발표하는 형태로 진행되었다. 준비 과정이 진짜 어려웠는데, 사람들이 잘 따라와 주고 발표 준비도 잘 해주셔서 재밌게 마무리 지을 수 있었다. 취준을 위해 했다고 할 수는 없어서 분류가 조금 애매하긴 한데, 아무튼 시스템 프로그래밍도 CS의 한 부분이기 때문에 취업에 도움이 안 되었다고 볼 수도 없을 것 같다. 알고리즘개발 공부 중에 추가된 것이라고 하면 알고리즘이 있을 것 같다. 알고리즘을 잘하는 편도 아니고, 경험이 많이 있는 편도 아니었다. 알고리즘 자체에 대한 경험을 쌓으려고 하루에 2, 3문제씩 풀었던 것 같다. 대충 200문제 넘어가고 나니까 알고리즘은 어떻게 푸는 거구나 느낌이 생겼던 것 같다. 물론 지금도 골드 1, 2만 만나면 좌절을 맛보고 있다. 그래도 어느 정도 코딩 테스트라고 하는 부분에서 막히던 걸 많이 해결해줬다. 알고리즘을 처음 공부할 때는 카테고리별로 공부했던 것 같다. 예를 들어서 Brute Force 문제, Greedy 문제, 이분 탐색, DP 이런 식으로 정해진 카테고리 문제들을 풀면서 이런 경우는 이 알고리즘이 도입되는 거구나? 이런 느낌으로 알고리즘을 풀었던 것 같다. 이 부분이 어느 정도 진행되고 나서부터는 그냥 프로그래머스 연습문제 2, 3단계에 있는 걸 쭉 풀었다. 알고리즘을 공부하면서 느낀 점은 알고리즘이 코딩하는 것과 크게 다른가? 라는 점이다. 이 부분은 아주 갑론을박 말이 많다. “알고리즘을 잘 못 한다고 개발을 못 하는 건 아니다.” 라든지 “사실 개발하면서 알고리즘을 제대로 써본 적이 없다.”든지… 본인은 알고리즘이 개발과 아주 유관하다고 생각하는 편이다. 그전에는 알고리즘은 아주 다른 영역이라고 생각했지만, 사실 기본적인 문제들을 해결하는 능력은 논리적 사고 능력일 뿐이다. 물론 굉장히 스킬을 가미하며 알고리즘을 해결해야 하는 경우는 조금 현실성 없다고 느낄 수도 있지만, 논리적 사고 연습이라는 측면에서 알고리즘을 연습하는 것이 개발을 더 잘하게 만들어준다는 느낌을 받는다. 따라서 취업하고도 알고리즘을 푸는 걸 취미 삼아 하나씩 하는 것도 좋을 것 같다는 생각이 들었다. 프로젝트프로젝트는 취준 목적으로 했다기보다는 개발을 안 하는 삶이 조금 무료한 감이 있어서 시작했다. 프로젝트를 하는 사회인 동아리? 라고 해야 할지… 아무튼 그런 곳에서 프로젝트를 진행했다. 초반에 프로젝트 진행이 많이 루즈해졌다. 이유는 진짜 창업 아이템을 고르듯 아이템에 대해 조사도 하고 기능 명세도 문서화하고 기본적인 컴포넌트들에 대한 합의 과정이 진짜 길어서 그랬다. 사실 이런 단계를 모두 거치는 사이드 프로젝트는 해본 적이 없지만, PO 역할을 해주시는 분이 요런 저런 걸 해보고 싶어 하시는 것 같기도 하고 나름 재밌어서 쭉 같이 진행했던 것 같다. 루즈한 출발 자체는 아쉽지만, 그 사이 과정에서 배운 점들이 많이 있어서 그래도 이 부분은 긍정적이었다. 개발에서는 아쉬운 점이 있었다. 개발 스택을 정하는 과정에서 같이 개발하는 팀원에게 많은 부분을 양보해드렸다. 팀원분도 취준 중이신데 목표하시던 회사 스택을 사용해보고 싶다고 하셔서 최근까지 더 이상 잡고 있지 않던 타입스크립트와 NestJS를 사용했고 MySQL을 사용했다. 사실 개발 스택은 별로 신경 쓰지 않기 때문에 상관은 없었지만 배울 수 있는 포인트가 줄었다는 점이 사이드 프로젝트의 매력을 조금 반감되게 했다. 심지어 서버 코드에 거의 기여한 바가 없으시다. 스켈레톤을 작성하는 것 외 서비스 로직이 머지된 적이 없었다. 이 부분은 좀 많이 아쉬웠다. 그러나 진짜 문제는… 사이드 프로젝트가 더 이상 진행되지 않는다는 것이었다. 물론 사이드 프로젝트는 그야말로 사이드니까 원래 하던 일이 있다면 우선순위가 뒤로 밀리는 경우가 허다하다. 그래도 뭔가 우리 팀이 흐지부지 프로젝트를 정지한 이유를 생각해봤는데 다음과 같은 이유가 있었던 것 같다. 주기적인 회의를 안 함: 주기적인 회의를 해도 했던 작업이 많이 없으니 회의 시간이 짧고 그걸 위해 약속을 취소해야 하는 등 부담이 좀 있다는 이유로 주기적 회의를 없앴다. 사실 이게 가장 큰 실패 원인이 아닐까 싶은데, 회의 주기가 뭔가 개발 기능을 마무리 짓는 주기로 동작했기 때문에 이 부분이 없어지면서 더 안 하게 된 것 같다. 그리고 애초에 이 정도의 강제성조차 안 가지고 사이드 프로젝트를 진행할 수가 없다. 작업에 정해진 시간이 없음: 언제까지는 해요! 라는 작업 시간이 없어서 무기한으로 미뤄진다. 정말 미친 듯이 바쁜 사람은 애초에 사이드를 할 생각을 안 한다는 가정하에, 일상 업무에 일반적인 시간 소비를 하는 사람들 기준으로 일주일 내내 일정 시간을 할애하도록 스케줄링하는 것이 불가능하지 않다. 강제적으로 3시간! 이런 식으로 정할 수 있는데, 각자 정한 이 시간을 기준으로 어떤 작업은 언제까지 마무리되어야 한다는 약속이 필요했던 것 같다. 원래 사이드 프로젝트 완성하기란 정말 어렵다는 것을 알고 있다. 그렇지만 이번 프로젝트는 진짜 사이드 팀 프로젝트에 대해 굉장한 회의를 느끼게 했다. 그냥 혼자 하는 것보다 못한 경험을 했다고 느꼈다. 앞으로 사이드 프로젝트는 진짜 친해서 강제성을 좀 더 부여할 수 있거나, 사이드 프로젝트에 진~심인 디자이너 + 프론트 개발자와 하거나 혼자 하거나 해야겠다고 생각했다. 인턴 길다면 길고 짧다면 짧은 듯한 취준 시간을 보내고 평소에 정말 가서 일해보고 싶던 기업의 플랫폼 개발 인턴으로 들어가게 됐다. 회사에서 크게 두 가지를 했던 것 같은데, 하나는 인턴 과제와 같은 서비스를 개발하고 이를 배포하는 과정이었고, 다른 하나는 팀에서 사용하고 있는 기술을 학습하는 과정이었다. 두 가지 모두 본인에게 큰 의미가 있었고 개발자로서는 굉장한 퀀텀 점프를 했던 경험이었다. 기술 학습에 대해회사에서 사용하고 있던 기술들을 내가 잘 알고 있지는 않았다. gRPC, Go를 공통으로 사용하고 있던 조직이었기 때문에 위 기술들을 학습하고, 전반적으로 팀이 사용하고 있던 NoSQL, RDB에 대해 학습했다. 이 학습의 가장 큰 포인트는 “깊숙한 이해“ 였다. 단순히 특징을 검색해서 나오는 얘기들 말고, 예를 들어서 “Go는 동시성 프로그래밍에 특화된 언어 구조를 가지고 있습니다.”라는 문구는 쉽게 찾아볼 수 있다. 그렇다면 “왜 특화되었다고 표현되어있지?”라는 질문이 나온다. 어느 겉핥는 학습을 해보면 그 질문에 대한 답변은 “go, channel 키워드와 여러 sync 패키지 등으로 개발자들이 동시성을 구성하면서 고민해야 하는 여러 부분을 해결해주고 있기 때문이다”라는 것을 알 수 있다. 그렇다면 그 고민이 구체적으로 무엇이고 어떻게 해결해주고 있는 것일지 구현체 레벨까지의 궁금증(코드 레벨까지는 아니고, 보다 추상적인 레벨에서)을 갖게 된다. 완전히 어떠한 특징에 대해 깊게 이해한 다음, 그렇다면 우리는 이 기술을 어떻게 활용할 수 있을까? 어떤 옵션들이 우리 상황에 더 잘 맞는지, 그리고 그 이유는 무엇일지? 를 고민하는 순서로 여러 기술들을 학습했던 것 같다. 사실 과거에 상당히 많은 부분이 경험을 통해 얻을 수 있는 영역이라고 생각하던 부분이 있었다. 어느 정도 학습하고 나면, “여기부터는 경험을 통해 알 수 있는 영역”이라고 생각하는 부분들을 마주한다. 물론 그 부분이 없다는 것은 아닌데, 이 과정을 거치면서 상당 부분은 이런 깊숙한 이해를 통해 많이 해결할 수 있다고 생각하게 되었다. 위에서는 조금 비중 있게 쓰진 않았지만, 사실 “깊숙한 이해”의 근본적인 목표는 “우리는 어떻게 사용할지”이다. 그래서 우리는 “어떻게 쓸 수 있을까?”를 조금 더 중요하게 본다. 식사를 기다리면서 시니어분에게 “대규모 서비스를 구성해본 경험이 없어서 ‘어떻게 쓸 수 있을까?’에 대한 정보는 떠올리기 어려운 것 같다”라고 말씀드린 적 있는데, 시니어분이 “기술적으로 까다로운 특정 상황에서 경험으로 어떤 기술을 사용하려는 사람은 사실 오히려 더 소수이다. 어떻게 쓰지?를 알기 위해서 깊숙한 이해가 요구되는 것”이라고 말씀하셨다. 깊숙한 이해가 기술을 대하는 핵심이지만, 그것이 핵심인 이유는 근본적으로 어떻게 쓸지를 보다 잘 알기 위해서이다. 잘 정리해서 쓴지 모르겠지만, 아무튼 이 공부 과정을 통해서 기술을 바라보고 대하는 태도, 학습 방법, 문제를 해결하기 위해 지나가야 하는 기술적 탐구 과정에 대한 에티튜드가 바람직한 방향으로 박힌 것 같아, 인턴이 끝나고 나서도 참 기분이 좋았다. 회사에서 학습했던 기술들은 조금 더 포괄적이거나 깊게 다시 학습해 정리하고 있다. 프로젝트에 대해회사에서 했던 프로젝트는 이 링크에서 구체적으로 확인할 수 있다. 과정은 위 링크에서 잘 정리되어있기 때문에 위 과정으로 뭘 배웠나에 대해서 간단하게 정리해보려고 한다. 일단 위 프로젝트에서 경험한 핵심은 “예측 가능한 애플리케이션”이다. 예측 가능하다는 것은 내가 만든 서비스가 얼마만큼의 성능을 낼 수 있는 앱인지를 말한다. 서비스에 어떤 부분에 노출이 되는지 또는 어떤 서비스 뒤에서 돌아가는 플랫폼 서비스인지, 그래서 얼마만큼의 TPS가 나올지를 알 수 있다면 약간의 버퍼를 두고 그만큼을 해결할 수 있는 TPS가 뽑히도록 앱을 설계하거나, 그것이 현실적으로 불가능하다면 지금 만든 앱이 몇 개의 서버로 부하를 분산해야 하는지를 아는 것이 예측 가능한 애플리케이션이라고 생각한다. 일단 위에서 말한 것처럼 어느 정도 규모가 있는 애플리케이션에서는 이런 작업이 필수적이기 때문에 서비스를 개발하는 일련의 과정을 학습했다는 것 자체를 배웠다. 그리고 최대한의 성능을 위해서 애플리케이션의 성능 테스트를 진행하고 병목 지점을 찾아서 고쳐서 다시 배포하는 과정에서 숫자적인 감각이 조금 생겼던 것 같다. 예를 들어서 어떤 서비스인지, 하드웨어 성능이 어떤지에 차이가 있지만 I/O 작업이 추가될 때와 아닐 때 어느 정도의 성능이 나오는 게 일반적인지라든지, 어떤 수치를 보고 어디서 생기는 병목인지 예측하는 감이 생겼던 것 같다. 일시적이지 않으려면 뭐 추가적인 경험을 해봐야 할 것 같은데, 일단은 이러한 경험을 한차례 했다. 졸업학교를 참 열심히 다녔는데, 학교 수업을 열심히 들었다기 보다는 해보고 싶었던 건 거의다 한 것 같다. 사실 학교 그 자체에 대해서는 좀 회의적이기도 하고 쓸 말도 없다. 졸업 당일에는 아쉬운 감정이 생길 줄 알았는데 그런 거 없고 그냥 행복하게 졸업했다. 지금 약 2, 3개월 지나고 나니까 실감이 나는듯하다. 그러나 여전히 아쉽지는 않다. 졸업식에는 다행히 친구 몇 명과 같이 사진 찍고 즐겁게 졸업식을 보낼 수 있었다. 굿. 회고 모임2021년 초 “왕각코”라는 왕십리에서 각자 코딩 모임을 했다. 친한 동생 한 명하고 같이 모여서 각자 코딩하거나, 책을 읽거나 그냥 만나서 할 거 하는 모임이었다. 둘이서 하니까 모임이라고 부르기 뭐하긴 했는데, 꾸준히 이 모임에 누군가를 초대해왔었다. 공통으로 알고 있는 여러 명을 초대했는데, 그중 한 분이 정기적으로 이 모임에 나오게 되었고 이 모임의 아이덴티티를 재설정했다. 재설정된 아이덴티티는 회고 모임이었고, 이름도 “우린 남이니까”라는 이름으로 바꾸었다. 우린 남이니까라는 말은 파카라는 방송인이 과거에 자주 하던 소리였는데, 모두 파카 방송을 재밌게 보는 사람들이기도 하고, 회고 후 피드백이나 조언을 가감 없이 전달할 수 있는 사람들이라는 의미를 붙여서 이름을 정했다. 이 회고 모임은 지금은 4명이 되었고 꽤 유쾌한 회고 모임이 되었다. 회고 모임에는 아주 각자 영역에서 열심히 살고 계시는 분들이 함께하고 있는데, 조금씩 좋아하는 것이나 생각하는 방향이 차이가 있으면서도 남들의 의견을 폭넓게 수용하고 자신들의 것으로 만드시는 모습을 보면 본인도 아주 삶의 동기부여가 된다. 또 일주일마다 무엇을 했는지 어떤 것이 아쉬웠고 어떤 것이 좋았는지, 다음 주는 어떤 것을 할지를 계획하는 것들이 인생을 막사는 것으로부터 어느 정도 방지턱 역할을 해준다. 막 사는 것을 막을 수는 없다. 그건 행복하다. 그리고 지금만 누릴 수 있는 것 같다. 그런데 그렇게 살고 난 주에 회고를 읽으면 자괴감도 들고 다른 분들이 훌륭하게 일주일을 마무리 지은 것을 보면서 반성하게 되는 효과가 있다. 말이 잘 통하고, 자신의 가치관에 대해 굳이 남을 동의하게 만들려는 분들이 아닌 사람들과 이런 회고 활동을 하는 것은 정말 추천할만하다. 최근 좀 아쉬운 점이 생겼는데, 세 분이 모두 같은 회사에 다니게 되었다는 점이다. 아직 그에 대한 피부에 와닿는 단점은 없지만, 그 전보다 다양한 얘기들을 듣기 힘들고 회사 욕을 하기도 어렵다는 점(예상)이 아쉽다. 그래도 원래 하던 기능은 온전히 잘하고 있어서 피부로 와닿지는 않는듯하다. 회고 전에는 뭔가 반성할 점이 많은 일 년이구나 싶었는데, 꽤 알찼던 것 같기도 하고? 이번 해에 개발자로서 한 단계 점프한 것 같다는 생각도 들었다. 벌써 상반기가 거의 다 지나가고 졸업 이후 첫 회사도 결정하는 단계가 되었고 고민 중이다. 다음 페이즈가 열리고 있다는 생각도 들고 커리어 골과 마일스톤 사이에 간극에 대한 고민도 많아진다. 이 글은 뇌에서 거의 바로 꺼내 쓴 글이라 두서가 없을 것 같은데 퇴고 과정 없이 올렸다. 회고는 참 어려운 것","link":"/posts/logs/20220417/"},{"title":"2023년, 개발 5년 차 회고","text":"2019년 1월 1일에 개발 공부를 시작하면서 블로그를 시작했는데, 벌써 이 블로그도 5년째 이어졌다. 이번 해는 나에게 꽤 특별한 해였다. 작년 12월부터 시작된 실존적 고민으로부터 시작되어 앞으로 나는 어떻게 살아갈 것인지, 무엇을 하는 것이 나의 꿈을 위해 가장 좋은 방법일지 고민도 많이 하고 결론도 나왔다. 먼 미래에 뒤돌아봤을 때 2023년에 내렸던 결정과 내 사고 방식의 변화로 인해 2023년은 내 인생의 챕터를 가르는 해라고 판단할 것이 분명하다. 이 글을 읽는 당신은 한 사람이 인생의 한 챕터를 여는 시작점을 보고 있다고 말할 수 있다. 내 꿈을 찾아서올해 초는 꿈을 구체화하는 시간이었다. 나는 ‘좋아하는 것을 열심히 한다’는 나름의 좌우명을 잘 지켜가며 살고 있는 것 같다. 그리고 나에게 정말 재밌는 것은 ‘많은 사람에게 내가 만든 가치가 전달되는 것’이다. 이 정도의 간단한 미래 꿈과 방향은 오래전부터 설정된 상태였다. 그리고 이 꿈을 구체화하는 과정에서 일론 머스크가 큰 영향을 줬다. 일론 머스크는 전 세계 사람들, 인류에게 중요한 기여를 하는 것이 정말 재밌을 것 같다고 생각하게 했다. 일론 머스크가 이끄는 기업은 모두 인류에게 어떤 가치를 전달해 주는 것을 목표로 하고 있다. Tesla: 세계의 지속 가능한 에너지로 전환을 가속화SpaceX: 인류 진화의 다음 단계로의 도약Neuralink: 미래의 인간 잠재력 실현 위는 미션, 비전 등에서 가져온 문구들이다. 일론 머스크는 이런 미친 꿈을 얘기해도 실현 가능성이 있어 보인다. 어떻게 그렇게 될 수 있었을까? 내가 생각한 이유는 이 사람이 그동안 보여준 문제 해결 능력으로 인해 생긴 영향력 때문이 아닐까 싶다. 자본주의 사회의 영향력은 자본이고 일론 머스크는 자산가다. 만약 내가 미래에 이런 인류의 문제를 해결하자는 꿈을 천재들에게 팔고 있는데 어떤 천재가 “정말 좋은 생각이고 당신의 의견에 동감합니다. 그런데 당신은 누구세요?”라고 했다고 가정해 보자. 그때 “저는 개발자입니다.”라고 하는 것은 설득력이 떨어진다. 쉽게 말해서 영향력이 큰 사람이 되면 내 꿈에 다가갈 수 있을 것 같다. 영향력을 키우는 방법은 부자가 되는 것 말고도 많다. 해결하고자 하는 영역의 아주 존경받는 지식인이 되어 직접 핵심적인 역할을 맡을 수도 있고, 한 국가의 수장이 되어 관련된 사람들을 모아 시작해 볼 수도 있을 것 같다. 하지만 내가 가장 즐거워하면서, 가장 가능성 높게 필요한 영향력을 만들어가는 방법은 일론 머스크처럼 비즈니스를 하는 것이 아닐지 하는 생각이 든다. 그래서 나는 일단 세상의 문제를 해결하는 걸로 부자가 되는 걸 먼저 마일스톤으로 잡아야 할 것 같다. 꿈은 인류의 문제를 해소하는 것으로 잡았다. 구체적으로는 정해가고 있는데, 문명의 발달 단계를 에너지 생산량에 따라 구분한 기준을 보고, 가장 영향을 크게 끼칠 수 있는 문제 해결은 인류의 에너지 생산 문제를 해소하는 것일 것 같다고 생각하는 중이다. 하지만, 여전히 추상적이긴 하다. 사람에게 얻은 인사이트위와 같은 생각을 하며 미래에 대한 숙고의 시간을 2023년 초반에 보낸 것 같다. 무엇을 하고 살아야 할까에 대한 고민, 언제 어떻게 해야 할지 고민 하면서 여러 사람을 만나봤다. 창업가, 개발자, 창업 팀과 얘기를 나눴다. SaaS를 개발하려고 하는 팀, 유니콘 또는 대기업에 다니는 개발자, 현재 시리즈 A를 완료한 팀과 그 창업자, 이제 팀 막 팀을 만들고 있는 창업자 등을 만났다. 몇 사람에게 큰 영향을 받은 이야기를 정리했다. 이게 정답이라고 생각하기 때문에어떤 결정에 대해 왜 그런지 물어봤을 때 정말 약 오르는 답변이 아닐까? “이게 정답이라고 생각했습니다.”라는 말은 너무 당연해서 다음 질문도 떠오르지 않는다. 국내 B2B SaaS를 만드는 팀 대표님과 얘기할 기회가 있었는데 얘기 하던 도중 대표님의 백그라운드가 엔지니어이고, 그 이후 VC, 그 이후 창업을 하게 됐다는 얘기를 들었다. 너무 흥미로운 커리어였다. 개발자를 하시다가 VC를 하게 되셨다는데 왜 그런 결정을 하신 걸까? 그리고 왜 창업을 하셨을까? 대표님은 자신이 무슨 일을 하든 네트워크가 정말 중요하다고 생각한다고 하셨다. 예를 들어 자신이 싱가폴 같은 곳에서 사업을 하려고 할 때 대통령하고 연결될 수 있는 사람과 네트워크가 있다면 무슨 일을 하든 영향력을 더 크게 만들 수 있다고 하셨다. 네트워크를 만드는 걸 제일 잘 할 수 있는 방법이 무엇인가 하고 생각했을 때 VC를 하는 것이라고 생각하셔서 그 길을 선택하셨다고 하셨다. 창업도 마찬가지로 비슷한 맥락에서 선택한 건데, VC를 하다 보니 많은 사업 대표를 만났던 것은 맞지만 부족함을 느꼈다고 하셨다. 그리고 은근히 노가다 작업도 많고 시스템화할 수 있는 부분들이 보이니 이 영역에서 창업하면 VC를 하던 때보다 더 많은 사람을 만날 수 있을 것으로 생각하고 창업을 시작했다고 말씀하시고 실제로 수십 배 더 큰 네트워크를 만들 수 있었다고 했다. 안되는 이유라도?창업을 막 결정한 대표님은 세계에서 손에 꼽는 컨설턴트 회사를 퇴사하시고 창업을 시작하고 계셨다. 팀을 구하고 계셨는데 지인에 의해 나도 팀원 물색 대상에 올라 짧게 얘기를 나눈 적이 있다. 창업가의 동기는 항상 궁금한 주제라 왜 이렇게 혹한기라고 불리는 시기에 창업을 결정하셨는지 여쭤봤다. 대표님은 나에게 지금 하지 말아야 할 이유가 딱히 없기 때문이었다고 말씀하셨다. 반박할 말이 떠오를 수 있지만, 하지 말아야 하는 이유를 자신에게서 찾을 때로 범위를 좁혀 생각해 보면 어느 정도 끄덕여진다. 내 입장에서 어떤 것을 미루는 건 준비 작업 때문이라고 할 수 있다. 예를 들어 “제가 창업을 하기 위해 초기 창업 팀과 함께 그 과정을 경험하려고 합니다.” 라든지… “초기 서비스 개발과 인프라를 배우고 시작하려고 합니다.” 라든지? 위 예시는 실제 내가 생각하던 준비 작업은 아니지만, 보통 이런 느낌이다. 여기까지 준비해보고! 여기까지 경험해보고! 하지만 준비는 천 년 만 년 할 수 있다. 그동안 창업을 몇 번 해보면서, 그리고 프로젝트를 리드 해보면서 느낀 점이고 이는 말씀 나눈 대표님도 하신 말이었다. 이 말씀에 정말 크게 공감했다. 이 두 사람의 얘기를 듣고 내가 하고자 하는 일을 위한 직접적인 방법이 아니라 간접적인 방법을 선택하고 있는 것은 아닐지 한 번 더 고민하게 됐다. “나는 A라는 걸 할 거야.”라고 생각하면 A를 달성할 수 있는 방법을 찾아가야 한다. 하지만 “나는 A를 위해서는 B를 먼저 해야겠다.”라고 하며 먼 길을 돌아가려고 하는 건 아닐까? 당시 조금 작은 팀으로 이직하려고 했던 나는 최종 합격을 포기했다. 처음 독특한 이력을 가진 대표님의 이야기는 사실 최종 면접에서 들은 얘기다. 아이러니하게도 대표님과 얘기한 후 그 회사를 안 가게 됐다. 친구와 얘기한 적이 있는 분야긴 한데, 꿈을 이루어 가는 과정에 있는 것인지, 미래에 꿈을 이룰 나를 위해 대비하고 있는 것인지 잘 구분할 필요가 있을 것 같다. 꿈을 이뤄가는 과정이라고 판단되는 어떤 것이든 A이며 해도 좋지만 꿈을 위해 지금은 꾹 참고 무엇을 한다든지, 뭘 배우고 온다든지 이런 것들은 B에 해당한다. 목표 세우고 달성하기하반기에는 꿈을 위한 첫 번째 목표를 세운 시기이다. 나는 앞으로 좋은 사람들과 좋은 기회를 얻으려면 지금의 기회비용을 모두 없애야 한다고 생각했다. 창업가를 만나서 얘기할 때 같이 하자고 말씀하시는 걸 들으며 지금 버리게 되는 회사의 연봉이나 커리어를 고민하는 내 모습을 보게 됐다. 나는 지금 제안받은 기회 앞에서 온전히 그 가치에 집중한 평가를 할 수 없다는 걸 느꼈다. 나는 적어도 생활이 가능한 수입이 일을 하지 않더라도 있어야 할 것 같다고 생각했다. 그래서 2023년의 목표를 하반기에 설정했는데 목표들은 다음과 같았다. 다른 것들도 중요한 목표였지만, 나에게는 추가 수입이 남은 약 5개월 동안의 가장 중요한 목표가 됐다. 3천만 원 정도의 수입이니까 대략 월 250만 원 정도의 수입을 목표로 했다. 최소한의 수입이라는 목표는 중요하지 않은 B가 아닌가? 고민한 적이 있다. 목표를 이뤄가는 과정에 아니라는 것을 느꼈다. 이 과정은 나에게 문제 해결 능력을 주고 꿈을 이루는 과정과 일직선 상에 있다고 느꼈다. 1. AI로 블로그를 흥행시킬 수 없을까?가장 먼저 떠올린 건 지금 블로그처럼 광고를 붙인 조금 더 일반적인 주제를 다루는 블로그이다. 지금 블로그는 한 달에 약 1,500명 정도가 방문하고 한 달에 약 $4가 Adsense 금액으로 들어온다. 선형적으로 증가할지 알 수는 없지만 선형적으로 증가한다면 한 달에 약 1,500,000명이 방문해야 하고 하루에 5만 명 정도가 방문하는 블로그를 만들면 Adsense로 충분한 수입을 만들 수 있을 것 같았다. 그 당시 내 생각에는 “내가 일반적인 주제에 대해 지식이 별로 없으니 AI가 영어로 쓰도록 하면 어떨까?”라는 생각을 했다. 그래서 운동, 여행, 코딩 관련된 내용으로 AI가 글을 쓰도록 했다. 하지만 글의 퀄리티를 높이기 쉽지 않았다. 이런 글을 많이 올리면 과연 노출이 잘 될까? 실제로는 그렇지 않았다. 지금은 모두 내렸지만, 그 당시에 블로그 글을 약 20개 정도 올렸는데 효율이 높지 않았다. 프롬포트를 잘 못 만든 것일 수도 있다. 하지만 글 쓰기 자동화도 쉽지 않았고 글의 퀄리티도 쉽게 나아지지 않았다. 올해 목표 달성을 위해서는 조금 더 빠르게 결과를 얻을 수 있었어야 했다. 2. 적은 돈이라도 벌 수 있는 서비스를 해보자.사소하게 돈을 벌 수 있는 아이템은 뭐가 있을까? 나는 대학생 미팅 앱을 가장 먼저 시도했다. 대학생 미팅 시장에는 시장을 대표하는 큰 기업이 없고, 시장의 수요도 어느 정도 검증된 영역이다. 지금 여기에 큰 플레이어가 없는 이유는 기업으로 발전할 정도의 돈이 모이는 영역이 아니기 때문이라고 생각했다. 만약 전국에서 서비스할 수 있고, 이 서비스가 1년에 사람당 만 원의 수입을 올릴 수 있다면 내가 생각한 작은 용돈벌이가 가능할 것으로 생각했다. 실제로 나와 팀원은 매칭 알고리즘을 준비하고 개강 시점과 축제 시즌에 맞춰 홍보했다. 에브리타임에는 우리와 동일한 생각을 한 아무 특색 없는 미팅 서비스가 비슷한 시기에 우르르 올라왔다. 물론 우리의 서비스 역시 아무 특색이 없는 매칭 서비스였다. 우리 서비스를 통해 매칭된 사람들은 있었지만, 사람들은 우리 서비스였는지 기억도 못했다. 서비스 운영 중에 매칭된 사람과 얘기를 한 적이 있는데 “여러 서비스에 지원했는데, 그게 어떤 건지는 잘 모르겠네요.”라는 말을 들었다. 이 경험을 하면서 ‘최소한 다른 서비스와 차별화될 수 있는 하나의 특징을 만들 필요가 있겠구나’라는 생각과 ‘문제를 해결하는 것 중 간단한 게 없는데, 작은 시장의 문제를 해결하기 위해 에너지를 쏟는 것보단 큰 시장의 문제를 해결하는 것이 좋겠다’라는 생각을 하게 됐다. 구체적인 피드백은 많이 있다. 예를 들어 왜 대학생 미팅 서비스가 돈을 못 벌까, 우리 서비스는 왜 안 됐을까 등 스스로 생각한 결론들은 많이 있다. 중요한 얘기는 아니라서 생략했다. 추가 에피소드를 적자면 최근 어떤 사람이 “안 팔린 이유가 특색이 없어서가 맞는가? 휴지도 특색은 없어도 팔린다”라는 말을 했다. 처음엔 차별화가 핵심 이유가 아닐 수도 있겠다고 생각했다가, 휴지랑 비교할 건 아닌 것 같다고 생각했다. 휴지는 생필품이고 사긴 해야 한다. 수요의 규모부터 차이가 나고, 실제로 싸거나 사용했던 경험이 좋았거나 등 작은 이유로(차별화로) 구매가 나눠지기도 한다. 두 번째 프로젝트는 해외 직구 관련된 프로젝트이다. 앱을 만드는 작업이라 대학생 미팅보다는 조금 더 할 게 많지만, 어느 정도 유명한 비즈니스 모델이 있고 운영해 가면서 발전시킬 영역이 많이 있다고 판단했다. 가장 초기 모델은 CamelCamelCamel처럼 가격 변경을 알려주고 할인이 시작되면 유저에게 알려주는 걸 생각 중이다. 하지만 유저 인터뷰를 하면서 해외 직구를 많이 하는 사람들을 만나보니 생각보다 가격에 민감하진 않았다. 오히려 가격보단 한국에 없는 상품을 찾기 위해 시간을 많이 쓰고, 있으면 일단 사는 경우가 많았다. 나처럼 가끔 해외 직구를 하는 사람들은 공감하지 않을 수 있지만 매일 사용하는 사람들이 말하는 말은 꽤 반복되고 설득력 있었다. 현재 이 문제를 해결하는 방법이 꽤 낡은 커뮤니티를 이용하는 것임을 알게 됐다. 앞으로 서비스를 발전시켜 갈 분야는 해외 직구 과정 중 탐색을 위해 사용하는 비용을 줄여주는 프로덕트이다. 한참 진행 중이고 어떻게 만들어갈지 상상하는 과정이 굉장히 즐겁다. 3. 내가 가진 지식을 팔아보자.지금까지 목표 달성에 가장 효율이 좋았던 방법은 지식을 파는 방법이다. 나는 올해 인프런에 내가 가진 지식을 정리해 올렸다. 내가 인프런에 올려야겠다고 생각한 주제는 다음과 같은 기준이 있었다. 만드는 데 시간이 너무 오래 걸려선 안 됨: 나는 강의 만들기에 익숙한 사람도 아니고 한 번의 사이클을 경험해 봐야 하는데 이 사이클이 너무 길어지지 않게 해야 했다. 너무 많은 경쟁자가 없어야 함: 처음 하는 강의다 보니 강의의 퀄리티를 아주 잘 뽑기 어려웠다. 니치한 영역이더라도 경쟁자가 적고 적당한 수요가 꾸준히 있는 영역을 찾았다. 그렇게 출시한 강의는 나름 좋은 성적을 보인다. 이 강의는 대략 달마다 50만 원 정도의 수입을 만들어준다. 목표한 금액은 아니었지만 그래도 상관없다. 다음 강의는 더 잘 만들 수 있고 더 빨리 만들 수 있다. 이런 강의를 두세 개 더 만들면 한 달 목표에 도달할 수 있을 것 같다. 물론 강의가 초반에 잘 팔리고 갈수록 판매량이 저조해지는 형태를 보일 수도 있을 것 같다. 최대한 그런 영향이 없는 내용을 고르겠지만 이 방법으로 얻은 돈은 다음 시도를 위한 체력 보충제 정도로 사용할 수 있을 것 같다. 예를 들어 연 3, 4천이 여기로 들어올 수 있게 하면 2, 3년 정도 다른 일을 할 수 있는 체력을 얻은 것이다. 약 5개월 조금 넘는 기간 동안 이런 일들을 했다. 회사 다니면서 이런 일들을 해내는 것이 쉽지 않았다. 회사 일 끝나고, 주말을 모두 갖다 박으면서 했다. 지난 5개월 동안 내 삶은 아주 단순했다. 크게 회사 일, 내 일, 잠 이렇게 세 개의 사이클을 돌았다. 열심히 살기 위한 필요 조건열심히 살고자 하는 모든 사람들에게 가장 필수적인 걸 깨닫게 됐다. 나는 기본적으로 체력이 좋은 사람이라 건강에 대해 큰 신경을 쓰는 스타일은 아니었다. 상반기 시작부터 대략 3개월 동안 잠도 잘 못자고, 맨날 회사 식대로 배달 음식 먹고 커피 하루 두 잔 마시고 아이스크림을 개많이 먹었다. 건강 검진한 이후 일이 터졌고 몸 상태가 심각한 수준으로 안 좋아졌다. 혈관 문제도 생기고 몸 컨디션도 너무 안좋아졌다. 그 이후는 가능한 매일 운동하고 체중 관리도 시작했다. 생활 습관 중 식습관을 제일 못 챙겼다고 생각해서 열심히 사는 것과 관련이 없을 수도 있다. 하지만 매일 앉아서 일하고 운동을 안 하는 것이 문제가 아니었다고 볼 수도 없다. 열심히 살려고 하는 세상 모든 사람들아, 할 일 하느라 운동할 시간이 없다는 소리는 하지마라. 운동할 시간은 무조건 만들어야한다. 열심히 살고 싶다면 꼭 운동을 하자. 여러 방법으로 얻은 인사이트들나는 원래 잘 바뀌지 않는 대전제를 기반으로 연결 고리가 충분히 합리적인 경우 나의 논리로 활용한다. 그렇게 해서 생긴 나만의 개념도 많고 이런 개념이 모여 나의 행동이 일관되도록 도와준다. 이번 해는 정말 배운 점이 많았다. 회고는 나에게 축복나의 회고 역사는 꽤 길지만, 이렇게 회고가 체계적이게 된 건 2023년이 최초이다. 위에서 말한 것처럼 하반기부터 남은 기간 동안 달성할 목표를 만들고 그걸 달성하기 위한 분기 목표, 월 목표, 주 목표를 만들고 주 목표를 이루기 위해 매일 Ivy Lee Method를 실천했다. 그리고 매주 일요일에 한 주를 회고하며 목표를 이루기 위한 더 좋은 방법들을 찾아 나섰다. 이 사이클은 나를 엔지니어링 하는 것과 같은 느낌이었다. 내가 조금 더 좋아지는 방향으로 미세한 튜닝을 해가며 아주 만족스러운 생활을 하도록 도와줬다. 이 과정에서 메모어라는 회고 모임과 지인들끼리 하는 회고 모임을 같이 했는데, 다른 사람들의 인사이트 및 생각들을 보며 새로운 시각을 흡수할 수 있었다. 동기 부여 영상 중독?동기 부여 영상이 유튜브에 유행한 덕분에 많은 부자들의 인사이트를 쉽게 접할 수 있었다. 그리고 그들의 각자 다른 표현에서 공통점을 뽑아낼 수 있었고 나의 개념으로 바꾸는 과정도 경험했다. 일론 머스크, 로버트 기요사키, 그랜트 카돈, 패트릭 벳 데이비드 같은 사람들의 얘기를 들으면서 이 사람들의 묘한 공통된 얘기, 방법론들을 하나씩 내 삶에 적용했다. 그중에는 만족도가 굉장히 높은 경우도 있어서 그런 경우는 별도로 회고에 정리했다. 기회가 된다면 나만의 방법으로 바뀐 여러 방법론을 글로 정리해 보고 싶다. 책이 중요하다는 것을 알아라.또한 책을 다른 해보다 다채롭게 읽었다. 나는 원래 엔지니어링 관련된 도서만 좋아하고 읽어왔는데, 올해는 여러 책을 읽었다. 새로운 것들을 시도하면서 배우는 것이 많아지니까 새로운 지식이 새로운 기회를 만들어준다는 것을 경험하게 됐다. 예를 들어 Adsense를 공부하며 광고 관련된 비즈니스를 찾아보니 Affiliate라는 것을 알게 됐고 그걸 활용한 다음 프로젝트를 기획할 수 있었다. 책은 이러한 과정을 간접적인 경험으로 만들어준다. 나는 엔지니어 역할을 하며 살아왔는데 엔지니어링 말고 다른 거 잘 아는 게 있는지 물어보면 하나라도 자신 있게 말할 수 있는 게 없다. 즉, 다른 영역에서 새로운 기회를 알아차릴 수 있는 가능성이 꽤 낮다. 책은 간접적인 경험을 제공하며 이 문제를 조금이나마 해결해 준다. 또한 내가 생각할 때 쓸 수 있는 도구를 제공해 준다. 개발자가 아닌 영역에서 살아남는 방법 중 쉽고 좋은 방법이 책이라는 것을 느끼고 있고 여러 책을 읽어보고 있다. 이동진 작가님이 어떤 것이든 재미를 느끼기까진 어느 정도의 노력이 필요하다고 하셨다. 예를 들어 게임을 잘 못하면 게임에 재미를 느끼기 어렵다. 게임을 잘해지기까지 어느 정도의 노력이 필요한 것이다. 책도 비슷한 느낌이었다. 처음엔 잘 안 읽어본 영역의 책이 그렇게 재밌진 않았는데, 최근에는 점점 재밌다. 그리고 오가는 길에 읽을 수 있는 밀리의서재가 도움을 많이 준다. 읽었던 책들도 하나씩 정리해 두고 싶다. 2024년의 목표는 무엇인가?아직 구체적인 목표는 작성하지 않았다. 일단 대략적인 내용이라도 적자면 서울로 집을 옮길 예정이다. 사람을 만나고 무언가를 하기에 일산이 좋은 곳은 아니다. 서울에서 지낼 수 있는 돈을 마련하려면 최소 월 100은 있어야 할 것 같다. 그를 위해서 연초에 강의를 하나 더 만들고 싶다. 그리고 꿈이 큰 사람, 몰입을 아주 잘하는 똑똑한 사람을 찾고 싶다. ‘적당한 돈이야 벌려면 얼마든 벌 수 있다. 나는 훨씬 더 큰 꿈이 있다.’라고 생각하며 작은 문제가 아니라 큰 문제 해결에 집중하고 싶어 하는 사람을 찾고 싶다. 그리고 가능성 있는 100개의 아이템을 돌면서 PMF를 찾아가는 과정을 함께할 수 있으면 좋을 것 같다. 지난해도 마찬가지로 좋은 사람을 항상 만나고 싶어 했던 것 같다. 하지만 약간 추상적이게도 다른 많은 사람을 만나는 정도? 사람을 적게 만난 건 아니었지만 훌륭한 팀원을 만들고 싶어 했던 측면에서 내 삶에 추가된 사람은 없었다. 시간이 지나서 점점 더 만나고 싶은 사람의 종류가 구체적으로 되어가는 것 같아서 좋다. 이번엔 꼭 내 삶에 추가될 수 있을 정도로 가까워지는 사람이 생기면 좋겠다.","link":"/posts/logs/20240101/"},{"title":"2022년, 개발 4년 차 회고","text":"개발을 시작한 지 4년, 블로그를 시작하고 4번째 회고가 되었다. 올해는 학생이 아닌 상태로 상태 전환이 발생하기도 했고, 어느 순간부터 마음속에 있던 삶의 마일스톤 중 하나를 해결한 것처럼 느껴지기도 한다. 지난 21년 회고를 거의 1분기가 끝나고 썼던 터라 이번 회고가 좀 짧게 느껴지긴 하지만 아무튼 그 이후(혹은 일부 포함해서) 어떤 일들이 있었나 정리해보려고 한다. 회사에서 하는 일지난 회고를 쓴 4월부터 회사에 다니고 있고 플랫폼 엔지니어링 팀에 있다. 이전 인턴을 했던 팀도 그 당시에는 플랫폼 조직이었는데, 그때 영향을 받아 플랫폼 조직에서 일하고 싶어 했다. 시간은 좀 흘렀지만, 올해 취업을 준비했던 사람으로서 플랫폼 조직이 유난히 경력을 보는 경향이 있다고 느꼈다. 회사마다 이유는 다르겠지만 요구사항들이 일반적으로 쉽지도 않고 만들어지는 프로젝트들이 영향을 주는 범위가 넓다 보니 안전하게 개발할 수 있는 능력을 갖추고 있어야 하기 때문이라고 생각한다. 관련된 경력이 그렇게 많은 편은 아니었지만, 주니어로서 입사할 수 있었다. 플랫폼이라는 말은 조금 모호하게 들릴 수 있다. 실제로 업무 범위도 모호하다. 보통 사내의 회색 영역이라고 불리는 어떠한 팀에도 속하기 애매한 영역의 개발을 맡는다고 생각하는데, 이런 정의조차 모호하다. 그래서 실제로 JD를 살펴보면 회사마다 이를 정의하는 방법이 다양하다. 내가 취업을 준비하던 시기에 생각하던 플랫폼을 조금 더 구체적으로 서술하자면 서비스들이 공통으로 사용하는 도구들, 예를 들어서 Push 알림(메시징), 지리 정보, 인증, 미디어 서비스를 만드는 것으로 생각했다. 현재 회사의 플랫폼 조직은 그런 역할은 아니고 인프라 레벨의 공통적인 문제를 해결하는 조직이다. 예를 들어 멀티 클러스터 안에서 서비스 디스커버리 서비스, 로깅 파이프라인, 배포 도구 같은 걸 만든다. 이 팀에 처음 왔을 때는 뭔가 SRE 조직 같다는 생각도 들었다. CTO님이 플랫폼 조직에서 서비스 영역의 개발을 하는 것이 이상하다고 느끼셨다고 들었다. 해외에서 개발하다가 오신 분들이 공통으로 이런 얘기를 하시는 걸 보면, “플랫폼”이라는 팀이 해외에서는 뭔가 인프라 엔지니어링을 하는 그런 느낌이 있나 보다. 이전 인턴을 하던 회사에서도 비슷한 이유로 “플랫폼”이라는 이름을 버리고 “서비스 코어”? 이러한 이름으로 변경된 것으로 알고 있다. 우리팀 역시 서비스 조직에서 직접 가져다 쓰는 컴포넌트를 개발하는 조직으로 “서비스 컴포넌트” 조직을 새로 만들었다. 생각하던 업무 범위는 아니었지만, 오히려 재밌었다. 지금 회사는 규모상 확장을 준비하고 있는 단계라고 느껴진다. 글로벌 서비스도 준비하고 리텐션을 유지하기 위한 여러 전략도 준비 중인 것 같다. 취업을 준비하던 시기에는 이 회사가 구체적으로 어떻게 비즈니스 발전이 있을 예정인지까지는 몰랐지만, 마이크로서비스가 도입되고 있다는 것까지는 알 수 있었다. 이 단계에서 플랫폼 팀의 업무인 쿠버네티스 컴포넌트를 개발한다든지, 인프라에 도입되는 것들을 개발하는 것 등은 경험하기 쉽지 않으리라 생각해서 지금 회사에 오게 되었다. 지금 업무에 대한 만족도는 꽤 높다. 사이드 프로젝트 언제나 그렇듯 사이드 프로젝트를 시작했다. 이번 사이드 프로젝트에서는 쿠버네티스에 어느 정도 익숙해지기 쿠버네티스를 사용해 서비스를 배포할 목적을 가지고 있었다. 그렇게 크지 않은 서비스지만 억지로 프로젝트를 나눠서 개발했다. 서비스는 거리 기반 공동 구매 플랫폼이다. 실제 시작할 때는 굉장히 유망할 것 같은 생각이 많았는데 바로 배민, 쿠팡, 당근마켓 등이 비슷한 컨셉의 서비스를 공개하고 있다. 공부 목적으로 개발하는 거지만 뭔가 아쉬운 느낌이 들었다. 이번 사이드 프로젝트는 꽤 많은 사람하고 시작했는데, 약 6명이 같이 시작했다. 클라이언트 개발을 도와주는 팀원 두 명과 백엔드 두 명, 디자인해주는 친구 한 명과 나까지 총 6명이었는데, 지금은 백엔드 해주시는 분이 한 분 나가시고 5명 체제로 여전히 진행 중이다. 사이드 프로젝트를 팀원들과 하는 이유는 혼자 하는 사이드 프로젝트를 지속하지 못할 것 같았기 때문인데, 지금까지 내가 하는 패턴을 보니 지속할 수 있었을 것 같다. 프로젝트 주제가 재밌어서 그런지 뭔가 계속하게 된다. 지난해 사이드 프로젝트를 할 때 잘 모르는 분들과 시작하지는 말아야겠다고 생각해서 모두 지인분들과 같이 했다. 확실히 프로젝트 진행이 원활하기도 하고 프로젝트 회의나 프로젝트를 같이 진행하는 날이 되게 재밌다. 혼자 진행하는 것과 팀으로 진행하는 것은 뭔가 다른 방면으로 장단점이 있긴 한 것 같다. 혼자서 진행했다면 이미 프로젝트를 완성했을 것 같다. 하지만 사이드 프로젝트를 하는 목적과 다른 여러 가지를 같이 해줘야 하기도 하고 비교적 지루하다. 팀으로 하는 것은 재밌으나 느리다. 우리 모두 이 작업에 몰두하고 있다면 얘기가 다르지만 서로 투자할 수 있는 시간도 다르고 서로에 의해 블로킹 되는 경향도 있기 때문에 점점 느려지는 것 같다. 스터디이번에도 스터디를 많이 시작했다. 기술 세션을 일정 기간 진행했고 데이터 중심 애플리케이션 설계, Rust In Actions, Database Internals 책을 읽었다. 4월 5월쯤부터 데이터 중심 애플리케이션 설계 스터디를 시작했다. 다른 한 분과 같이 했는데, 각자 챕터를 나눠서 내용을 정리하고 발표했다. 해당 챕터는 각자 읽어오기는 하지만 발표를 맡은 챕터는 심도있게 이해하고 정리해가는 스터디였다. 책 내용이 워낙 어려워서 각자 맡은 챕터에 집중해서 공부하는 것과 다른 사람이 꼼꼼히 이해한 다음 이해하기 쉽게 설명해주는 게 생각보다 도움이 많이 됐다. 책 내용도 데이터베이스의 분산 환경의 사용과 관련된 내용이 많이 있었는데, 재밌어하는 주제라 스터디가 더 재밌었던 것 같다. 기술 세션은 각자 2, 3주 정도 텀을 가지고 기술 학습을 해서 세션을 준비해오는 구조였는데, 이런 비슷한 스터디를 몇 번 했던 적이 있어서 블로그로 쓰려고 했던 내용이나 이전 세션에 했던 내용 등을 다듬어서 발표했다. 세션을 많은 사람과 진행한 것이 아니라서 타인으로부터 매우 많은 정보를 얻을 수는 없었지만, 이 세션을 준비하면서 특정 주제에 대해 또 깊게 공부해볼 기회가 되어서 개인적으로 만족스러웠다. 기술 세션을 진행하다가 Database Internals 책을 읽는 스터디로 바뀌었다. 이 책도 데이터 중심 애플리케이션 설계 책하고 아주 비슷한 내용이 많이 섞여 있어서 뭔가 복습이 되는 것도 있고 조금 더 깊게 공부하는 것도 있다. 특정 DB 애플리케이션에 종속적이지 않은 DB 내부적인 이야기를 학습하는 것이라 재밌게 읽고 있다. 현재도 진행 중이지만, 이후 나올 원격 근무로 인해 잠시 정지 상태이다. Rust 스터디는 Rust In Action을 공부하는 것을 목표로 했지만, Rust를 처음 만져보는 입장에서는 Rust가 공식적으로 제공해주는 The Rust Programming Language 책을 읽는 게 좋다고 판단해서 이 책을 읽고 있다. 스터디에서는 일단 문법을 공부하는 시간을 한 달 반 정도 진행했는데, 지금까지 배웠던 언어 중에서 Rust가 제일 공부할 게 많은 것 같다고 느낀다. 이전에 Kotlin을 공부할 때 이렇게 느꼈는데 코틀린은 뭔가 Helper 같은 도구가 많은 것처럼 느껴졌는데, Rust는 그냥 언어 자체가 조금 복잡한 듯한 인상을 받았다. Rust는 내가 좋아하는 특징과 싫어하는 특징을 모두 갖추고 있다. Go를 좋아하는 나는 언어가 복잡도가 높다는 점이 굉장히 불호인데 강한 타입과 안전한 프로그램을 위한 특징들은 굉장히 마음에 든다. 복잡하다는 건 어느 정도 학습으로 커버가 가능할 것이라 믿어서 지금까지 스터디는 이어지고 있다. 대충 문법들은 마쳤는데, Rust In Action 책에서는 시스템 프로그래밍을 하는 내용이 반 이상이라 그 부분을 시작했다. 4월에 썼던 회고에서도 어떻게 기술을 바라봐야 하는가, 어떻게 학습해야 하는가에 대한 기준이 생겼다고 했는데 이 기준은 여전히 유지되고 있다. 깊게 공부하는 습관을 들여서 여러 스터디와 기술 학습에 적용하면서 이전까지 가지고 있던 마일스톤을 달성한 것 같다. 마일스톤은 “기술적 목마름”이라고 내가 이전부터 표현하던 것인데, 도달하고 싶은 기술적 능력치가 어느 정도 있고 이를 달성하는 것을 의미했다. 이 마일스톤을 달성했다는 것은 그럼 목표로 하던 “어느 정도의 기술적 능력”을 갖춘 것일까? 그렇지는 않다. 여전히 부족함을 많이 느끼기도 하고 공부할 것 자체가 너무 많다. 하지만 이렇게 공부하면 “기술적 목마름”을 해결하는 방향으로 나아간다는 것을 느꼈다. 그전에는 “기술적 목마름”이 경험에 의해 채워질 수 있다고 생각했는데, 내가 원하던 것은 이런 학술적인 접근을 통해 이룰 수 있는 경지였던 것 같다. 주변 시니어분과 경력이 기술적 측면의 목표를 달성하기에 중요한 것일까에 대해 얘기를 나눈 적이 있는데, 시니어분은 플랫폼 개발 말고 트래픽도 관리해보고 장애도 맞아보는 것이 많이 가르침을 주기 때문에 경력이 중요한 것 같다고 하셨다. 정확히 맞는 말이라고 생각하지만, 다시 생각해도 내가 원하는 기술적 목표 달성은 학술적인 접근으로 달성하는 것 같다. 경험으로 얻을 수 있는 능력치와 깊은 학습으로 얻을 수 있는 능력치가 겹치는 부분도 분명히 있으나 각자가 강화 해주는 영역이 다른 것 같이 느껴진다. 이게 항상 머릿속으로 생각하던 거라 글로 썼을 때 온전히 전달이 안 될 수도 있다. 무슨 얘기를 하는 걸까 싶을 수도 있지만… 결론은 올해 나는 아직 내가 정해둔 기술적 경지에 도달한 것은 아니지만 도달하는 방법을 알았다고 느꼈다. 그럼, 마일스톤을 달성한 것이 맞느냐? 아직 기술적 목표에 도달한 것은 아니지 않는가? 하지만 이 과정은 내가 개발하는 동안 무한히 지속되어야 한다. 이를 달성하겠다는 말은 애초에 불가능하다. 방향을 정확히 맞춰놓고 나아간다는 것으로 충분히 마일스톤 달성에 체크를 해도 좋을 것 같다. 네이버 부스트캠프 코드 리뷰어이번 해 하반기 조금 넘어서 한 달 조금 넘는 기간 동안 네이버 부스트 캠프 코드 리뷰어로써 활동했다. 경력을 기록하는 단계가 있어서 지금까지 돈을 받으면서 일한 기간이 얼마나 되는지 정리하는 시간을 가졌다. 그 당시 기준으로 한 2년 몇 개월이 나왔던 것 같은데, 지금은 거의 3년이 되어갈 것 같다. 연차가 중요한 건 아니지만 평소에 스스로의 처우에 대해 생각할 때 항상 1년 중간쯤 했나? 라는 기준으로 생각했는데, 앞으로는 정확히 기간에 대해 인지해야할 것 같다. 아무튼 다행스럽게 네이버 부스트 캠프에서 코드 리뷰어로 선정되어 학생들의 코드를 리뷰했다. 리뷰 내용은 사실 그렇게 대단하지 않기도 하고 도움이 얼마나 됐는지 잘 모르겠지만 리뷰를 받는 학생분들과 따로 티타임 비슷한 시간을 격주에 한 번 정도 다양한 사람들과 진행했는데, 이때 얘기했던 것들이 내가 학생이었다면 더 들어보고 싶었던 내용들이 아니었을까 싶었다. 개인적으로 보람차기도 했고 재밌던 시간을 보낸 것 같아 다음에도 이러한 기회가 생기면 다시 해보고 싶다. Working From Anywhere지금 일하고 있는 회사는 6개월에 한 달은 출근을 하지 않고 어디서든 일을 할 수 있는 제도가 있다. 이를 Working From Anywhere(WFA)라고 부른다. 원래 여행을 그렇게 자주 하거나 즐기는 편은 아닌데, WFA를 하는 김에 오래전부터 해보고 싶던 해외에서 디지털 노마드를 준비했다. 그리고 이왕 비행깃값 쓰는 김에 12월과 1월 WFA를 붙여서 갔다 오기로 했다. 해외에 가서 WFA를 한다고 일을 제대로 못 하는 것으로 평가가 된다면 앞으로 해외로 WFA를 하기 눈치가 보인다든지, 조금 과장되어 아예 제도가 폐지되어버릴 수도 있으므로 국내에서 일하는 것보다 더 잘해야겠다고 생각했다. 그래서 지낼 국가를 선택할 때 기준은 다음과 같았다. 업무를 할 수 있는 공간 (무선 인터넷, 책상, 혼자 쓰는 공간 등) 업무를 수행한 다음에는 어느 정도 여행을 할 수 있는 시간 (적절한 시차) 한 달 살이를 하기 적절한 생활 물가, 좋은 날씨 이렇게 고르고 나니까 그리스와 포르투갈 정도로 선택이 되었다. 이 계획을 짤 때까지만 해도 서머타임이 적용되고 있어서 포르투갈도 새벽에 일어나서 일할 수 있다고 생각하는 범주에 들어왔다. 하지만 지금은 잘 모르겠다. 이제 곧 포르투로 이동하는데 해봐야 할 것 같다. 그래서 현재는 그리스에 있다. 1월 초까지 그리스에 있고 그 이후부터 2월 초까지는 포르투갈에 있는다. 아테네에서 한 달을 지내고 있고 주말에는 여기저기 근교나 섬에 가보고 있다. 아테네에 도착한 초반에는 너무 바쁘게 지내서(엄청난 스터디로 인해) 관광 시간을 낼 수가 없었다. 그래서 지금은 Rust만 진행하고 있고, Database Internals 스터디와 알고리즘은 멈췄다. 개인적으로 WFA는 굉장히 만족스럽다. 바쁘게 지내던 시점에서 조금 벗어나서 혼자 여행하니까 현재 상황에 대해 생각해볼 시간이 조금 늘어난 것 같다. 그리고 여유롭게 여행하기도 하고 쉬기도 하고 하면서 컨디션을 적절히 조절하면서 여행이 가능하다. 한국 시간대로 일을 하다 보니 일찍 잠을 자야만 하는 단점도 있는데, 그리스 시차 정도는 여행과 일 모두 가능한 정도로 느껴진다. 여유롭게 한국에서 여유롭게 살았어도 얻는 장점 아닌가? 라고 생각이 들 수도 있으나… 한국에 있으면 절대 여유롭게 살 생각이 안 들었을 것 같다. 하지만 비용은 만만치가 않다. 새벽에 일어나고 업무 공간이 있어야 한다는 것 때문에 에어비엔비에서 집을 30일씩 빌려서 쓰는데, 이 값이 가장 큰 것 같고 외식을 주로 하다 보니 유럽 외식 비용을 감당해야 한다. 이번 해의 목표아테네에 있으면서 실존적인 고민을 많이 하게 되었는데, 마일스톤을 이룬 것과도 연관이 된다. 현재 고민이 많던 기술적 목마름을 어느 정도 해소했는데, 이걸 계속 지속하는 것이 10년 뒤의 내 모습이라면 만족할까? 혹은 현재 수행하고 있는 일을 10년 정도 한다고 하면 내가 원하는 모습이 되어있을까? 하는 질문을 많이 하게 되었다. 일단 질문을 많이 했다는 것 자체가 그렇지 않을 것 같다는 생각 때문인데, 마땅히 무엇을 해야겠다는 결론은 당연히 안 나온다. 고민 끝에 원초적으로 다음으로 이루고 싶은 목표가 “훌륭한 팀을 만들어보고 싶다”라는 결론에 이르렀다. 논리적인 이유는 딱히 없고 내가 “왜 예전부터 어떤 조직을 만들어서 도전해보길 원할까?”라는 생각을 했는데 그 과정 중 어떤 포인트에 가장 매력을 느끼는 건지 고민해보다가 나온 결론이다. 관련된 얘기를 지인과 했는데, 똑똑한 사람들과 모여서 문제를 해결하는 과정을 겪는 건 원초적으로 재미를 느끼는 요소일 수밖에 없다는 얘기를 했다. 생각해보면 원시 부족을 이루던 시기부터 훌륭한 팀원을 모아서 부족을 유지하는 그 욕구가 DNA로 박혀있을 것 같다. 내가 생각한 나의 원동력이 굉장히 원초적인 욕구로부터 왔다고 생각하니 실존적 고민의 결론이 잘 도출된 것 같은 생각이 든다. 그래서 목표는 무엇인가? 첫 번째로는 회사든 어디서든 새로운 사람들을 많이 만나볼까 한다. 보통 이런 성격이 아니라서 사람들을 사귀는 것을 목표로 삼은 적은 없는데 이번 해에는 한 번 노력해보려고 한다. 두 번째는 개발 말고 다른 것을 개인적으로 해볼까 한다. 뭘 해볼지는 안 정했는데, 다른 영역에 대한 경험이 좀 필요한 것 같다는 생각을 여행하러 와서 많이 하고 있다.","link":"/posts/logs/20230105/"},{"title":"2024년, 창업 1년 차 회고","text":"2019년 1월 1일 개발 공부를 시작하며 이 블로그를 쓰기 시작했고, 벌써 6년째이지만 이제는 창업가와 개발자 사이에 어중간한 위치에 있는 사람으로서 ‘창업 1년 차’라는 이름으로 회고를 처음 써본다. 제목을 이렇게 정하니까 굉장히 색다른 느낌이다. 작년의 나는 꿈을 확정하고 꿈을 이루기까지 나에게 방해가 되는 요소를 제거하는 한 해를 보냈고 이번 해는 실제로 창업에 뛰어들었던 첫 번째 해였다. 창업하면 일반적으로 정말 대부분의 영역에서 불편함을 느끼는 상태가 되어 엄청난 성장을 만들어낼 수 있는 것 같다. 그래서 이번 해는 배운 게 너무 많아서 추리는 과정이 더 오래 걸렸다. 추려낸 것들을 기록하지 못해 아쉬울 정도로 재밌는 한 해를 보냈다. 창업을 시작하고 블로그에 한 차례도 글을 남기지 않았다는 걸 후회하고 있다. 사실 중간중간 쓸 일이 있었지만 23년 회고 글 다음이 24년 회고 글인 모습도 궁금했다. 2024년의 목표24년 1월 1일 역시 새로운 1년 목표를 만들었는데, 다음과 같다. 서울에 집을 구한다. (출가!) 창업을 유지할 수 있는 돈이 들어온다. 창업 팀을 꾸린다. PMF를 찾는다. 건강해진다. 책을 15권 이상 읽는다 (AI, 비즈니스, 인간에 대한 책 각각 5권씩!) 지금 상황을 요약하자면 서울에서 지내고 있고, 창업을 유지할 수 있는 돈이 들어오는 상태다. 책도 15권 이상 읽었다. 하지만 건강해지는 건 실패한 것 같다. 오히려 퇴보한 것 같다. 그리고 퇴보의 대부분은 4Q에 발생했다. 이번 회고는 아직 말하지 않은 두 가지에 초점이 있다. 창업 팀 찾기와 PMF를 찾기 위한 여정이 이번 연도에 주된 목표였다. 책도 너무 재밌게 읽어서 기록하고 싶은 게 너무 많지만, 이 부분은 별도로 기록 해보려고 한다. 2024년의 목차퇴사를 하며, 그리고 창업을 위한 여러 준비를 하던 초반 시기를 지나 프로덕트를 만들고 공동 창업자를 찾기 위해 엄청나게 많은 사람들을 만나던 중반 시기, 그리고 앤틀러(Antler)라는 글로벌 VC가 운영하는 스타트업 제네레이션 프로그램(이하 앤틀러)에 참여하는 후반기로 나눠진다. 아이템 찾기와 사람 찾기 모두 이 구분으로 크게 나눠진다. 준비 창업 완료 올해 1Q는 창업 준비를 하며 보냈다. 1월 2일에 팀 리더님에게 퇴사 의사를 전달했고 오늘의집 사람들과 퇴사 계획을 만들기 시작했다. 나는 언제 퇴사할 것이며 그 기간 월루하지 않기 위해 어떤 일을 할 것인지, 인수인계는 어떻게 할 것인지 등을 얘기했다. 퇴사 과정 자체가 깔끔했다고 말하긴 어렵지만 지저분하지도 않았다. 앞으로 현금이 중요해지는 나의 입장에서 회사의 마지막 정을 모두 챙기지 않으면 나중에 후회할 것으로 생각했다. 하지만 연봉 협상이라든지 스톡옵션을 채울 정도로 오래 기다릴 수는 없었다. 그래서 2월 말 퇴사가 결정되었다. 그 사이 기간 나는 (회사 일 제외하고) AI를 공부하고 사람들을 슬슬 만나기 시작했다. AI 기술을 알아보자올해 초에 지금 시점, 다시 읽는 W라는 아티클을 읽었다. 23년에 SORA 데모를 보면서 다음 W가 AI임을 확신한 내가 떠올랐다. 그러면서 AI에 관한 공부를 시작했다. AI를 위한 기본적인 수학과 기술적 기반이 되는 개념을 공부했고 모델을 사용하는 입장으로서 어떻게 모델을 쓰는지에 관한 공부도 많이 했다. 기본과 기술 자체에 관한 공부를 할 때 흥미로운 점도 있었지만 이걸 내가 직접 만들 필요는 없겠다는 생각을 많이 했다. 마치 내가 컴퓨터의 컴포넌트와 프로토콜들이 어떻게 네트워크를 만들어내는지는 이해하고 있지만 직접 만들 필요는 없는 느낌으로. 나는 사용자의 관점에서 어떻게 하면 모델을 더 잘 쓸 수 있는지에 집중해 공부하고 실습했다. 자연스럽게 Prompt Engineering, LLMOps 등을 많이 공부하게 됐다. 다행히 이때 내린 나의 판단은 꽤 훌륭했던 것 같다. 지금까지도 적절하게 공부한 기본 개념 덕분에 모델을 사용하는 방법에 대해서도 더 잘 이해할 수 있는 것 같고 무엇보다 그때 많이 공부했던 응용 레벨의 지식을 지금까지도 잘 사용하고 있다. 특히 중간에 만들었던 프로덕트가 Lovable, Marblism 같은 Full-stack AI를 만드는 것이었는데, 이때 AI 사용을 굉장히 빡세게 하면서 단순한 것보다는 조금 더 많은 걸 공부하고 사용해 봤던 것 같다. AI 비즈니스를 알아보자기술 공부를 하면서 동시에 AI를 사용한 비즈니스를 같이 공부했다. 케이스 스터디에 가까웠는데 AI 리서치 클럽이라는 활동을 했다. VC 호스트가 AI와 관련된 비즈니스에 대한 인사이트를 나눠주고 읽기 좋은 아티클을 전달해 주면 사람들이 이를 읽고 공부하는 형식의 스터디였다. ‘지금 시점, 다시 읽는 W’라는 아티클도 여기서 소개를 받았고 이것뿐만 아니라 재밌는 아티클을 많이 소개받아서 소장하고 있다. 사실 이걸 처음 시작할 때는 AI의 흐름에 타고 싶은 창업가를 만나보고 싶은 생각이었는데 아쉽게도 여기서 더 연락을 이어간 사람은 없긴 하다. 하지만 활동 자체가 꽤 마음에 들었다. 지금 시점, 다시 읽는 W한 외과 의사는 지인에게 특별한 강연에 초대를 받았다. 바쁜 시간을 쪼개서 갔더니 강연자 W는 WWW를 칠판에 적고 ‘인터넷이 세상을 지배한다’는 주제로 강연했고 외과 의사는 미친 소리라며 강연에 초대한 지인을 구박했다. 반면 이를 같이 들은 친구는 W를 찾아가 더 알려달라며 따라다니고 인생을 건 배팅(사업)을 시작했다. 그 사업은 2조가 넘는 사업으로 성장했으며, 강연자 W는 이재웅 대표님이라고 한다. 외과 의사는 ‘같은 강연을 들었는데 왜 본인은 미친 소리로 듣고, 다른 친구는 인생을 건 배팅을 할 수 있었을까?’에 대해 고민했고 제레미 러프킨으로부터 답을 얻었다고 한다. 지금의 문명에 ‘인류가 기여했다’라고 말하곤 하지만 사실은 인간의 역사는 1%가 만들어낸 역사라고 말한다. 1%는 새로운 문명을 개척하고 99%의 인류는 “세상 참 좋아졌다.”, “기술 발전이 참 빠르다.”라고 말하며 세상을 따라가는 ‘잉여 인간’이라고 말한다.1% 중에서도 크게 둘로 나눠지는데, 0.1%는 새로운 영역을 만들어내는 사람이고 0.9%는 통찰력을 가지고 그 세상으로 뛰어든다. 이야기에서 이재웅 대표님은 0.1%, 친구는 0.9%, 외과 의사는 99%이고 포드가 0.1%, 록펠러는 0.9%, 그 외는 99%인 것이다.인터넷은 이미 성공으로 판명 난 인류 문명 혁신이다. 따라서 우리가 이 글을 읽더라도 적어도 0.9%가 되는 것이 그렇게 어렵지 않은 것처럼 느껴진다. 하지만 우리는 아직 불확실한 현재 상황에서 돈키호테와 진짜를 구분할 수 있어야 한다. 회사를 퇴사한 시점부터 크게 2개의 Phase로 나눠진다. 앤틀러 참여 전(Before Antler)과 앤틀러 참여 후(After Antler)이다. 공동 창업자를 찾는 과정에 대한 회고와 PMF를 찾는 과정에 대한 회고 모두 적용되는 구분법이다. 공동 창업자를 찾기 위한 여정창업을 하면서 1월 1일부터 사람을 만나기 시작했으나 본격적으로 공동 창업자를 찾을 목적으로 사람들을 탐색하는 건 3월 초부터인 것 같다. 내가 사람을 만났던 채널은 앤틀러를 제외하면 지인의 소개, LinkedIn에서 직접 연락드리기, Y Combinator의 Co-Founder 매칭 플랫폼이었다. 앤틀러 외에 거의 50분 가까이 뵌 것 같고 앤틀러에서는 80명 정도 되는 인원을 만났다. 당연히 너무 인상 깊었던 사람도 있고 와중에 많이 친해졌던 사람도 있다. 공동 창업자를 찾다 보니 자신의 목표를 이루기 위해 컴포트존에서 벗어나 자신의 목표를 이루기 위해 노력하는 사람들을 많이 만나게 됐다. 많은 분으로부터 자신들의 여정에 대해서도 얘기해주고 비슷한 상황을 바라보는 신선한 생각을 얻고 재밌게 얘기해 볼 수 있었다. 지금 회고 쓰는 시점에서 생각해 보니 앞으로 이런 사람들과 더 많이 만나게 될 거고 그런 사람들하고 일하게 될 것으로 생각하면 희열감을 느낀다. 어떤 사람과 같이 오랜 시간을 보낼 수 있나요?그러나 기쁜 건 기쁜 거고 공동 창업자를 찾는 여정 자체는 너무나 험난했다. 아마 작년에 퇴사 준비를 하면서 ‘어떤 사람과 함께 일해야 할까?’에 대한 생각을 조금씩 했던 것 같다. 그리고 1월 1일부터 사람들을 조금씩 만나보면서 이 생각을 구체적으로 하기 시작했다. 과거 나는 공동 창업자의 자리를 내려놓고 퇴사를 결정한 적이 있다. 나의 이러한 선택은 어떻게 만들어진 걸까? 협상 자체는 사소한 문구들까지 검토하면서 얘기를 나눴지만, 사실은 큰 원인은 하나 있었던 것 같다. 나는 창업 말고도 하고 싶은 게 너무 많았다. 엄청나게 잘하는 엔지니어가 되고 싶다는 생각도 아주 컸고, 막연하게 미국에서 일하고 싶다는 생각도 했다. 당시 나에게 공동 창업자 자리는 이 둘을 만족시키는 가장 빠르고 확실한 길이 아니었다. 그래서 계약서상 오랜 기간을 헌신해야 하는 역할에 큰 고민을 했던 것 같다. 다른 말로 하자면 나는 공동 창업자로서 적합한 인물은 아니었다. 투자 직전에 이런 결정을 내리고 나오게 된 것은 어찌 보면 팀에게 다행일 수도 있지만 창업을 하는 이 시점에서 다시 생각해 보면 대표님이 짧게나마 고통을 느끼셨으리라는 짐작도 한다. 어찌 됐든 나는 ‘내가 지금보다 더 성장했을 때, 또 이러한 좋은 기회가 올 것이다’라고 생각하고 나의 발목을 잡던 다른 목표를 해치우러 떠났다. 지금은 그런 것들을 모조리 해치웠고 남은 인생의 목표가 창업을 통해 달성할 것들만 남았다. 나는 훌륭한 사람과 최대한 오래 붙어 회사를 만들어내고 싶다. 그러려면 적어도 나는 ‘19시즌 김창회’ 같은 사람은 걸러낼 수 있어야 한다. 그래서 가장 중요하게 생각했던 건 창업의 동기였다. 왜 창업을 하고 싶어 하는가? 꽤 오랜 시간을 이 여정에 걸어도 괜찮을 만큼, 창업 말고는 이룰 수 없는 어떤 인생의 최종 목표가 있는 건가를 열심히 들어보려고 했다. 그리고 그 당시 ‘어떻게 하다가 팀이 깨질 수 있을까?’를 고민하면서 몇 가지를 만들어봤다. 나만큼 꿈이 큰 사람: 아마도 오랜 기간 같이 할 사람이라는 기준이 있기 때문에 필요하다고 생각했던 것 같다. 꿈의 크기는 목표하는 비즈니스의 크기라고 생각해 봐도 좋을 것 같다. 우여곡절 끝에 PMF를 찾더라도 정말 끝까지 갔을 때 캡이 제한적이라고 확실하게 판단되면 과감하게 다른 걸 해보자고 말해줬으면 좋겠다. 도메인에 특정되지 않는 사람: 어떤 특정 도메인에 엮여있다면 그 영역에서 문제를 찾기에 실패했을 때 팀이 와해 되지 않을까 하는 걱정이 있다. 일이 즐거운 사람: 일을 많이 하는 것 자체에 스트레스를 느끼지 않고 즐겁게 일했으면 좋겠다. 배우는 것에 자신 있는 사람: 생소한 영역에서도 ‘배우면 되지’라는 마인드가 필요하고 순수 지능과도 관련이 있는 것 같다. 여러 도메인을 시도할 수 있다는 것도 장점이 된다. 앤틀러 전까지 대충 4~50명을 만나면서 얘기하면서 느낀 점은 위 네 가지는 몇 번 얘기해 본다고 짐작하기 어렵다는 것이다. 일 즐겁게 하기 능력 평가, 배우기 자신감 자격시험 같은 걸 만들 수 없는 아주 개인적인 기준표를 가지고 타인과 얘기하므로 서로 같은 단어를 쓰더라도 서로 다른 모습을 그리는 경우가 많다. 그래서 창업 동기까지만 열심히 물어보고 나머지는 얘기는 해보지만, 함께 뭔가 해보면서 알아가야겠다고 생각했다. Before Antler(BA)위에서 말했던 것처럼 앤틀러 시작 전까지 LinkedIn, YC Co-Founder Matching을 애용해다. 그때 만난 분들과 같이 뭔가 해보지는 못했다. 왜 그랬을까? 창업을 하는 사람 중에서 공동 창업자를 찾는 것에만 몰두하며 다른 것을 손 놓고 있는 사람은 거의 없다. 다들 들고 있는 아이템 한두 개는 있고 거기에 관심 있는 사람을 찾게 된다. 근데 서로 창업을 하려고 하는 둘이 방금 만나서 지금 내가 하고 있는 아이템을 같이 해보지 않겠냐고 했을 때 상대가 그에 동의하며 시작되는 케이스가 얼마나 있을까 싶다. 그래서 사실 내가 지금 하고 있던 아이템을 멈추고라도 같이 일을 해보는 것에 초점을 맞춰봤더라면 어땠을까? 반면 저 기준들이 생각보다 까다로운 것일 수도 있겠다는 생각도 든다. 생각보다 까다로워서 지금 하는 걸 멈추고 ‘이 사람과 뭔가 더 해봐야겠어’라는 생각이 잘 들지 않았던 것 같다. 특히 창업 동기가 창업이 아니라면 이룰 수 없는 무언가가 있으면서도 어떤 특정 도메인에 묶이지 않은 사람은 양립이 어려운 조건일 수도 있을 것 같다. 나 역시 에너지라는 도메인에 최종적으로 접근하고 싶다는 목표가 있다. 다만 이 목표는 인생의 오랜 기간을 거쳐 점진적으로 도달할 목표라고 생각하다 보니 지금 특별히 이 도메인에 묶이지 않게 된 것이다. 이런 경우가 아니라면 어떤 특별한 도메인의 문제를 해결하고 싶은 것도 아니면서 엄청 오랜 시간을 여기에 쏟아야겠다고 생각하는 사람을 보기 쉽지 않다. 앤틀러를 했다는 것은 결국 이 기간에 공동 창업자를 찾지 못했다는 것을 의미하지만 정말 감사한 인연들을 많이 만났다. 당장 창업에 뜻이 있는 것은 아니지만 프로덕트를 만들 때 직접적으로 도움을 주셨던 분들도 있고, 심지어 여름 동안 집의 방 한 칸을 내어주신 분도 만났다. 정말 뭐라도 행동해야 이런 인연도 생기고 조금씩이라도 앞으로 간다는 걸 많이 느꼈다. 에어컨이 없어 너무 더운 여름에 출가를 도와주신 고마운 분을 YC를 통해 알게 됐다. 서로의 타이밍이 안 맞아서 함께 팀을 이루지 못했지만, 여름 동안 서울에서 지내면서 많은 사람을 더 쉽게 만날 수 있었고 지금 지낼 집을 찾는 것도 수월했고 무엇보다 탈 없이 지낼 수 있었다. 창업에 대한 이해도 높은 분이라 가끔 나의 상황에 대한 의견을 여쭤보기도 했다. 앤틀러 직전 기수를 하셔서 앤틀러를 추천해 주셨고 최종적으로 앤틀러를 하기로 결정하게 되기도 했다. 내가 드린 도움은 한 개도 없지만 언젠간 도움을 갚을 날이 생기길 바라면서 연락은 가끔 하고 있다. 내가 하고 있던 프로덕트에 관심이 있으셔서 같이 프로젝트를 시작했던 사람도 있다. 중간에 아이템이 바뀌면서 사실 관심사에서 멀어졌을 수도 있는데 그다음 아이템까지도 도움을 주셨다. 이 사람도 정말 똑똑하고 배울 점 많은 친구로 잘 지내고 있다. 이분에게도 언젠간 도움을 드릴 수 있길 바란다. After Antler(AA)앤틀러는 내가 ‘창업가를 찾고, 연락드리고, 얘기를 나눠보고’ 하는 사이클의 대부분을 해결해 주는 고마운 프로그램이었다. 나는 애초에 목표를 많이 두고 다음 액션을 결정하는 사람은 아닌지라 오직 ‘공동 창업자를 찾자’라는 목표만 생각하고 앤틀러에 참여했다. 그러나 80명 정도 되는 창업가들을 만나면서 공동 창업 여부와 상관없이 귀중한 인연도 생기고 내가 가지고 있던 생각들도 몇몇 바뀌는 경험도 했다. 공식적인 활동 기간 동안그러나! 여전히! 공동 창업자를 찾는 과정이 쉽지는 않았다. 창업하겠다는 인재를 80명이나 모아놔도 각자의 기준에 따라 같이 창업을 할 사람과 아닌 사람을 확실히 구분할 수 있게 된다. 그래서 ‘80명이나 되는데 언제 다 맞춰보지~’ 이런 고민은 별로 의미가 없는 고민이 된다. 공식적으로 한 달 반 조금 넘는 기간 동안 10번 정도 팀을 바꿔가면서 아이템을 만들어보게 되는데 이때 정말 많은 사람들과 같이 해본 것 같다. 특히 나는 거의 겹치지 않고 사람들하고 팀을 맞춰봐서 더 많이 해볼 수 있었던 것 같다. 그중 두 번은 공동 창업자끼리 해야 하는 50개의 질문(앤틀러에서는 한글로 간단히 번역된 걸 제공해 주는 것 같다!)도 해보고 지분에 대한 얘기도 깊게 하는 등 창업 팀으로 발전해 가는 경험도 있었다. 첫 번째 팀은 개인적으로 기대가 되는 팀이었지만 많은 걸 해보지는 못한 상태로 끝났다. 공동 창업자분이 앤틀러 활동을 하다 보니 생각보다 자신이 하고 싶은 도메인이 정해져 있는 것 같다고 하셨다. 그리고 그 부분이 내 생각에는 좁아서 위에 얘기했던 기준에 부합하지 않았다. 생각보다 앤틀러 참여자 중 이런 경우가 종종 있었다. 도메인에 특별히 묶이지 않았다고 생각했지만, 하다 보니 관심 있는 도메인 외에는 하지 않고 싶어 하는 경우이다. 나도 그런 게 아예 없는 건 아니었지만, ‘꼭 하고 싶은 도메인’이 생기기보단 ‘하기 싫은 도메인’이 생겼다. 보통 최종 모습을 상상했을 때 그 규모가 상당하면 재미를 느꼈지만 에듀 테크에는 흥미를 못 느꼈다. 그다음 팀도 이상적인 팀을 만났다고 생각했지만, 생각보다 일하는 방식이 잘 안 맞아서 팀이 와해됐다. 이때 만나 뵌 형님들은 그냥 하는 소리가 아니라 배울 점이 너무 많고 친절하시고 오래오래 친하게 지내고 싶은 사람들이었지만 일하면서 커뮤니케이션 문제가 몇 번 발생했다. 지금도 돌이켜 생각해 보면 서로의 긴밀한 피드백으로 해결할 수 있지 않았을까? 하는 생각은 있다. 워낙 이상적이라고 느꼈던 탓인가 내가 계속 지랄 지랄했던 것 같기도 하다. 정말 비범한 무언가를 만들어보자며 만난 사람들에게 누구에게든(나는 당연히 포함해서) 충족하기 어려운 강도 높은 기준을 요구하는 게 이상한 일은 아니라고 생각했다. 당시 팀의 리더분은 그 시점의 불협화음이 아마도 조율하기 어려운 성질의 차이라고 보신 것 같다. 아마 이런 마찰 자체는 내가 이번에 지랄 지랄하지 않았어도 언젠간 나왔을 것 같기 때문에 잘못된 선택을 하신 것 같지 않다. 이때 나 스스로에게 실망스러운 일도 있었고 유지 가능한, 그러면서도 정말 Outstanding 한 팀을 만드는 것에 대한 고민도 많이 하게 됐다. 당시 생각과 행동이 지금 아주 많이 달라진 건 아니다. 여전히 공동 창업자에게, 특히 대표에게 이런 어려운 것들을 해내길 바란다. 그래서 내가 대표를 해야겠다는 생각이 들었다. 외부인앤틀러에서 마지막 팀이 와해되고 우연히 LinkedIn을 통해 알게 된 앤틀러 외부 창업가와도 힘을 맞춰봤다. 이때는 앤틀러에서의 활동과 다르게 프로덕트를 하나 잡고 만들면서 시작했다. 간만에 이렇게 외부에서 ‘일단 해보자’ 같은 느낌으로 시작해 보니 기분이 묘했다. 매우 날카롭게 문제 정의하는 데 시간을 엄청나게 쓰는 앤틀러에 익숙해져서 불편함이 있긴 했다. 당시 내가 만들던 아이템은 내가 공감하기는 어려운 제품이었지만, 팀에서 이걸 메인으로 하기보단 팀끼리 AI 프로덕트로 돈을 버는 경험을 만들기 위한 제품이었다. 이 제품을 만드는 데 대충 3주를 꽉 채웠다. 그다음 4주 차가 되는 날에 공동 창업자로 괜찮은 사람이었을지 여쭤봤다. 3주까지는 이미 계획된 프로덕트를 만들었으니. 프로덕트를 주도적으로 만드는 모습을 더 보면 좋겠다고 하셨다(토시가 모두 다른 것 같지만 뭐… 이런 뉘앙스였던 듯). 그리고 지금의 모습으로는 판단하기 어려울 것 같다고 하셨다. 이걸 여쭤보면서도 그런 답을 주실 것을 예상은 했지만, 어차피 같이하게 되면 내가 당분간 메인으로 사용할 능력치를 보여드린 것이기도 하고, 말씀하신 그런 주도적인 모습이 4주 차까지 안 보였다면 아마 이후에도 만족하실 만큼 보여드릴 순 있을까? 하는 생각이 들었다. 그리고 무엇보다 공식적인 엔틀러 일정이 곧 끝나가서 공동 창업자를 찾을 기회가 줄어들 것 같아 여기까지 맞춰보는 걸로 하고 팀에서 나오게 됐다. 결과적으로 나의 상대적 부족함으로 함께 창업하지 않게 됐다. 말씀하신 역량이 정말로 부족했던 것이라고 생각이 들진 않았다. 하지만 결과적으로 내가 보여주지 못한 거라서 어떻게 일해야 이런 부분이 드러날까, 왜 잘 드러나지 않았을까 하는 고민을 해보게 됐다. 대표님이 그간 일한 걸 월급으로 주려고 하셨는데 나도 같이 일해보기로 한 주체인데 돈 얼마 달라고 하는 것도 좀 이상한 그림이고… 뭐 결과 낸 것도 없기도 하고… “다음번에 제가 도움받을 일이 있겠죠.”하고 마무리 지으려고 했지만, 대표님이 정말 비싼 밥을 사주시는 걸로 마무리됐다. 같이 일할 때 대표님이 ‘자신이 사회적 자본을 잘 쌓아 도와주시는 분들이 많다’고 하셨는데 이렇게 여러 해를 일하셨다면 사회적 자본이 잘 쌓였을 것 같다는 생각이 들었다. 비록 같이 창업하진 않지만, 종종 인사드리면서 친하게 지내면 좋겠다는 생각했다. 물론 밥 잘 사줘서 사회적 자본이 쌓였다고 말씀하신 건 아니다. 이루어온 업적을 통해 주로 획득하셨겠지만 사회적 자본이란 종합적인 거니까. 종종 인사드리고 싶은 것은 맛있는 저녁을 사주신 것과 관계가 희미하다. 3주 동안 만든 프로덕트는 자꾸 iOS의 거센 반대에 좌절됐다. 다 만들었으니 올리는 것까지 도와드리겠다고 했는데 맛있는 것만 얻어먹고 지금까지도 못 하고 있다는 사실에 우울감이 든다. 돌아온 탕아외부에서 열심히 힘을 맞춰보고 다시 앤틀러 내부로 돌아왔다. 당시 계획은 두 개 있었다. 앤틀러 안에서 다시 한번 같이 창업할 사람을 찾거나, 혼자서 아이디어를 깎고 프로덕트를 만드는 것이다. 일단 내부에 있던 사람 중 그래도 합이 괜찮을 것 같다고 생각하는 분들과 얘기를 간단히 했고 그중에서 첫 번째로 팀을 이뤘던 분이 도메인이 좁아졌던 것이 해소가 되었고 팀도 따로 없는 상태가 되셨다고 해서 다시 팀을 꾸려서 무언가 해보고 있다. 이전에도 그렇고 지금도 그렇고 특별히 잘 안 맞는 부분이 없던 팀이라 아직도 걱정되는 부분 없이 프로덕트를 만들어가고 있다. 그래서 공동 창업자를 찾겠다는 24년도 목표가 하나 해소된 상태이다. 한때 YC 매칭 플랫폼에 안 들어가던 시기가 있었다. 그러자 YC가 뭔가 메일을 보내줬는데, 거기 내용에는 공동 창업자를 찾는 사람의 중간값은 100일, 20%는 8개월이 걸린다는 내용이었다. 그러니 쉽게 포기하지 말라는 얘기였다. 나는 3월부터 찾아 나섰으니까 거의 9개월 조금 넘게 걸렸다. 거의 8등급에 해당하는 속도. 기준을 낮추지 않고 신중하게 사람을 만난 것으로 생각하고 싶다. 만약 공동 창업자를 찾고 나서 시작한 사람이라면 남들보다 이 기간을 아낀 것이라고 볼 수 있다. 그렇지만 퇴사하지 않고 공동 창업자를 찾는 것은 좋은 팀을 만들 가능성을 낮추는 것 같다. 어떤 분들은 한 다리 걸쳐놓고 있는 상태는 창업에 대한 의지가 없는 상태로 판단하시고 함께 해볼 생각도 안 하시기도 하기 때문이다. 그래서 나는 지금 생각을 가지고 다시 창업 처음으로 돌아가도 공동 창업자를 찾고 나가야겠다고 생각하진 않을 것이다. 공동 창업자를 찾는 과정 동안 배운 점도 많고 회사에 다녔다면 얻지 못했을 경험도 있다. 그래서 창업가 X가 공동 창업자가 없어서 퇴사를 고민한다면 정말 고민의 원인이 공동 창업자인지 잘 생각해 보라고 하고 싶고, 만약 그 외 이유가 없는 것 같다면 퇴사하고 더 적극적으로 찾아보면 어떻겠냐고 말해주고 싶다. 나는 내용을 ‘평균적으로 8개월 걸립니다’로 기억하고 있었는데 지금 다시 찾아보니 20%라고 한다. 나는 평균보다 한참 늦었다고 생각이 바뀌었지만, 아무런 심리적 타격도 없긴 했다. 찾았음 됐지~ PMF를 찾기 위한 여정앤틀러 전에는 큼직한 프로젝트 몇 개를 시도했지만 앤틀러에서는 PMF를 찾기 전에 논리적으로 사업의 성패를 추론해 보는 훈련을 했다. 그래서 앤틀러 기간에 실제로 프로덕트를 만들어내는 경험을 많이 하지는 않았다. 하지만 이 과정에서 배운 것도 꽤 크다. Fullstack AI: BA 6 month ~ BA 3 month3월부터 시도했던 건 Fullstack AI를 만드는 것이었다. 그 당시에는 예비 창업가분들을 많이 만났는데, 겹치는 고통 중 하나가 ‘개발자가 없으니 초기 프로덕트를 만들기 위해 채용을 하거나 외주를 맡겨야 하는데 둘 다 시간 소비가 심하고 비용도 엄청 많이 든다’는 것이었다. 말씀하시는 스펙은 사실 만드는데 엄청 어려운 것은 아니지만 외주 개발을 맡기면 2~3천만 원 사이가 되는 스펙이라고 한다. 당시 나는 지금의 AI 수준에서는 그 정도 스펙의 코드 베이스는 관리할 수 있을 것 같다고 생각했다. 언제나 그렇듯 실전은 이론과 다르다. 정확히 말하자면 이론과 다르다기보단 현실엔 더 다양하게 고려해야 하는 요소가 많다. 모든 코드 베이스를 올리는 것은 Context 사이즈 문제도 있고, 백만 토큰을 쓸 수 있는 모델을 사용하더라도 비용 이슈도 있다. 그리고 무엇보다 정밀해야 하는 프로그래밍 언어가 정밀하지 않은 결과를 자꾸 받아서 테스트해야 하는 것도 문제였다. 하지만 여전히 불가능하지 않다. 개발자가 API 코드를 작성할 때 사고하는 흐름을 AI에 몇 단계로 나눠서 파이프라이닝 해두면 유의미한 결과를 만든다. 디버깅 과정도 코드를 컴파일하고 컴파일 에러가 생기면 LLM에 에러를 넘겨주고 코드를 수정하는 사이클을 만들 수 있다. 이런 걸 시도하는 팀도 많고 관련된 논문도 많아서 이때 논문을 엄청 많이 읽으면서 간단한 API 서버를 만들 수 있는 서비스까지는 도달했다. 단순하게 서버와 데이터베이스만 사용해야 하는 경우는 사용할 수 있는 정도? 하지만 OAuth라든지 클라우드 컴포넌트를 사용하는 것 등을 수행하지 못했다. 만들면서 든 생각은, ‘이거 파이프라인을 한 천 개 정도 만들면 정말 꽤 정밀하게 돌아갈 수 있을 것 같아’였다. 그런데 이 성공의 성패가 나에게 있지도 않은데 어느 정도 AI의 비용 문제와 성능 문제가 조금씩 더 나아질 것을 기대하면서 비즈니스를 해야 하나 싶은 생각도 들었고, 같은 도전을 하는 다른 팀과 비교했을 때 자본도 인력도 없는 내가 이길 수 있는 요소가 뭘까 싶은 생각도 들었다. 나는 1,000개의 고도화된 AI 파이프라인을 만들 때까지 얼마나 걸릴까? 만약 불완전한 제품이더라도 고객에게 가치가 있는 걸까? 그런 대답들에 쉽게 끄덕이기 어려웠다. 이런저런 고민을 하던 와중에 다른 아이템을 해도 괜찮겠다는 시그널을 보게 되어서 이 프로젝트를 멈추고 다음으로 넘어갔다. LLMOps - Flow: BA 3 month ~ BA 1 month이전 아이템을 만들면서 내부 툴을 만들려고 한 적이 있었다. 이 내부 툴은 여러 Depth가 있는 LLM 호출을 쉽게 만들어주는 도구였다. 열심히 연구하는 동안 LLM 파이프라인을 자주 바꿔가며 테스트해야 했는데, 이 도구가 없으면 코드에서 자료 구조와 프롬프트를 관리하고 실행 순서도 코드에서 짜야 하는데, 이걸 반복하기에 시간 소비가 너무 심해 힘들었다. Dify 같은 서비스이고 내부적으로 Langgraph를 활용하고 있는 도구였다. 당시에 프로젝트를 도와주시던 분이 이를 같이 만들어주셨는데, 이런 내부 도구가 자신의 회사에서도 꼭 필요할 것 같다고 말씀을 해주셨다. 그래서 이 파이프라인 버전 관리 도구를 Flow라고 이름 붙이고 고도화하기와 고객을 찾아보려는 시도를 한 달 반 정도 했다. 고객을 10팀 정도 만났던 것 같은데 대부분이 LLM 파이프라이닝을 통해 고도화할 필요성을 느끼고는 있지만 그것이 지금 당장인가? 하는 생각을 하고 계셨던 것 같다. 대부분의 서비스가 LLM 호출을 굉장히 단순하게 한 번 호출하는 정도로 사용하고 있었다. 큰 기업의 일부 Feature에 AI를 적용하는 것부터 시작해서 AI가 메인 Feature를 담당하는 작은 스타트업들 모두 그랬다. 생각보다 AI를 복잡하게 사용해야만 하는 비즈니스를 하는 사람을 찾기 어렵다는 생각이 들었다. 하지만 고객 인터뷰는 나름 좋은 인사이트를 줬고, Flow가 해결해 주는 문제보다 더 큰 문제를 푸는 프로덕트를 만들 수 있을 것 같았다. 고객들은 다 동일하게 이런 얘기를 했다. Prompt 관리를 잘하고 싶어 한다. 빠르게 좋은 퀄리티 프롬프트에 도달하고 싶다. LLMOps - Platea AI: BA 1 month ~ AA 0 month 인터뷰하던 팀들의 대부분은 프롬프트 평가와 관리를 Notion이나 스프레드시트로 했다. Prompt, Input, Output, Evaluation 값을 관리하는 데이터베이스를 만들어서 팀원끼리 공유하는 방식이었다. 이는 사실 협업을 위한 결과지에 가까웠다. Prompt를 작성하고 테스트해야 하는 도메인 전문가와 개발자 사이에 어떤 협업 레이어가 부족함을 느꼈다. 그리고 기본적으로 메이저 모델 여러 개를 테스트하는 환경을 마련하기 쉽지 않았다. Claude, Gemini, OpenAI 모델의 결과를 빠르게 비교하고 싶은데 각각 한 번씩 돌려보고 결과를 모아서 평가하는 방식이었다. 인터뷰를 도와주신 분 중에 우리나라에서 정말 AI 프로덕트를 잘 만들고 있다고 생각하는 팀의 대표님인 O 님이 자기 팀에서 만들어서 쓰고 있는 협업 도구를 보여주셨다. 여러 프롬프트 N개와 데이터 셋 M개를 NxM 형태의 테이블로 표현해 병렬 실행하는 아주 단순한 도구이다. 여러 모델과 설정값을 갖는 여러 프롬프트 결과를 한 번에 볼 수 있다. 하지만 로컬에서만 동작하는 서비스였기 때문에 이걸 협업할 수 있게만 만들어줘도 일단 우리 팀은 쓸 것 같다는 말씀을 주셨다. 꽤 현실적인 문제이면서 금방 만들어서 가져다드릴 수도 있고 많은 팀의 문제를 해소해 줄 수 있을 것 같다고 생각했고 2주 동안 만들어서 냅다 가져다드렸다. 그리고 인터뷰했던 여러 팀에게도 소개해 드렸다. 하지만 생각보다 전환이 발생하지는 않았다. 앤틀러에 들어가기 전에 더 많은 고객에게 전달해 보자는 생각도 했고 결제가 발생하지 않는다면 정말 중요한 문제는 아닌 것으로 생각해서 Paypal을 붙인 뒤 Product Hunt에 게시했다. 특별히 타겟팅된 건 아니었지만 결제는 발생하지 않아서 그 이상 고도화하지 않았다. 올렸던 시기가 앤틀러에 들어온 이후였다. 최근엔 Platea AI와 비슷한, 그리고 조금 더 발전된 모습을 보이는 프로덕트들이 좀 보이고 있는데, 이 문제가 워낙 명확하다 보니 경쟁 제품이 많이 생기는 것 같다. 이걸 조금 더 붙잡고 싶었지만, 일단 앤틀러에 참여하면서 공동 창업자를 찾는 데 좀 더 초점을 옮기기로 했다. 4Q에 뒤적인 아이템4Q에 뒤적였던 아이템을 다 나열하려면 진짜 1,000,000자 뚫을 것 같아서 다 적지는 못할 것 같다. 앤틀러에서는 창업자들에게 생각의 흐름이라고 해야 할지, 프레임워크라고 해야 할지, 아무튼 아이템을 찾는 방법을 훈련한다. 이 방법이 굉장히 독특하다기보다는 다들 생각하는 진짜 문제가 뭘지 알아내는 합리적인 방법이다. 개인적으로 ‘Five Whys’를 아주 디테일하게 훈련하는 것과 비슷한 것 같다. 누구의 어떤 문제인지 정의하고 대안으로 해결되지 않는 걸 찾고 왜 그 대안이 여전히 해결되지 않았는지 알아봐야 한다. 대기업은 왜 안 하고 있을까? 기존에 대안들은 왜 해결되지 않는 이 부분을 남겨둔 걸까? 이런 질문을 하면 이 문제를 해결할 때 진짜 어려운 점을 알게 된다. 앤틀러 이전에 얘기가 나오면 ‘대기업은 이런 거 안 하니까’라든지 ‘기존 상황에 익숙해져 있으니까’ 이런 얘기를 가장 많이 듣는 것 같다. 사실 이런 경우가 실제로 많을 수도 있지만, 이 아이디어가 내가 최초라고 생각하는 것도 오만하고, 뭐가 진짜 어려워서 이게 아직 해결되지 않은 것일지 미리 생각해 두는 것은 솔루션을 생각할 때 도움이 된다. 당연히 이런 것에 답하기 어렵다고 시작하지 못하는 건 아니다. 이렇게 찾다 보면 정말 아이템 찾는 게 어려워진다. 물론 이 프레임워크가 항상 정답은 아니다. 모든 질문에 답하지 못한다고 비즈니스를 시작하지 못할 건 아니다. 하지만 우리가 이런 질문에 답변을 완벽히 해도 비즈니스는 어렵다. 그래서 이렇게 날카롭게 깎아서 시작하는 것이 생존율을 올리는 데 도움을 주는 것 같다. 지금 해보고 있는 것지금 해보고 있는 건 정보를 관리하고 소비하고 재생산하는 이 사이클에 들어가 보고 있다. B2C 제품이고 이 영역에서 아주 큰 문제를 겪는 사람이 어떤 사람들일지 아직은 정확하지 않은 상태다. 그렇지만 우리가 이 사이클에 관심이 많은 사람이기도 하고 콘텐츠를 수집부터 재생산 사이클로 돈을 버는 사람들에게 실질적인 도움을 주는 프로덕트를 만들 수 있을 것 같다. 또 글로벌 프로덕트로 확장이 비교적 쉬운 영역이라고 생각이 들었다. 이 프로덕트의 마켓핏을 찾는 것에 실패하더라도 괜찮다… 또 찾아 나서면 되니까… 2025년에는이번 해는 지난해에 못 이룬 PMF 찾기 목표를 위해 열심히 뛰어야 한다. MAU 10만 이상의 프로덕트를 만들고 월 반복 매출 $50,000을 달성하고 싶다. 그리고 지난 해에 이어서 책 읽기 목표를 만들었다. 지난해보다 5권 더 많이 읽기를 목표로 할 예정이다. 그리고 잃었던 건강도 또다시 되찾기 위해 노력하리라… 올해는 정말 너무 많은 이벤트가 있어서 정말 기록하고 싶던 핵심적인 것만 담았음에도 엄청나게 긴 글을 만들어냈다. 중간중간 글을 좀 써야 할 것 같다.","link":"/posts/logs/20250101/"},{"title":"창업록: Bayesians","text":"지난 2025년 4월 17일 우리 팀은 미국에 법인을 만들었다. 최종적으로는 그다음 날에 승인됐지만 우리는 내 생일인 4월 17일을 우리의 Day One으로 삼았다. 이 글은 25년 1월 17일부터 25년 4월 17일까지 90일 동안 팀이 만들어지는 과정을 기록한 글이다. 전설의 시작1월 중순, 앤틀러에서 만났던 인연과 각자 갈 길을 가기로 하고 또다시 개인이 되었다. 그 뒤로도 혼자 일을 계속했지만 돌이켜 생각해 보니 그때는 막막한 감이 있었던 것 같다. 반드시 같이 해야만 하는 건 아니라고 생각했지만 역시 혼자서 하다 보니 내 능력의 한계 내에서만 사고한다는 느낌이 들었다. 당시에 생각하던 비즈니스는 개인을 위한 정보 관리 서비스였다. 그중에서도 뉴스레터를 요약하고 정리하는 걸 생각했는데, 그 문제가 나에게 있었기 때문이었다. 이렇게 뭔가 내가 직관적으로 느끼는 것, 이해하는 것으로 문제가 한정되는 느낌이 들자 ‘나에게는 공동 창업자가 필수군’하는 생각이 들었다. 즉시 링크드인에 커피챗을 하자고 글을 올렸다. 공동 창업자를 바로 찾겠다는 생각은 아니었다. 글에서처럼 실제로 창업이라는 과정을 정말 정말 긴 호흡으로 바라봐야겠다고 생각한 시점이라, 아이디어도 충전하고, 어떻게든 연결될 수 있는 사람들을 많이 만들어야겠다고 생각하며 글을 썼다. 정말 신기하게도 13,790 impressions가 나왔다. 여태 썼던 글 중 가장 많이 사람들에게 퍼졌던 글인 것 같다. 정말 많이 퍼진 만큼 정말 많은 분이 커피챗을 하자고 말씀 주셨다. 일주일 동안 22번의 약속을 19명과 잡았다. ‘무료 개발자를 찾습니다.’ 같은 커피챗도 요청도 있었다. “개발자가 없습니다.”, “내가 진짜 해보고 싶은 게 있습니다.” 요런 느낌? 그런 경우는 팀에는 조인하지 않는다고 말씀드리고 그럼에도 커피챗이 괜찮을지 여쭤봤다. 단 한 명도 빠짐없이 안 보기로 하고 건승을 바란다고 해주셨다. MJ는 19명 중 한 명이었고 두 번 만나본 사람이기도 하다. MJ는 첫 만남에 시원하게 지각하고 점심을 샀다. 그때 뵀을 때는 캐주얼한 정도로 얘기를 나누고 늘 그렇듯 창업의 동기나 꿈의 크기를 보려고 열심히 노력했던 것 같다. 두 번째 만남은 카페에서 First Round가 배포한 공동 창업자 간 50 Questions에 대한 답변을 각자 준비해 와서 맞춰봤다. 개인적으로 작성했던 25년 4주 차 회고를 보니 50 Questions에 대한 답변도 잘 맞는다고 느끼고 서로에게 기대하는 점, 창업을 대하는 태도에도 만족을 느꼈던 것 같다. 그러나 가장 만족스러웠던 점은 공동 창업자에게 10~15년 클리프를 걸고 싶어 했던 점이었다. 공동 창업자가 나가지 못하게 하는 데 나만큼이나 관심이 있는 사람인 것 같았고 ‘이 사람은 끝까지 창업하고 살겠구나!’ 하는 생각이 들었다. 그다음 주부터 본격적으로 일을 같이하기 시작했다. 합의된 목표와 팀의 색우리는 둘 다 미국에 가는 것, 그리고 Day 1부터 글로벌 서비스를 만드는 걸 기본으로 두었다. 이번 사업에서 특별한 도메인을 정하지 않았던 우리의 기준은 최소 1조 5천억($1B) 이상의 기업이 될 수 있는 문제를 해결하는 거였다. 범지구적으로 50조~100조 정도의 시장이면 어떤 세그먼트로 파는지와 관계없이 충분히 도달할 수 있을 것 같다. 처음 합을 맞춰본 영역은 연구자의 영역이었다. 우리는 Cursor가 그랬던 것처럼 연구자들이 논문을 읽고 정리하는 패턴에서 최적의 UX를 만들 수 있다고 봤다. 10명 정도의 연구자를 인터뷰한 다음 동일한 패턴을 확인했고 일단 첫 번째 제품을 Notion for Researcher 같은 형태로 접근했다. 우리는 Notion처럼 정리할 수 있는 에디팅 도구, PDF Viewer, PDF를 아주 잘 이해한 RAG를 붙여놓은 서비스를 3일 만에 만들고 배포했다. 그리고 인터뷰했던 사람 중 몇 사람과 다시 얘기했지만 우리가 전달하는 가치가 매우 모호했다. 지금 다시 생각해 봐도 우리는 ‘인터뷰를 하면서 봤던 패턴을 최적화한 제품이 이게 맞을까?’ 하는 고민이 있었던 것 같다. 처음 이 영역에 대해 고민했을 때는 ‘이렇게 비좁은 시장을 파보는 게 맞는 건가…’ 싶었다. 그리고 그 생각은 지금도 어느 정도 가지고 있다. 결과적으로 우리는 이 영역에서 더 구체화하는 과정에 고객의 세그먼트가 더 좁아졌고 그렇게 되면 우리의 목표에 맞지 않는 규모가 된다고 판단해 다른 아이템으로 넘어갔다. 이 과정에서 사실 유의미했던 건 MJ와 일한다면 어떤 느낌이겠구나 하는 대략적인 감을 잡을 수 있었다. 우리는 성질이 매우 급한 사람들이다. 팀에는 결론 이외는 부수적인 것으로 만들고 결론으로 달려가는 힘이 있다. 이러한 성질이 항상 긍정적인 결과를 주는 건 아니지만 앞으로 꽤 유용할 것으로 생각한다. 우리는 둘 다 이성을 잘 사용한다. 문제를 정의하는 방식이라든지, 상대를 설득하기 위해 어떤 근거가 있어야 합리적인지 생각할 줄 안다. 우리는 일을 비슷한 정도로 좋아하는 것 같다. 공동 창업자를 만날 때 항상 다들 일 하는 걸 좋아한다고는 하지만 지속 가능한 수준을 유지하기 위해 일하는 시간이 얼마나 되어야 하는지 보면 아주 잘 맞는 경우가 없었던 것 같다. 만약 일하는 게 너무 좋으면 더 많이 해도 지속 가능하지 않을까 하는 생각인데, 그런 면에서 우리는 비슷하게 일을 좋아하는 것 같다. 우리가 꽤 오래 해도 괜찮은 영역그 뒤로 우리는 매우 빠르게 아이템을 생각해 내고 검증해 갔다. 지금 하고 있는 아이템 전에 다섯 개 정도가 있었고 거의 1주일에 한 개의 아이템을 확인했다. 아이템 찾는 게 쉽지 않을 거라는 생각으로 계속하고 있었지만, 아이템이 도메인 단위로 바뀌는 경우 이전에 조금이라도 쌓였던 팀의 노하우, 배운 점들이 무의미하게 되어버리는 현상이 아쉬웠다. 그래서 우리가 꽤 오래 해도 괜찮은 영역 안에서 계속 피봇하며 나아갈 영역이면 더 좋겠다는 생각이 들었다. 처음으로 2주 이상 진행했던 도메인이 ‘애니메이션’ 영역이다. 2D Animation은 MJ의 애착이 있는 콘텐츠 영역이기도 하고 졸업 전에 관련 AI 연구를 진행한 적이 있는 영역이었다. 나 역시도 덕후의 영역까진 도달하진 못했지만 2D 애니메이션을 보는 걸 어렸을 때부터 즐기고 있다. 처음에는 애니메이션 제작 영역에서의 AI 도구를 생각했는데 팀 안에서의 생각과 여러 도움을 주시는 분들의 생각을 조합해 Consumer 쪽까지 할 수 있는 방향으로 나아가고 있다. Consumer 쪽은 어떤 문제의 영역이라기보단 재미의 영역이라, 논리보다는 믿음이 필요한 것 같은 부분이 있다. 창업자 개인들의 경험에 따라 좋은 콘텐츠를 만들어내는 데 비용을 기존보다 1/10000 해버릴 수 있다면 성공시킬 수 있을 것이라는 믿음이 있고 우리가 이걸 잘하는 사람들이라는 믿음이 있는 것 같다. 지금은 Wattpad의 수백만 뷰의 작품을 AI Animation으로 Adoption하고 자동화하는 중이다. 원작자와 긴밀히 협업하는 중인데, 다행히 AI Anime의 초기 퀄리티에 대해 긍정적인 반응을 보인다. AI Video 영역에서 어떤 큰 서비스가 나올 On-Time에 가까워진다는 생각이 많이 든다. 지금 당장 만들어내는 퀄리티가 부족하거나 비용이 많이 들더라도 GPT처럼 곧 Consumer 수준까지 떨어지고 빨라지고 좋아질 것이다. 우리는 그 On-Time을 준비하고 있는 상태로 느껴진다. 성공하거나 성공 중이거나 나는 노홍철 형님의 사고를 굉장히 좋아한다. 빠니보틀 채널에서 홍철 형님이 보틀 형님과 여행하던 중에 했던 말이 내 생각과 거의 일치한다. ‘어떻게 하고 싶은 것만 하고 사는데 모두 잘 되세요?’라는 질문에 ‘될 때까지 했다’라고 답했다. 포기하기 전까지는 모두 성공하는 중인 것이다. 그런 의미에서 포기할 것 같지 않은 팀은 실패할 것 같지 않은 팀과 동일한 말이다. 내 생각에 지금 우리 팀은 개인 또는 가족의 심각한 상해 및 질병으로 인해 일하는 것이 불가능한 상황이 아니라면 실패할 것 같지 않은 팀이다. 앞으로 성공하는 방향으로 가기 위해 많은 사람들과 함께하게 될 텐데, 그 사람들도 우리를 보고 ‘어쨌든 해낼 팀’이라는 느낌을 받았으면 좋겠다. Day 1 여러 복합적 이유로 미국 법인을 후다닥 만들게 되었다. 생일에 법인을 발족했다는 게 또 다른 애착을 만들어주는 것 같다. Bayesians는 작년에 창업하면서 여러 상황의 Organization 이름으로 사용했던 이름이다. AI와 밀접하기도 하지만 조건부 확률을 고민할 수 있는 사람은 의사결정의 승률을 높일 수 있다고 생각해 지었던 이름이다. MJ가 이름이 괜찮은 것 같다 해 지금까지 아이템과 무관하게 팀 이름으로 사용 중이다. 지금까지는 Stealth 상태로 있었지만, 이제는 확정적으로 팀을 드러낼 수 있는 상태가 된 것 같다.","link":"/posts/logs/20250420/"}],"tags":[{"name":"review","slug":"review","link":"/tags/review/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"distributed_system","slug":"distributed-system","link":"/tags/distributed-system/"},{"name":"dynamodb","slug":"dynamodb","link":"/tags/dynamodb/"},{"name":"serverless","slug":"serverless","link":"/tags/serverless/"},{"name":"architecture","slug":"architecture","link":"/tags/architecture/"},{"name":"etcd","slug":"etcd","link":"/tags/etcd/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"cs","slug":"cs","link":"/tags/cs/"},{"name":"rdb","slug":"rdb","link":"/tags/rdb/"},{"name":"system_design","slug":"system-design","link":"/tags/system-design/"},{"name":"memcache","slug":"memcache","link":"/tags/memcache/"},{"name":"package_architecture","slug":"package-architecture","link":"/tags/package-architecture/"},{"name":"package","slug":"package","link":"/tags/package/"},{"name":"programming","slug":"programming","link":"/tags/programming/"},{"name":"gc","slug":"gc","link":"/tags/gc/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"lambda","slug":"lambda","link":"/tags/lambda/"},{"name":"grpc","slug":"grpc","link":"/tags/grpc/"},{"name":"http2","slug":"http2","link":"/tags/http2/"},{"name":"protocolbuffer","slug":"protocolbuffer","link":"/tags/protocolbuffer/"},{"name":"typescript","slug":"typescript","link":"/tags/typescript/"},{"name":"nestjs","slug":"nestjs","link":"/tags/nestjs/"},{"name":"NestJS_빠르게_배우기","slug":"NestJS-빠르게-배우기","link":"/tags/NestJS-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%EB%B0%B0%EC%9A%B0%EA%B8%B0/"},{"name":"서버사이드_개발환경구성","slug":"서버사이드-개발환경구성","link":"/tags/%EC%84%9C%EB%B2%84%EC%82%AC%EC%9D%B4%EB%93%9C-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD%EA%B5%AC%EC%84%B1/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"도커로_개발_환경_구축하기","slug":"도커로-개발-환경-구축하기","link":"/tags/%EB%8F%84%EC%BB%A4%EB%A1%9C-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0/"},{"name":"aws","slug":"aws","link":"/tags/aws/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"linux_cheatsheet","slug":"linux-cheatsheet","link":"/tags/linux-cheatsheet/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"network_layer","slug":"network-layer","link":"/tags/network-layer/"},{"name":"http3","slug":"http3","link":"/tags/http3/"},{"name":"serverless_프레임워크_빠르게_배우기","slug":"serverless-프레임워크-빠르게-배우기","link":"/tags/serverless-%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%EB%B0%B0%EC%9A%B0%EA%B8%B0/"},{"name":"memory","slug":"memory","link":"/tags/memory/"},{"name":"react","slug":"react","link":"/tags/react/"},{"name":"redux","slug":"redux","link":"/tags/redux/"},{"name":"data_structure","slug":"data-structure","link":"/tags/data-structure/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"refactoring","slug":"refactoring","link":"/tags/refactoring/"},{"name":"rxjs","slug":"rxjs","link":"/tags/rxjs/"},{"name":"RxJS_빠르게_배우기","slug":"RxJS-빠르게-배우기","link":"/tags/RxJS-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%EB%B0%B0%EC%9A%B0%EA%B8%B0/"},{"name":"terraform","slug":"terraform","link":"/tags/terraform/"},{"name":"iac","slug":"iac","link":"/tags/iac/"},{"name":"devops","slug":"devops","link":"/tags/devops/"},{"name":"retrospect","slug":"retrospect","link":"/tags/retrospect/"},{"name":"log","slug":"log","link":"/tags/log/"}],"categories":[{"name":"books","slug":"books","link":"/categories/books/"},{"name":"essay","slug":"essay","link":"/categories/essay/"},{"name":"database","slug":"database","link":"/categories/database/"},{"name":"go","slug":"go","link":"/categories/go/"},{"name":"backend","slug":"backend","link":"/categories/backend/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"network","slug":"network","link":"/categories/network/"},{"name":"rust","slug":"rust","link":"/categories/rust/"},{"name":"serverless","slug":"serverless","link":"/categories/serverless/"},{"name":"os","slug":"os","link":"/categories/os/"},{"name":"etc","slug":"etc","link":"/categories/etc/"},{"name":"logs","slug":"logs","link":"/categories/logs/"}],"pages":[{"title":"About","text":"Backend engineer Self learner What I interested in Go, Kotlin Infrastructure &amp; Architecture Computer Science Code Detail RESUME Link Github Email: changhoi0522@gmail.com","link":"/about"},{"title":"","text":"google-site-verification: google9b7d9fdb248b2d4e.html","link":"/google9b7d9fdb248b2d4e"}]}